{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 3. Optimization and Linear Regression**\n",
        "## Applied Machine Learning \n",
        "**Volodymyr Kuleshov** \\\n",
        "Cornell Tech"
      ],
      "metadata": {
        "id": "cXlzpWLXl3aX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 1: Optimization and Calculaus Background**\n",
        "In the previous lecture, we have learned what is a supervised machine learning problem. \\\n",
        "Before we turn our anttention to Linear Regression, we will dive deeper into the question of Optimization."
      ],
      "metadata": {
        "id": "kC8O_fpGmREa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Review: Components of A Supervised Machine Learning Problem**\n",
        "At a high-level, a supervised machine learning problem has the following structure:\n",
        "$$Dataset + \\underbrace{\\text{Algorithm}}_\\text{Model Class + Objective + Optimizer} \\to Predictive ~ Model$$\n",
        "\n",
        "The predictive model is chosen to model the relationship between the inputs and targets. For instance, it can predict future targets.\n"
      ],
      "metadata": {
        "id": "c1NnHYplm_RV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Optimizer: Notation**\n",
        "At a high level, an optimizer takes:\n",
        "\n",
        "\n",
        "*   An objective $J$ (also called a loss function) and\n",
        "*   A model class $\\mathcal{M}$ and finds a model $f \\in \\mathcal{M}$ with the smallest value of the objective $J$.\n",
        "$$min_{f \\in \\mathcal{M}}J(f)$$\n",
        "\n",
        "Intuitively, this is the function that bests 'fits' the data on the training dataset $\\mathcal{D} = \\{(x^{(i)},y^{(i)}) \\mid i = 1,2, \\dots, n\\}$.\n",
        "\n",
        "We will use the quadraric function as our running example for an objective $J$.\n",
        "\n"
      ],
      "metadata": {
        "id": "xtVy_4VynVLc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GeOjo9FlgDP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [12,4]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def quadratic_function(theta):\n",
        "  ''' The cost function , J(theta) '''\n",
        "  return 0.5*(2*theta - 1)**2"
      ],
      "metadata": {
        "id": "1QkP_k_8nCOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize it"
      ],
      "metadata": {
        "id": "SowxOpuKnilj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First construct a grid of theta1 parameter pairs and their corresponding cost function values\n",
        "thetas = np.linspace(-0.2,1,10)\n",
        "f_vals = quadratic_function(thetas[:,np.newaxis])\n",
        "\n",
        "plt.plot(thetas, f_vals)\n",
        "plt.xlabel('Theta')\n",
        "plt.ylabel('Objective value')\n",
        "plt.title('Simple quadratic function')"
      ],
      "metadata": {
        "id": "5vYdfOHsnkh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mk3wJnEDunx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Calculus Review: Derivatives**\n",
        "Recall that the derivatives:\n",
        "$$\\frac{df(\\theta_0)}{d\\theta}$$\n",
        "\n",
        "of an univariate function $f : \\mathbb{R} \\to \\mathbb{R}$ is the instantaneous rate of change of the function $f(\\theta_0)$ with respect to its parameter $\\theta$ at the point $\\theta_0$."
      ],
      "metadata": {
        "id": "WZJg6jhluoCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quadratic_derivative(theta):\n",
        "  return (2*theta - 1)*2\n",
        "\n",
        "df0 = quadratic_derivative(np.array([[0]])) # derivative at zero\n",
        "f0 = quadratic_function(np.array([[0]]))\n",
        "line_length = 0.2\n",
        "\n",
        "plt.plot(thetas, f_vals)\n",
        "plt.annotate('', xytext = (0 - line_length, f0 - line_length*df0), xy = (0 + line_length, f0 + line_length*df0),\n",
        "             arrowprops={'arrowstyle':'-','lw':1.5}, va = 'center', ha='center')\n",
        "plt.xlabel('Theta')\n",
        "plt.ylabel('Objective value')\n",
        "plt.title('Simple quadratic function')"
      ],
      "metadata": {
        "id": "O5pDJKZrvhEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pts = np.array([[0, 0.5, 0.8]]).reshape((3,1))\n",
        "df0s = quadratic_derivative(pts)\n",
        "f0s = quadratic_function(pts)\n",
        "\n",
        "plt.plot(thetas, f_vals)\n",
        "for pts, df0s, f0s in zip(pts.flatten(), df0s.flatten(), f0s.flatten()):\n",
        "  plt.annotate('', xytext = (pts - line_length, f0s - line_length*df0s), xy = (pts + line_length, f0s + line_length*df0s),\n",
        "             arrowprops={'arrowstyle':'-','lw':1}, va = 'center', ha='center')\n",
        "\n",
        "plt.xlabel('Theta')\n",
        "plt.ylabel('Objective value')\n",
        "plt.title('Simple quadratic function')\n",
        "  "
      ],
      "metadata": {
        "id": "G2xpMDWGx3IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Calculus Review: Partial Derivative**\n",
        "The partial derivative\n",
        "$$\\frac{\\partial f(\\theta_0)}{\\partial \\theta_j}$$\n",
        "of a multivariate function $f : \\mathbb{R}^d \\to \\mathbb{R}$ is the derivative of $f$ with respect to $\\theta_j$ while all other inputs $\\theta_k$ for $k \\neq i$ are fixed."
      ],
      "metadata": {
        "id": "uslYdGfRzK2E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Calculus Review: The Gradient**\n",
        "The gradient $\\nabla_{\\theta}f$ further extends the derivative to multivariate functions $f : \\mathbb{R}^d \\to \\mathbb{R}$, and is defined at the point $\\theta_0$ as: \n",
        "\n",
        "$$\\nabla_{\\theta}f(\\theta_0) = \\begin{bmatrix}\n",
        "\\frac{\\partial f(\\theta_0)}{\\partial \\theta_1}\\\\\n",
        "\\frac{\\partial f(\\theta_0)}{\\partial \\theta_2}\\\\\n",
        "\\vdots\\\\\n",
        "\\frac{\\partial f(\\theta_0)}{\\partial \\theta_j}\n",
        "\\end{bmatrix}.$$\n",
        "\n",
        "The $j$-th entry of the vector $\\nabla_{\\theta}f(\\theta_0)$ is the partial derivative $\\frac{\\partial f(\\theta_0)}{\\partial \\theta_j}$ of $f$ with respect to the $j$-th component of $\\theta$"
      ],
      "metadata": {
        "id": "YtH6QNjE0Wk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a quadratic function as a running example:"
      ],
      "metadata": {
        "id": "UhQo_rn92HHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quadratic_function2d(theta0, theta1):\n",
        "  \n",
        "  \"\"\" Quadratic objective function J(theta0, theta1).\n",
        "  \n",
        "      The inputs theta0 and theta1 are 2-d arrays and we evaluate \n",
        "      the objective at each point theta0[i,j] and theta1[i,j].\n",
        "      We implement in this way so it is easier to plot the level\n",
        "      curves of the function in 2d.\n",
        "\n",
        "      Parameters:\n",
        "      theta0 (np.array): 2d array of first parameter theta0\n",
        "      theta1 (np.array): 2d array of second parameter theta1\n",
        "\n",
        "      Returns:\n",
        "      fvals (np.array): 2d array of objective function values.\n",
        "      fvals is the same dimension as theta0 and theta1.\n",
        "      fvals[i,j] is the value at theta0[i,j] and theta1[i,j]. \n",
        "\n",
        "      \"\"\"\n",
        "  theta0 = np.atleast_2d(np.asarray(theta0))\n",
        "  theta1 = np.atleast_2d(np.asarray(theta1))\n",
        "  return 0.5*((2*theta1 - 2)**2 + (theta0 - 3)**2)"
      ],
      "metadata": {
        "id": "qMlXPyq-2Oib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize this function"
      ],
      "metadata": {
        "id": "MgHi_Uzt4Rm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta0_grid = np.linspace(-4,7,101)\n",
        "theta1_grid = np.linspace(-1,4,101)\n",
        "theta_grid = theta0_grid[np.newaxis,:], theta1_grid[:,np.newaxis]\n",
        "J_grid = quadratic_function2d(theta0_grid[np.newaxis,:], theta1_grid[:,np.newaxis])\n",
        "\n",
        "X,Y = np.meshgrid(theta0_grid, theta1_grid)\n",
        "contours = plt.contour(X,Y,J_grid,10)\n",
        "plt.clabel(contours)\n",
        "plt.axis('equal')"
      ],
      "metadata": {
        "id": "1D0adqaD4Uvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's write down the derivative of the quadratic function"
      ],
      "metadata": {
        "id": "0955S7LJH0Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quadratic_derivative2d(theta0, theta1):\n",
        "  \"\"\" Derivative of quadratic objective function \n",
        "\n",
        "  The input theta0, theta1 are 1d arrays and we evaluate \n",
        "  the derivative at each value theta0[i], theta1[i].\n",
        "\n",
        "  Parameters:\n",
        "  theta0 (np.array): 1d array of first parameter theta0\n",
        "  theta1 (np.array): 1d array of second parameter theta1\n",
        "\n",
        "  Returns:\n",
        "  grads (np.array): 2d array of partial derivative\n",
        "  grads is of the same size as theta0 and theta1\n",
        "  along first dimension and of size \n",
        "  two dimension along second dimension\n",
        "  grads[i,j] is the j-th partial derivative \n",
        "  at input theta0[i] and theta1[i].\n",
        "  \"\"\"\n",
        "  # This is the gradient of 0.5*((2*theta1 - 2)**2 + (theta0 - 3)**2)\n",
        "  grads = np.stack([theta0-2, (2*theta1 - 2)*2], axis = 1)\n",
        "  grads = grads.reshape([len(theta0),2])\n",
        "  return grads"
      ],
      "metadata": {
        "id": "2ZY88WvKHzd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize the derivative"
      ],
      "metadata": {
        "id": "FdEzv1MKJ5_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta0_pts, theta1_pts = np.array([2.3,-1.35,-2.3]), np.array([2.4,-0.15,2.75])\n",
        "dfs = quadratic_derivative2d(theta0_pts, theta1_pts)\n",
        "line_length = 0.2\n",
        "\n",
        "contours = plt.contour(X, Y, J_grid, 10)\n",
        "for theta0_pt, theta1_pt, df0 in zip(theta0_pts, theta1_pts, dfs):\n",
        "  plt.annotate('', xytext=(theta0_pt, theta1_pt),\n",
        "               xy = (theta0_pt - line_length*df0[0], theta1_pt - line_length*df0[1]),\n",
        "               arrowprops={'arrowstyle':'->', 'lw':2}, va = 'center', ha = 'center')\n",
        "\n",
        "plt.scatter(theta0_pts, theta1_pts)\n",
        "plt.clabel(contours)\n",
        "plt.xlabel('Theta0')\n",
        "plt.ylabel('Theta1')\n",
        "plt.title('Gradient of the Quadratic function')\n",
        "plt.axis('equal')\n"
      ],
      "metadata": {
        "id": "OMLFcNbbKDoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part1b: Gradient Descent**\n",
        "Next, we will use gradients to define an important algorithm called *gradient descent*"
      ],
      "metadata": {
        "id": "_Psox0ScJ8Rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Calculus Review: The Gradient** \n",
        "The gradient $\\nabla_{\\theta}f$ further extends the derivative to multivariate functions $f : \\mathbb{R}^d \\to \\mathbb{R}$, and is defined at the point $\\theta_0$ as: \n",
        "\n",
        "$$\\nabla_{\\theta}f(\\theta_0) = \\begin{bmatrix}\n",
        "\\frac{\\partial f(\\theta_0)}{\\partial \\theta_1}\\\\\n",
        "\\frac{\\partial f(\\theta_0)}{\\partial \\theta_2}\\\\\n",
        "\\vdots\\\\\n",
        "\\frac{\\partial f(\\theta_0)}{\\partial \\theta_j}\n",
        "\\end{bmatrix}.$$\n",
        "\n",
        "The $j$-th entry of the vector $\\nabla_{\\theta}f(\\theta_0)$ is the partial derivative $\\frac{\\partial f(\\theta_0)}{\\partial \\theta_j}$ of $f$ with respect to the $j$-th component of $\\theta$"
      ],
      "metadata": {
        "id": "JL3mMKLiMjaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradient Descent: Intuition**\n",
        "Gradient descent is a very common optimization algorithm used in machine learning.\\\n",
        "The intuition behind the gradient descent is to repeatedly obtain the gradient to determine the direction in which the function decrease most steeply and take a step in that direction."
      ],
      "metadata": {
        "id": "B15xo0UjM1FN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradient Descent: Notation**\n",
        "More formally, if we want to optimize $J(\\theta)$, we start with an initial guess $\\theta_0$ for the parameters and repeat the following update until $\\theta$ is no longer changing:\n",
        "$$\\theta_i := \\theta_{i-1} - \\alpha.\\nabla_{\\theta}J(\\theta_{i-1}) $$ \n",
        "\n",
        "As code, this method may look as follows:\n",
        "```python\n",
        "theta, theta_prev = random_initialization()\n",
        "while norm(theta - theta_prev) > convergence_threshold:\n",
        "    theta_prev = theta\n",
        "    theta = theta_prev - step_size * gradient(theta_prev)\n",
        "```\n",
        "In the above algorithm, we stop when $||\\theta_i - \\theta_{i-1}||$ is small."
      ],
      "metadata": {
        "id": "X0PYnhviNme0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing in numpy"
      ],
      "metadata": {
        "id": "at_rNBLASdMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convergence_threshold = 1e-1\n",
        "step_size = 2e-1\n",
        "theta, theta_prev = np.array([[-2],[3]]), np.array([[0],[0]])\n",
        "opt_pts = [theta.flatten()]\n",
        "opt_grads = []\n",
        "\n",
        "while np.linalg.norm(theta - theta_prev) > convergence_threshold:\n",
        "  # We repeat this while the value of the function is decreasing\n",
        "  theta_prev = theta\n",
        "  gradient = quadratic_derivative2d(*theta).reshape([2,1])\n",
        "  theta = theta_prev - step_size*gradient\n",
        "  opt_pts += [theta.flatten()]\n",
        "  opt_grads += [gradient.flatten()] "
      ],
      "metadata": {
        "id": "6X10hKmiT2_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now visualize the gradient descent"
      ],
      "metadata": {
        "id": "Dk5xPRXnQGBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt_pts = np.array(opt_pts)\n",
        "opt_grads = np.array(opt_grads)\n",
        "\n",
        "contours = plt.contour(X, Y, J_grid, 10)\n",
        "plt.clabel(contours)\n",
        "plt.scatter(opt_pts[:,1], opt_pts[:,0])\n",
        "\n",
        "for opt_pt, opt_grad in zip(opt_pts, opt_grads):\n",
        "  plt.annotate('', xytext=(opt_pt[0], opt_pt[1]),\n",
        "               xy = (opt_pt[0] - 0.8*step_size*opt_grad[0], opt_pt[1] - 0.8*step_size*opt_grad[1]),\n",
        "               arrowprops = {'arrowstyle':'->','lw':2 }, va = 'center', ha = 'center')\n",
        "  \n",
        "plt.axis('equal')"
      ],
      "metadata": {
        "id": "lUjxOvUUTGAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 2: Gradient Descent in Linear Model**\n",
        "Let's now use gradient descent to derive a supervised learning algorithm for linear models.\n"
      ],
      "metadata": {
        "id": "0l4ini0cUx0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Linear Model family**\n",
        "Recall that a linear model has the form\n",
        "$$y = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\dots + \\theta_dx_d$$ \n",
        "\n",
        "where $x \\in \\mathbb{R}^d$ is a vector of features and $y$ is the target. The $\\theta_j$ is the parameter of the model.\n",
        "\n",
        "By using the notation $x_0 = 1$, we can represent the model in a vectorized form:\n",
        "$$f_{\\theta}(x) = \\sum_{j=0}^d\\theta_jx_j = \\theta^\\top x$$"
      ],
      "metadata": {
        "id": "C1tiY7oRVICv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define our model in Python"
      ],
      "metadata": {
        "id": "4_MKaXq7W7JF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(X,theta):\n",
        "  \"\"\" The linear model we are trying to fit.\n",
        "\n",
        "  Parameters:\n",
        "  theta (np.array): d-dimensional vector of parameters\n",
        "  X (np.parameters): (n,d)-dimensional data matrix\n",
        "\n",
        "  Returns:\n",
        "  y_pred (np.array): n-dimensional vector of predicted target \n",
        "  \"\"\"\n",
        "  return X.dot(theta)"
      ],
      "metadata": {
        "id": "vcoHP943W-dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **An Objective: Mean Squared Error**\n",
        "We pick $\\theta$ to minimize the mean squared error (MSE). Slight variants of this objective are also known as the residual sum of squares (RSS) or the sum of squares residual (SSR).\n",
        "$$J(\\theta) = \\frac{1}{2n}\\sum_{i=1}^{n}(y^{(i)} - \\theta^\\top x^{(i)})^2$$\n",
        "In other words, we are looking for the best compromise in $\\theta$ over all the data pointd."
      ],
      "metadata": {
        "id": "CTLvHeCOHFy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's implement the MSE."
      ],
      "metadata": {
        "id": "kljrF9tpIkVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_squared_error(theta, X, y):\n",
        "  return 0.5*np.mean((y - f(X,theta))**2)"
      ],
      "metadata": {
        "id": "35lHjT6UIplW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mean Squared Error: Partial Derivatives**\n",
        "Let's work out what a partial derivative is for MSE loss for a linear model.\n",
        "\n",
        "$$ \\begin{align*}\n",
        "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} &= \\frac{\\partial}{\\partial \\theta_j}\\frac{1}{2}(f_{\\theta}(x) - y)^2 \\\\\n",
        "&= (f_{\\theta}(x) - y)\\frac{\\partial}{\\partial \\theta_j}(f_{\\theta}(x) - y) \\\\\n",
        "&= (f_{\\theta}(x) - y)\\frac{\\partial}{\\partial \\theta_j}(\\sum_{k=0}^d\\theta_kx_k -y)\\\\\n",
        "&= (f_{\\theta}(x) - y).x_j\n",
        "\\end{align*}$$"
      ],
      "metadata": {
        "id": "3dwDeKvBJJLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mean Squared Error: The Gradient**\n",
        "We can use this derivative to obtain the gradient for MSE for a linear model.\n",
        "$$\\begin{align*}\n",
        "\\nabla_{\\theta}J(\\theta)  = \\begin{bmatrix}\n",
        "\\frac{\\partial J(\\theta)}{\\theta_0}\\\\\n",
        "\\frac{\\partial J(\\theta)}{\\theta_1}\\\\\n",
        "\\vdots\\\\\n",
        "\\frac{\\partial J(\\theta)}{\\theta_d}\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "(f_{\\theta}(x) - y).x_0 \\\\\n",
        "(f_{\\theta}(x) - y).x_1 \\\\\n",
        "\\vdots \\\\\n",
        "(f_{\\theta}(x) - y).x_d\n",
        "\\end{bmatrix}.\n",
        "= (f_{\\theta}(x) - y)\\cdot\\bf{x}.\n",
        "\\end{align*}$$"
      ],
      "metadata": {
        "id": "ERL44V2BLvol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's implement the gradient"
      ],
      "metadata": {
        "id": "AqXTW5d5OMQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_gradient(theta, X, y):\n",
        "  \"\"\" Returns: \n",
        "  grads (np.array): d-dimensional gradient of the MSe \n",
        "  \"\"\"\n",
        "  return np.mean((f(X,theta) - y)*X.T, axis=1)"
      ],
      "metadata": {
        "id": "TrliqPzUIpsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **EXAMPLE: The UCI Diabetes Dataset**\n",
        "In this section, we are going to again use the UCI Diabetes dataset.\n",
        "\n",
        "\n",
        "*   For each patient, we have a acess to a measurement of their body mass index (bmi) and a quantative diabetes risk score (0-400).\n",
        "*   We are interested in understanding how the BMI  affects the individual's diabetes risk.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y4fUKo_wPAtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [8,4]\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "\n",
        "# Load the diabetes dataset\n",
        "X, y = datasets.load_diabetes(return_X_y=True, as_frame=True)\n",
        "\n",
        "# Add an extra column of ones\n",
        "X['one'] = 1\n",
        "\n",
        "# Collect 20 data point and only use bmi dimension\n",
        "X_train = X.iloc[-20:].loc[:,['bmi','one']]\n",
        "y_train = y.iloc[-20:]/300\n",
        "\n",
        "plt.scatter(X_train.loc[:,['bmi']], y_train, color='black')\n",
        "plt.xlabel('Body Mass Index (BMI)')\n",
        "plt.ylabel('Diabetes Risk')"
      ],
      "metadata": {
        "id": "YI8gldM6PzrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradient Descent for Linear Regression**\n",
        "Putting this together with the gradient dascent algorithm, we obtain a method for training linear model.\n",
        "```python\n",
        "theta, theta_prev = random_initialization()\n",
        "while abs(J(theta) - J(theta_prev)) > conv_threshold:\n",
        "    theta_prev = theta\n",
        "    theta = theta_prev - step_size * (f(x, theta)-y) * x\n",
        "```\n",
        "This update also known as Least Mean Square (LMS) or Widrow Hoff learning rule."
      ],
      "metadata": {
        "id": "gB-pM6OZR68X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 1e-6\n",
        "step_size = 4e-1\n",
        "theta, theta_prev = np.array([2,1]), np.ones(2,)\n",
        "opt_pts = [theta]\n",
        "opt_grads = []\n",
        "iter = 0\n",
        "\n",
        "while abs(mean_squared_error(theta, X_train, y_train) - mean_squared_error(theta_prev, X_train, y_train)) > threshold:\n",
        "  if iter % 100 == 0:\n",
        "    print('Iteration %d. MSE: %6f' %(iter, mean_squared_error(theta, X_train, y_train)))\n",
        "  theta_prev = theta\n",
        "  gradient = mse_gradient(theta, X_train, y_train)\n",
        "  theta = theta_prev - step_size*gradient\n",
        "  opt_pts += [theta]\n",
        "  opt_grads += [gradient]\n",
        "  iter += 1\n",
        "\n"
      ],
      "metadata": {
        "id": "DX9MK74fRW6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_line = np.stack([np.linspace(-0.1, 0.1, 10), np.ones(10,)])\n",
        "y_line = opt_pts[-1].dot(x_line)\n",
        "\n",
        "plt.scatter(X_train.loc[:,['bmi']], y_train, color='black')\n",
        "plt.plot(x_line[0], y_line)\n",
        "plt.xlabel('Body Mass Index (BMI)')\n",
        "plt.ylabel('Diabetes Risk')"
      ],
      "metadata": {
        "id": "TzfEfkm1Z5ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 3: Ordinary Least Square**\n",
        "In practice, there is more effective way than gradient descent to find linear  model parameters.\\\n",
        "We will see this method here, which will lead us to our first non-toy algorithm: Ordinary Least Squares.\n"
      ],
      "metadata": {
        "id": "3yhSn4GHgL8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: The Gradient**\n",
        "The gradient $\\nabla_{\\theta}f$ further extends the derivative to multivariate functions $f : \\mathbb{R}^d \\to \\mathbb{R}$, and is defined at a point $\\theta_0$ as:\n",
        "$$\\nabla_{\\theta}f(\\theta_0) = \\begin{bmatrix}\n",
        "\\frac{\\partial f(\\theta_0)}{\\partial \\theta_1}\\\\\n",
        "\\frac{\\partial f(\\theta_0)}{\\partial \\theta_2}\\\\\n",
        "\\vdots\\\\\n",
        "\\frac{\\partial f(\\theta_0)}{\\partial \\theta_d}\\\\\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "In other words, the $j$-th entry of the vector $\\nabla_{\\theta}f(\\theta_0)$ is the partial derivative $\\frac{\\partial f(\\theta_0)}{\\partial \\theta_j}$ of $f$ with respect to the $j$-th component of $\\theta$"
      ],
      "metadata": {
        "id": "SVpvyQzugtL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The UCI Diabetes Dataset**"
      ],
      "metadata": {
        "id": "lehnH9uiiNwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [8,4]\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "\n",
        "# Load the diabetes dataset\n",
        "X, y = datasets.load_diabetes(return_X_y=True, as_frame=True)\n",
        "\n",
        "# Add an extra columns of ones\n",
        "X['ones'] = 1\n",
        "\n",
        "\n",
        "# Collect 20 data point and only use bmi dimension\n",
        "X_train = X.iloc[-20:]\n",
        "y_train = y.iloc[-20:]\n",
        "\n",
        "plt.scatter(X_train.loc[:,['bmi']], y_train, color='black')\n",
        "plt.xlabel('Body Mass Index (BMI)')\n",
        "plt.ylabel('Diabetes Risk')"
      ],
      "metadata": {
        "id": "N_LV_TwdafPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Notation: Design Matrix**\n",
        "Machine Learning algorithms are most easily defined in the language of linear algebra. Therefore, it will be useful to represent the entire dataset as one matrix $X \\in \\mathbb{R}^{n \\times d}$, of the form:\n",
        "$$X = \\begin{bmatrix}\n",
        "x_1^{(1)} &x_2^{(1)} \\ldots &x_d^{(1)} \\\\\n",
        "x_1^{(2)} &x_2^{(2)} \\ldots &x_d^{(2)} \\\\\n",
        "\\vdots \\\\\n",
        "x_1^{(n)} &x_2^{(n)} \\ldots &x_d^{(n)} \\\\\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "- & (x^{(1)})^\\top & - \\\\\n",
        "- & (x^{(2)})^\\top & - \\\\\n",
        "& \\vdots & \\\\\n",
        "- & (x^{(n)})^\\top & - \\\\\n",
        "\\end{bmatrix}$$"
      ],
      "metadata": {
        "id": "d7dnEvjBUyeH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can view the design matrix for the diabetes dataset"
      ],
      "metadata": {
        "id": "u0ZvvR4NW8lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "id": "Dnf7BDjLXBsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Notation: Design Matrix**\n",
        "Similarly, we can vectorize the target variables into a vector $y \\in \\mathbb{R}^n$ of the form:\n",
        "$$y = \\begin{bmatrix}\n",
        "y^{(1)}\\\\\n",
        "y^{(2)}\\\\\n",
        "\\vdots \\\\\n",
        "y^{(n)}\n",
        "\\end{bmatrix}.$$"
      ],
      "metadata": {
        "id": "h2uaVfhnXU_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Squared Error in Matrix Form**\n",
        "Recall that we may fit a linear model by choosing $\\theta$ that minimize the squared error:\n",
        "$$J(\\theta) = \\frac{1}{2}\\sum_{i=1}^n(y^{(i)} - \\theta^\\top x^{(i)})^2$$\n",
        "\n",
        "In other words, we are looking for the most compromise in $\\theta$ over all the data points. \\\\\n",
        "\n",
        "We can write this sum in vector-form as:\n",
        "$$J(\\theta) = \\frac{1}{2}(y - X\\theta)^\\top(y - X\\theta) = \\frac{1}{2}\\|y - X\\theta\\|^2.$$\n",
        "where $X$ is the design matrix and $\\|\\cdot\\|$ denotes the Euclidean norm."
      ],
      "metadata": {
        "id": "s4uuHLwGYELb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The Gradient of the Squared Error**\n",
        "We have a gradient for the mean squared error as follows:\n",
        "\n",
        "$$ \\begin{align*}\n",
        "\\nabla_{\\theta}J(\\theta) &= \\nabla_{\\theta}\\frac{1}{2}(X\\theta - y)^\\top(X\\theta - y)\\\\\n",
        " &= \\frac{1}{2}\\nabla_{\\theta}((X\\theta)^\\top(X\\theta) - (X\\theta)^\\top y - y^\\top (X\\theta) +y^\\top y) \\\\\n",
        " &= \\frac{1}{2}\\nabla_{\\theta}(\\theta^\\top (X^\\top X)\\theta - 2(X\\theta)^\\top y)\\\\\n",
        " &= \\frac{1}{2}(2(X^\\top X)\\theta - 2X^\\top y)\\\\\n",
        " &= (X^\\top X)\\theta - X^\\top y.\n",
        "\\end{align*}$$\n",
        "\n",
        "We used the facts that $a^\\top b = b^\\top a$ (line 3), that $\\nabla_x(bx)^\\top = \\nabla_x b^\\top x = b$ (line 4), and that $\\nabla_xx^\\top Ax = 2Ax$ for a symetric matrix $A$ (line 4)."
      ],
      "metadata": {
        "id": "g75J5oqqaLWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Normal Equations**\n",
        "Setting the above derivative to zero, we obtain the normal equations:\n",
        "$$(X^\\top X)\\theta = X^\\top y$$\n",
        "Hence, the value $\\theta^*$ that minimizes this objective is given by:\n",
        "$$\\theta^* = (X^\\top X)^{-1}X^\\top y.$$\n",
        "Note that, we assumed that the matrix $X^\\top X$ is invertible; if this is not the case, there are easy ways of addressing this issue."
      ],
      "metadata": {
        "id": "Zl_HzlZqeql3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's apply the normal equations"
      ],
      "metadata": {
        "id": "kkAO8CcRgKoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "theta_best = np.linalg.inv(X_train.T.dot(X_train)).dot(X_train.T).dot(y_train)\n",
        "theta_best_df = pd.DataFrame(data=theta_best[np.newaxis,:], columns=X.columns)\n",
        "theta_best_df"
      ],
      "metadata": {
        "id": "tB84beKRXEsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now use our estimate of theta to compute predictions for 3 new data points."
      ],
      "metadata": {
        "id": "p_3-tZz3hYpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect 3 new data points\n",
        "X_test = X.iloc[:3]\n",
        "y_test = y.iloc[:3]\n",
        "\n",
        "# Generate predictions on the new patients\n",
        "y_test_pred = X_test.dot(theta_best)"
      ],
      "metadata": {
        "id": "FLDgabh-hCgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the predictions"
      ],
      "metadata": {
        "id": "FiluJzdLh8e1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the result\n",
        "plt.xlabel('Body Mass Index (BMI)')\n",
        "plt.ylabel('Diabetes Risk')\n",
        "plt.scatter(X_train.loc[:,['bmi']], y_train)\n",
        "plt.scatter(X_test.loc[:,['bmi']], y_test, color='red', marker='o')\n",
        "plt.plot(X_test.loc[:,['bmi']], y_test_pred, 'x', color='red', mew=3, markersize=8)\n",
        "plt.legend([ 'Initial Patients', 'New Patients', 'Predictions'])"
      ],
      "metadata": {
        "id": "mwCFP6BLh_JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Algorithm: Ordinary Least Squares**\n",
        "\n",
        "\n",
        "*   **Type**: Supervised Learning (regression)\n",
        "*   **Model Family**: Linear Models\n",
        "*   **Objective function**: Mean Squared Error\n",
        "*   **Optimization**: Normal Equations\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MwGUIdE2jYw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 4: Non-Linear Least Squares**"
      ],
      "metadata": {
        "id": "cyo32C0Ukegw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we have learned about a very simple linear model. These can only capture the simple linear relationships in the data. How can we use what we learned so far to model more complex relationships? \\\\\n",
        "We will now see a simple approach to model complex non-linear relatioships called *least squared*"
      ],
      "metadata": {
        "id": "TmYNP70JkmBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Recall: Polynomial functions**\n",
        "Recall that a polynomial function of degree $p$ is a function of the form:\n",
        "$$a_px^p + a_{p-1}x^{p-1} + \\dots + a_1x^1 + a_0$$\n",
        "\n",
        "Below are some examples of polynomial functions "
      ],
      "metadata": {
        "id": "RG8ek6NelPRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.figure(figsize=(16,4))\n",
        "x_vars = np.linspace(-2,2)\n",
        "\n",
        "plt.subplot(131)\n",
        "plt.title(\"Quadratic Function\")\n",
        "plt.plot(x_vars, x_vars**2)\n",
        "plt.legend(['$x^2$'])\n",
        "\n",
        "plt.subplot(132)\n",
        "plt.title(\"Cubic Function\")\n",
        "plt.plot(x_vars, x_vars**3)\n",
        "plt.legend(['$x^3$'])\n",
        "\n",
        "plt.subplot(133)\n",
        "plt.title(\"Third degree polynomial\")\n",
        "plt.plot(x_vars,x_vars**3 + 2*x_vars**2 + x_vars + 1 )\n",
        "plt.legend(['$x^3 + 2x^2 + x + 1$'])"
      ],
      "metadata": {
        "id": "FpvIfQqKl60R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Modeling Non-Linear Relationships With Polynomial Regression**\n",
        "\n",
        "Specifically, given a one-dimensional continuous variable $x$, we can defining the feature function $\\phi : \\mathbb{R} \\to \\mathbb{R}^{p+1}$ as:\n",
        "$$\\phi(x) = \\begin{bmatrix}\n",
        "1 \\\\\n",
        "x \\\\\n",
        "x^2\\\\\n",
        "\\vdots \\\\\n",
        "x^p\n",
        "\\end{bmatrix}.$$\n",
        "\n",
        "The class of the model of form:\n",
        "$$f_{\\theta}(x) := \\sum_{j=0}^p\\theta_px_p = \\theta^\\top \\phi(x)$$\n",
        "\n",
        "with parameters $\\theta$ and polynomial feature $\\phi$ is the set of $p$-degree polynomials.\n",
        "\n",
        "*   This model is non-linear in the input variable $x$, meaning that we can model complex data relationships.\n",
        "*   It is a linear model as a function of the parameters $\\theta$, meaning that we can use ordinary least squares algorithm to learn these features.  \n",
        "\n"
      ],
      "metadata": {
        "id": "BIfFjbxIntoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The UCI Diabetes Dataset**"
      ],
      "metadata": {
        "id": "FsPWrOOgqPCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [8, 4]\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "\n",
        "# Load the dataset\n",
        "X, y = datasets.load_diabetes(return_X_y = True, as_frame=True)\n",
        "\n",
        "# Add an extra column of ones\n",
        "X['ones'] = 1\n",
        "\n",
        "# Collect 20 data points\n",
        "X_train = X.iloc[-20:]\n",
        "y_train = y.iloc[-20:]\n",
        "\n",
        "# Visualize the datapoint\n",
        "plt.scatter(X_train.loc[:,['bmi']], y_train, color='black')\n",
        "plt.xlabel('Body Mass Index (BMI)')\n",
        "plt.ylabel('Diabetes Risk')"
      ],
      "metadata": {
        "id": "2EzO2clynale"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Diabetes Dataset: A Non-Linear Featurization**\n",
        "Let's now obtain non-linear feature for this dataset. "
      ],
      "metadata": {
        "id": "2CKnr8YtrcqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_bmi = X_train.loc[:,['bmi']]\n",
        "\n",
        "X_bmi_p3 = pd.concat([X_bmi, X_bmi**2, X_bmi**3], axis = 1)\n",
        "X_bmi_p3.columns = ['bmi', 'bmi2', 'bmi3']\n",
        "X_bmi_p3['ones'] = 1\n",
        "X_bmi_p3.head()"
      ],
      "metadata": {
        "id": "dg-sI2ncrqAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_bmi_p3.T.shape"
      ],
      "metadata": {
        "id": "AB6no4WCznh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "0K-pCCZW0Eai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.linalg.inv(X_bmi_p3.T.dot(X_bmi_p3)).dot(X_bmi_p3.T).dot(y_train)"
      ],
      "metadata": {
        "id": "GfGg5hB1z2MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Diabetes Dataset: A Polynomial Model**\n",
        "By training a linear model on this featurization of this diabetes dataset, we can obtain a polynomial model of diabetes risk as a function of BMI."
      ],
      "metadata": {
        "id": "ZNyKghdZsSKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit a linear model\n",
        "theta = np.linalg.inv(X_bmi_p3.T.dot(X_bmi_p3)).dot(X_bmi_p3.T).dot(y_train)\n",
        "\n",
        "# Show the polynomial curve\n",
        "x_line = np.linspace(-0.1,0.1,10)\n",
        "x_line_p3 = np.stack([x_line, x_line**2, x_line**3, np.ones(10,)], axis = 1)\n",
        "y_train_pred = x_line_p3.dot(theta)\n",
        "\n",
        "plt.xlabel('Body Mass Index (BMI)')\n",
        "plt.ylabel('Diabetes Risk')\n",
        "plt.scatter(X_bmi, y_train)\n",
        "plt.plot(x_line, y_train_pred)"
      ],
      "metadata": {
        "id": "Rx3nWN4PsLX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Multivariate Polynomial Regression**\n",
        "We can also take this approach to constructs non-linear functions of multiple variabels by using multivariate polynomial.\\\n",
        "For example, a polynomial of degree $p$ over two variables $x_1, x_2$ is a function of form:\n",
        "$$a_{20}x_1^2 + a_{10}x_1 + a_{02}x_2^2 + a_{01}x_2 + a_{11}x_1x_2 + a_{00}$$\n",
        "\n",
        "In general, a polynomial of degree $p$ over two variables $x_1, x_2$ is the function of the form:\n",
        "\n",
        "$$f(x_1, x_2) = \\sum_{i,j \\geq 0, i + j \\leq p}a_{ij}x_1^ix_2^j$$\n",
        "\n",
        "In our two-dimensional example, this correspond to a feature function $\\phi : \\mathbb{R}^2 \\to \\mathbb{R}^p$ of the form \n",
        "$$\\phi(x) = \\begin{bmatrix}\n",
        "1 \\\\\n",
        "x_1 \\\\\n",
        "x_1^2 \\\\\n",
        "x_2 \\\\\n",
        "x_2^2 \\\\\n",
        "x_1x_2\n",
        "\\end{bmatrix}.$$\n",
        "\n",
        "The same approach to holds for polynomial of an degree and any number of variables."
      ],
      "metadata": {
        "id": "BbRGXQE_xSu0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Towards General Non-Linear Features**\n",
        "Any non-linear feature map $\\phi(x): \\mathbb{R}^d \\to \\mathbb{R}^p$ can be used in this way to general model of the form:\n",
        "$$f_{\\theta}(x) := \\theta^\\top \\phi(x)$$\n",
        "\n",
        "that is highly non-linear in $x$ but linear in $\\theta$.\n",
        "\n",
        "\\\n",
        "For example, here is a way of modeling complex periodic function via a sum of sines and cosines"
      ],
      "metadata": {
        "id": "7ZjMUUsj0cIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.figure(figsize=(16,4))\n",
        "x_vars = np.linspace(-5,5)\n",
        "\n",
        "plt.subplot(131)\n",
        "plt.title('Cosine function')\n",
        "plt.plot(x_vars, np.cos(x_vars))\n",
        "plt.legend(['Cos(x)'])\n",
        "\n",
        "plt.subplot(132)\n",
        "plt.title('Sine function')\n",
        "plt.plot(x_vars, np.sin(x_vars))\n",
        "plt.legend(['Sin(x)'])\n",
        "\n",
        "plt.subplot(133)\n",
        "plt.title('Combination of Sine and Cosine function')\n",
        "plt.plot(x_vars, np.cos(x_vars) + np.sin(2*x_vars) + np.cos(4*x_vars))\n",
        "plt.legend(['Cos(x) + Sin(2x) + Cos(4x)'])"
      ],
      "metadata": {
        "id": "g-Lt2u8qsF-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Algorithm: Non-Linear Least Squares**\n",
        "\n",
        "\n",
        "*   Type: Supervised Learning (Regression),\n",
        "*   Model family: Linear in the parameters, non-linear with respect to the raw inputs,\n",
        "*   Features: Non-linear functions of the attributes\n",
        "*   Objective function: Mean Squared Error\n",
        "*   Optimizer: Normal Equations\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KxD7j-Qa30VN"
      }
    }
  ]
}