{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 4: Foundations of Supervised Learning**\n",
        "## Applied Machine Learning"
      ],
      "metadata": {
        "id": "nvapfiX45Ucp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Why does Supervised Learning work?**\n",
        "Previously, we learned about supervised learning, derived our first algorithm, and used it to predict diabetes risk.\\\n",
        "\n",
        "In this lecture, we are going to dive deeper into why really supervised learning works."
      ],
      "metadata": {
        "id": "onRNRzB-5UaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 1: Data Distribution**\n",
        "First, let's look at the data, and define where it comes from.\\\n",
        "Later, this will be useful to precisely define when supervised learning guaranteed to work.\n",
        "\n"
      ],
      "metadata": {
        "id": "-kxHxnMT5UXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Components of A Supervised Machine Learning Problem**\n",
        "At a high level, a supervised machine learning problem has the following structure:\n",
        "$$\\underbrace{\\text{Training  Dataset}}_\\text{Features + Attributes} + \\underbrace{\\text{Learning Algorithm}}_\\text{Model class + Objective + Optimizer} \\to Predictive ~ Model$$\n",
        "\n",
        "Where does the data come from?"
      ],
      "metadata": {
        "id": "faiouZYw6sDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Distribution**\n",
        "We will assumed that the dataset is sampled from a probability distribution $\\mathbb{P}$, which we will call the *data distribution*. We will denote this as\n",
        "$$x, y \\sim \\mathbb{P}$$\n",
        "The training set $\\mathcal{D} = \\{(x^{(i}),y^{(i)} \\mid i = 1,2,\\dots, n \\}$ consists of independent and identically distributed (IID) samples from $\\mathbb{P}$."
      ],
      "metadata": {
        "id": "Gr7yIUvL7r7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Distribution: IID Sampling**\n",
        "The key assumption in that the training examples are independent and identically distributed (IID).\n",
        "\n",
        "*   Each training example is from the same distribution.\n",
        "*   This distribution does not depend on previous training example\n",
        "\n",
        "**Example:** Flipping a coin. Each flip has same probability of heads & tails and does not depend on the previous flip.\n",
        "\n",
        "**Counter-Example:** Yearly census data. The population on each year will be close to that of the previous year"
      ],
      "metadata": {
        "id": "58VIw7Hd9G3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Distribution: Example**"
      ],
      "metadata": {
        "id": "oO9baLuW-VcR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's implement an example of data distribution in numpy"
      ],
      "metadata": {
        "id": "ONJcie-f1plR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "def true_fn(X):\n",
        "  return np.cos(1.5*np.pi*X)\n"
      ],
      "metadata": {
        "id": "UE3MCThR-Uzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize it"
      ],
      "metadata": {
        "id": "auxhK19W2Oyz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDCApImK5ILY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [12,4]\n",
        "\n",
        "X_test = np.linspace(0, 1, 100)\n",
        "plt.plot(X_test, true_fn(X_test), label=\"True function\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now draw the sample from the distribution. We will generate random variable $x$, and then generate random $y$ using\n",
        "$$y = f(x) + \\epsilon$$ \n",
        "for a random noise $\\epsilon$"
      ],
      "metadata": {
        "id": "DkGY4pRW3gWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = 30\n",
        "\n",
        "X = np.sort(np.random.rand(n_samples))\n",
        "y = true_fn(X) + np.random.rand(n_samples)*0.1"
      ],
      "metadata": {
        "id": "Z7YdJTT52rtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize the samples"
      ],
      "metadata": {
        "id": "uR1ik4v54yiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(X_test, true_fn(X_test), label='True function')\n",
        "plt.scatter(X, y, edgecolor='b', s=20, label='Samples' )\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "amimMArs401O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Distribution: Motivation**\n",
        "Why assumed that the dataset is sampled from the same distribution?\n",
        "\n",
        "\n",
        "*   There is inherent uncertainty from the data. The data may consist of noisy measurements (readings from an imperfect thermometer),\n",
        "*   There is uncertainty in the process we model, if $y$ is a stock price, there is randomness in the market that cannot be modeled,\n",
        "*   We can use probability and statistics to analyze supervised machine learning algorithm and prove that they work. \n",
        "\n"
      ],
      "metadata": {
        "id": "D4WR4AVj5YAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![cornell_tech2.svg](data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iNTAwcHgiIGhlaWdodD0iNTAwcHgiIHZpZXdCb3g9IjAgMCA1MDAgNTAwIiB2ZXJzaW9uPSIxLjEiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgeG1sbnM6eGxpbms9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkveGxpbmsiPgogICAgPCEtLSBHZW5lcmF0b3I6IFNrZXRjaCA0MC4xICgzMzgwNCkgLSBodHRwOi8vd3d3LmJvaGVtaWFuY29kaW5nLmNvbS9za2V0Y2ggLS0+CiAgICA8dGl0bGU+cmVkPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGRlZnM+PC9kZWZzPgogICAgPGcgaWQ9InR3aXN0ZWQtdCIgc3Ryb2tlPSJub25lIiBzdHJva2Utd2lkdGg9IjEiIGZpbGw9Im5vbmUiIGZpbGwtcnVsZT0iZXZlbm9kZCI+CiAgICAgICAgPGcgaWQ9InJlZCI+CiAgICAgICAgICAgIDxnIGlkPSJncm91cCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoNzcuMDAwMDAwLCAzMC4wMDAwMDApIj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0yNjcuMzE3ODExLDAgTDM0NS4yOTYzMzcsNDMuMTcxMTg3OSBMMzQ1LjI5NjMzNywxMzcuMDU1NTE4IEwyNjcuMzE3ODExLDkzLjk5NTc4NzcgTDI2Ny4zMTc4MTEsMCBaIE0zMzkuMjgwODUsMTQ2LjM4MDc5MiBMMjYxLjk3MDcxMiwxODkuMDY4OTk3IEwyNjEuMDQyMzk3LDE4OC41NDg4NjMgTDE4NC40NzQ5MTEsMTQ2LjM0MzYzOSBMMjYxLjc0NzkxNywxMDMuNjE4MjgxIEwzMzkuMjgwODUsMTQ2LjM4MDc5MiBaIE04My44NDU0ODEsMjg3LjMzNzMyMSBMNi4xMjY4ODQxMywyNDQuNzk3NzI1IEw3OC43NTgzMTA2LDIwNC42NzMwNDEgTDc5LjEyOTYzNjksMjA3LjM4NTE3MyBDODEuNTA2MTI1MywyMjQuNDM4MTYzIDg4LjMwMTM5NjgsMjQ1LjU3NzkyOCAxMDEuMTQ5Mjg3LDI3NS44OTQzNTYgTDEwMS44MTc2NzQsMjc3LjQ1NDc2IEw4My44NDU0ODEsMjg3LjMzNzMyMSBaIE04OS4zNzgyNDMxLDM5Ny45NDAyNyBMODkuMzc4MjQzMSwzOTYuNzUxMzkgQzkwLjg2MzU0ODMsMzc0LjQ1OTg5OSAxMDQuMjMxMjk2LDM0MC4zNTM5MTcgMTE1LjM3MTA4NSwzMTUuNjEwMzYyIEwxMTcuMDc5MTg2LDMxMS44OTUxMTQgTDEyMS43NTc4OTcsMzIyLjAzNzc0MiBDMTM1LjQ5Njk3MSwzNTEuNzU5NzMgMTYxLjExODQ4Niw0MDcuMjI4MzkxIDE2NS44NzE0NjMsNDM2LjY1MzE1OSBMMTY2LjUwMjcxOCw0NDAuMzY4NDA4IEw4OS4zNzgyNDMxLDM5Ny45NDAyNyBaIE0yNTYuMTc4MDIyLDM5Ni43MTQyMzggTDI1Ni4xNzgwMjIsMzk3LjkwMzExNyBMMTc3LjgyODE3LDQ0MSBMMTc3LjQ1Njg0NCw0MzguMjg3ODY5IEMxNzMuMTEyMzI2LDQwNy4zMzk4NDggMTUwLjk4MTI3OCwzNTcuNDQ0MDYxIDEyNy42OTkxMTgsMzA4LjI1NDE3IEM4OS4wMDY5MTY4LDIyNi4yMjE0ODMgODkuMTkyNTc5OSwyMDQuNzg0NDk5IDg5LjM0MTExMDQsMTg2LjA1OTY0NiBMODkuMzQxMTEwNCwxODYuMDU5NjQ2IEwwLDIzNS4yNDk1MzcgTDAsMTQxLjY2MjQyNiBMMjU2LjIxNTE1NSwwLjA3NDMwNDk3MDUgTDI1Ni4yMTUxNTUsOTMuODg0MzMwMiBMMTY3LjA5Njg0LDE0My4wMzcwNjggTDE2Ny4wOTY4NCwxNDQuMDQwMTg1IEMxNjYuODc0MDQ0LDE1NS42MzE3NjEgMTY2LjM5MTMyLDE3OS44MTgwMjkgMjEyLjIxMjk4NywyNzYuMDA1ODEzIEMyNDAuNjkzNzE1LDMzNS43MDk4NTcgMjU0LjY1NTU4NCwzNzQuMDUxMjIyIDI1Ni4xNzgwMjIsMzk2Ljc1MTM5IEwyNTYuMTc4MDIyLDM5Ni43MTQyMzggWiBNMjMwLjE4NTE4LDI4MC43MjQxNzkgTDIyOC4zMjg1NDksMjg0LjczNjY0NyBMMjI2LjU4MzMxNSwyODAuNzYxMzMxIEMyMjQuOTQ5NDc5LDI3Ny4wNDYwODMgMjIyLjQyNDQ2LDI3MS41MTAzNjIgMjE5LjM3OTU4NSwyNjQuOTM0MzcyIEMyMDYuMDQ4OTcsMjM1Ljk1NTQzNCAxODMuNzY5MzkxLDE4Ny41MDg1OTMgMTc5LjcyMTkzNSwxNjAuMDkwMDU5IEwxNzkuMTY0OTQ1LDE1Ni4zNzQ4MSBMMjU2LjE3ODAyMiwxOTguNzI4NjQ0IEwyNTYuMTc4MDIyLDE5OS45NTQ2NzYgQzI1NC4zMjEzOSwyMjIuNjkxOTk3IDI0MS4zOTkyMzUsMjU1LjA4ODk2NCAyMzAuMTg1MTgsMjgwLjc2MTMzMSBMMjMwLjE4NTE4LDI4MC43MjQxNzkgWiIgaWQ9IlNoYXBlIiBmaWxsPSIjQUIxNjJDIj48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMjY3LjMxNzgxMSwwIEwzNDUuMjk2MzM3LDQzLjE3MTE4NzkgTDM0NS4yOTYzMzcsMTM3LjA1NTUxOCBMMjY3LjMxNzgxMSw5My45OTU3ODc3IEwyNjcuMzE3ODExLDAgWiBNMzM5LjI4MDg1LDE0Ni4zODA3OTIgTDI2MS45NzA3MTIsMTg5LjA2ODk5NyBMMjYxLjA0MjM5NywxODguNTQ4ODYzIEwxODQuNDc0OTExLDE0Ni4zNDM2MzkgTDI2MS43NDc5MTcsMTAzLjYxODI4MSBMMzM5LjI4MDg1LDE0Ni4zODA3OTIgWiBNODMuODQ1NDgxLDI4Ny4zMzczMjEgTDYuMTI2ODg0MTMsMjQ0Ljc5NzcyNSBMNzguNzU4MzEwNiwyMDQuNjczMDQxIEw3OS4xMjk2MzY5LDIwNy4zODUxNzMgQzgxLjUwNjEyNTMsMjI0LjQzODE2MyA4OC4zMDEzOTY4LDI0NS41Nzc5MjggMTAxLjE0OTI4NywyNzUuODk0MzU2IEwxMDEuODE3Njc0LDI3Ny40NTQ3NiBMODMuODQ1NDgxLDI4Ny4zMzczMjEgWiBNODkuMzc4MjQzMSwzOTcuOTQwMjcgTDg5LjM3ODI0MzEsMzk2Ljc1MTM5IEM5MC44NjM1NDgzLDM3NC40NTk4OTkgMTA0LjIzMTI5NiwzNDAuMzUzOTE3IDExNS4zNzEwODUsMzE1LjYxMDM2MiBMMTE3LjA3OTE4NiwzMTEuODk1MTE0IEwxMjEuNzU3ODk3LDMyMi4wMzc3NDIgQzEzNS40OTY5NzEsMzUxLjc1OTczIDE2MS4xMTg0ODYsNDA3LjIyODM5MSAxNjUuODcxNDYzLDQzNi42NTMxNTkgTDE2Ni41MDI3MTgsNDQwLjM2ODQwOCBMODkuMzc4MjQzMSwzOTcuOTQwMjcgWiBNMjU2LjE3ODAyMiwzOTYuNzE0MjM4IEwyNTYuMTc4MDIyLDM5Ny45MDMxMTcgTDE3Ny44MjgxNyw0NDEgTDE3Ny40NTY4NDQsNDM4LjI4Nzg2OSBDMTczLjExMjMyNiw0MDcuMzM5ODQ4IDE1MC45ODEyNzgsMzU3LjQ0NDA2MSAxMjcuNjk5MTE4LDMwOC4yNTQxNyBDODkuMDA2OTE2OCwyMjYuMjIxNDgzIDg5LjE5MjU3OTksMjA0Ljc4NDQ5OSA4OS4zNDExMTA0LDE4Ni4wNTk2NDYgTDg5LjM0MTExMDQsMTg2LjA1OTY0NiBMMCwyMzUuMjQ5NTM3IEwwLDE0MS42NjI0MjYgTDI1Ni4yMTUxNTUsMC4wNzQzMDQ5NzA1IEwyNTYuMjE1MTU1LDkzLjg4NDMzMDIgTDE2Ny4wOTY4NCwxNDMuMDM3MDY4IEwxNjcuMDk2ODQsMTQ0LjA0MDE4NSBDMTY2Ljg3NDA0NCwxNTUuNjMxNzYxIDE2Ni4zOTEzMiwxNzkuODE4MDI5IDIxMi4yMTI5ODcsMjc2LjAwNTgxMyBDMjQwLjY5MzcxNSwzMzUuNzA5ODU3IDI1NC42NTU1ODQsMzc0LjA1MTIyMiAyNTYuMTc4MDIyLDM5Ni43NTEzOSBMMjU2LjE3ODAyMiwzOTYuNzE0MjM4IFogTTIzMC4xODUxOCwyODAuNzI0MTc5IEwyMjguMzI4NTQ5LDI4NC43MzY2NDcgTDIyNi41ODMzMTUsMjgwLjc2MTMzMSBDMjI0Ljk0OTQ3OSwyNzcuMDQ2MDgzIDIyMi40MjQ0NiwyNzEuNTEwMzYyIDIxOS4zNzk1ODUsMjY0LjkzNDM3MiBDMjA2LjA0ODk3LDIzNS45NTU0MzQgMTgzLjc2OTM5MSwxODcuNTA4NTkzIDE3OS43MjE5MzUsMTYwLjA5MDA1OSBMMTc5LjE2NDk0NSwxNTYuMzc0ODEgTDI1Ni4xNzgwMjIsMTk4LjcyODY0NCBMMjU2LjE3ODAyMiwxOTkuOTU0Njc2IEMyNTQuMzIxMzksMjIyLjY5MTk5NyAyNDEuMzk5MjM1LDI1NS4wODg5NjQgMjMwLjE4NTE4LDI4MC43NjEzMzEgTDIzMC4xODUxOCwyODAuNzI0MTc5IFoiIGlkPSJTaGFwZSI+PC9wYXRoPgogICAgICAgICAgICA8L2c+CiAgICAgICAgPC9nPgogICAgPC9nPgo8L3N2Zz4=)\n",
        "\n",
        "## **Part 2: Why Does Supervised Learning Work?**\n",
        "\n",
        "We made an assumption that the training dataset is sampled from a data distribution.\n",
        "\n",
        "Let's now use it to gain intuition about why supervised learning works"
      ],
      "metadata": {
        "id": "KHox0lvu-Jnl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Data distribution**\n",
        "\n",
        "We will assumed that the dataset is sampled from a probability distribution $\\mathbb{P}$, which we will call the *data distribution*. We will denote this as\n",
        "$$x, y \\sim \\mathbb{P}$$\n",
        "The training set $\\mathcal{D} = \\{(x^{(i}),y^{(i)}) \\mid i = 1,2,\\dots, n \\}$ consists of independent and identically distributed (IID) samples from $\\mathbb{P}$."
      ],
      "metadata": {
        "id": "o8ZJ1-gjC8_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Supervised Learning Model**\n",
        "We'll say that a model is a function\n",
        "$$f: \\mathcal{X} \\to \\mathcal{Y}$$\n",
        "\n",
        "that maps inputs $x \\in \\mathcal{X}$ to targets $y \\in \\mathcal{Y}$"
      ],
      "metadata": {
        "id": "CF7rnPjXDby0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What Makes A Good Model?**\n",
        "There are several things we may want out of a good model.\n",
        "\n",
        "1.   Interpretable features that explain how $x$ affects \n",
        "$y$,\n",
        "2.   Confidence intervals around y (we will see later how to obtain these),\n",
        "3. Accurate predictions of the targets $y$ from inputs $x$.\n",
        "\n",
        "In this lecture, we will focus on the latter.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DG4mSCJFDzVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What Makes A Good Model?**\n",
        "A good predictive model is one that makes **accurate prediction** on **new data** that it has not seen at training time."
      ],
      "metadata": {
        "id": "9frI8kZBE32W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hold-out Dataset: Definition**\n",
        "A hold-out dataset:\n",
        "$$\\dot{\\mathcal{D}} = \\{(\\dot{x}^{(i)},\\dot{y}^{(i)})\\mid i = 1,2,\\dots,m\\}$$\n",
        "is another dataset that is sampled IID from the same distribution $\\mathbb{P}$ as the training dataset $\\mathcal{D}$ and the two datasets are disjoint."
      ],
      "metadata": {
        "id": "dStjNd3FFmEb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's generate a hold-out dataset for the example we saw earlier"
      ],
      "metadata": {
        "id": "dJdnDvWsGd9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(0)\n",
        "\n",
        "def true_fn(X):\n",
        "    return np.cos(1.5 * np.pi * X)\n",
        "\n",
        "X_test = np.linspace(0, 1, 100)\n",
        "plt.plot(X_test, true_fn(X_test), label=\"True function\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "pG1XsLmz5REh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples, n_holdout_samples = 30, 30\n",
        "\n",
        "X = np.sort(np.random.rand(n_samples))\n",
        "y = true_fn(X) + np.random.rand(n_samples)*0.1\n",
        "X_holdout = np.sort(np.random.rand(n_holdout_samples))\n",
        "y_holdout = true_fn(X_holdout) + np.random.rand(n_holdout_samples)*0.1\n",
        "\n",
        "plt.plot(X_test, true_fn(X_test), label='True Function')\n",
        "plt.scatter(X,y, edgecolor='b', s=20, label='Samples')\n",
        "plt.scatter(X_holdout, y_holdout, edgecolor='r', s=20, label='Holdout Samples')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "PaUPWCjuGuC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Defining What is an Accurate Prediction**\n",
        "Suppose that we have a function $isaccurate(y,y')$ that determines if $y$ is an accurate estimate of $y'$, e.g.:\n",
        "\n",
        "\n",
        "*   Is the target variable is close enough to the true target\n",
        "$$isaccurate(y,y') = true ~ if (|y - y'| ~ is ~ small) ~ else ~ false$$\n",
        "*   Did we predict the right class?\n",
        "$$isaccurate(y,y')=true ~ if (y=y')~ else ~false$$\n",
        "\n",
        "This defines accuracy on tha data point. We say a supervised learning model is accurate if it correctly predicts the target on new (held-out) data. "
      ],
      "metadata": {
        "id": "suRS_4OUIOYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Defining What is an Accurate Model**\n",
        "We can say that a predictive model $f$ is accurate if it has probability of making error on a random hold-out sample is small\n",
        "$$1 - \\mathbb{P}[isaccurate(\\dot{y}, f(\\dot{x}))] \\leq \\epsilon$$\n",
        "for $\\dot{x}, \\dot{y} \\sim \\mathbb{P}$, for some small $\\epsilon > 0$ and some definition of accuracy.\n",
        "\n",
        "We can also say that a predictive model $f$ is inaccurate if it has probability of making error on a random holdout sample is large\n",
        "$$1 - \\mathbb{P}[isaccurate(\\dot{y}, f(\\dot{x}))] \\geq \\epsilon$$\n",
        "Equivalently,\n",
        "$$\\mathbb{P}[isaccurate(\\dot{y}, f(\\dot{x}))] \\leq 1 - \\epsilon$$"
      ],
      "metadata": {
        "id": "Fo3X-tZJJuX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Generalization**\n",
        "In machine learning, **generalization** is the property of the predictive model to achieve good performance on new, heldout data that distinct from the training set.\n",
        "\n",
        "Will supervised learning return a model that generalizes?"
      ],
      "metadata": {
        "id": "V6BeXk2ILoNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Recall: Supervised Learning**\n",
        "Recall out ituitive definition of supervised learning\n",
        "\n",
        "\n",
        "1.   First, we collect a dataset of labeled training examples.\n",
        "2.   We train a model to output accurate predictions on this dataset.\n",
        "3. When the model see new, similar data, it will also be accurate.\n"
      ],
      "metadata": {
        "id": "ViUN2KqvNMun"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Recall: Supervised Learning**\n",
        "Recall that supervised learning at high level performs the following procedure:\n",
        "\n",
        "\n",
        "\n",
        "1.   Collect the training dataset $\\mathcal{D}$  of labeled examples,\n",
        "2.   Output a model that is accurate on $\\mathcal{D}$\n",
        "\n",
        "I claim that the output model is also guaranteed to generalize if $\\mathcal{D}$ is large enough.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vS4ZHOoJNM0B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Applying Supervised Learning**\n",
        "In oder to prove that supervised learning works, we will make two simplifying assumptions:\n",
        "\n",
        "1.   We define a model class $\\mathcal{M}$ containing $H$ different models.\n",
        "2.   One of these models fits the training dataset perfectly (is accurate on every datapoint) and we choose that model.\n",
        "\n",
        "(Both of these assumptions are relaxed)\n",
        "\n"
      ],
      "metadata": {
        "id": "G0eNW7LJNNT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Why Supervised Learning Works**\n",
        "**Claim:** The probability that supervised learning will return an inaccurate model decrease exponentially with training set size $n$\n",
        "\n",
        "1.   A model $f$ is inaccurate if $\\mathbb{P}[isinaccurate(\\dot{y}, f(\\dot{x})] \\leq 1 - \\epsilon$. The probability that an inaccurate model $f$ fits perfectly the training set is at most $\\prod_{i=1}^n\\mathbb{P}[isinaccurate(y^{(i)}, f(x^{(i)})] \\leq (1 - \\epsilon)^n$\n",
        "2.   We have $H$ models in  $\\mathcal{M}$, and any of them could be inaccurate. The probability that at least one the at most $H$ inaccurate models will fit the training set perfectly is $\\leq H(1 - \\epsilon)^n$.\n",
        "\n",
        "Therefore, the claim holds.\n",
        "\n"
      ],
      "metadata": {
        "id": "eblLgyi7PxRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 3: Underfitting and Overfitting**\n",
        "Let's now dive deeper into the concept of generalizaation and two possible failure modes of supervised learning: overfitting and underfitting "
      ],
      "metadata": {
        "id": "W1lgMih0TMi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Generalization**\n",
        "We will assume that the dataset is governed by a probability distribution $\\mathbb{P]$, which we will call the *data distribution*. We will denote this as\n",
        "$$x, y \\sim \\mathbb{P}$$\n",
        "\n",
        "A hold-out set $\\dot{\\mathcal{D}} = \\{(\\dot{x}^{(i)}, \\dot{y}^{(i)}) \\mid i= 1,2, \\dots, m\\}$ consists of independent and identically distributed (IID) samples from $\\mathbb{P}$ and is distinct from the training set.\n",
        "\n",
        "The model that **generalizes** is accurate on a hold-out dataset"
      ],
      "metadata": {
        "id": "Sq8hf08YTMwp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Polynomial Regression**\n",
        "In 1D polynomial regression, we fit a model\n",
        "$$f_{\\theta}(x) := \\theta^\\top \\phi(x)$$\n",
        "that is linear in $\\theta$ but non-linear in $x$ because the features $\\phi(x) : \\mathbb{R} \\to \\mathbb{R}^p$ is non-linear.\n",
        "\n",
        "By using polynomial features such as $\\phi(x) = [1\\; x\\; x^2\\; \\dots\\; x^p]$, we can fit any polynomial of degree $p$"
      ],
      "metadata": {
        "id": "dRJa1SNHWAuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Polynomial Better Fit the Data**\n",
        "When we switch from linear models to polynomials, we can better fit the data and increae the accuracy of our models."
      ],
      "metadata": {
        "id": "H1ik9ZfwXWpK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the synthetic dataset we have seen earlier"
      ],
      "metadata": {
        "id": "yVOK7Js8XqcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "np.random.seed(0)\n",
        "n_samples = 30 \n",
        "X = np.sort(np.random.rand(n_samples))\n",
        "y = true_fn(X) + np.random.rand(n_samples)*0.1\n",
        "\n",
        "X_test = np.linspace(0, 1, 100)\n",
        "plt.plot(X_test, true_fn(X_test), label='True Functions')\n",
        "plt.scatter(X, y, edgecolor='b', s=20, label='Samples')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "TV-YF7scH7Lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although fitting a linear model does not work well, quadratic or cubic polynomials improve the fit."
      ],
      "metadata": {
        "id": "k_SPh09zZKNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "degrees = [1, 2, 3]\n",
        "plt.figure(figsize = (14,5))\n",
        "for i in range(len(degrees)):\n",
        "  ax = plt.subplot(1, len(degrees), i+1)\n",
        "\n",
        "  polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias = False)\n",
        "  linear_regression = LinearRegression()\n",
        "  pipeline = Pipeline([(\"pf\", polynomial_features), ('lr', linear_regression)])\n",
        "  pipeline.fit(X[:, np.newaxis], y )\n",
        "\n",
        "  ax.plot(X_test, true_fn(X_test), label='True Functions')\n",
        "  ax.plot(X_test, pipeline.predict(X_test[:,np.newaxis]), label='Model')\n",
        "  ax.scatter(X, y, edgecolor='b', s=20, label='Samples')\n",
        "  ax.set_xlim((0,1))\n",
        "  ax.set_ylim((-2,2))\n",
        "  ax.legend(loc='best')\n",
        "  ax.set_title(\"Polynomial of Degree {}\".format(degrees[i]))\n"
      ],
      "metadata": {
        "id": "ewtuh_orY9_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Towards Higher-Degree Polynomial Features?**\n",
        "As we increase the complexity of our model class $\\mathcal{M}$ to even higher degree polynomials, we are able to fit the data increasingly even better.\n",
        "\n",
        "What happen if we increse the degree of the polynomial?"
      ],
      "metadata": {
        "id": "2UBUfGq6c3Hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "degrees = [30]\n",
        "plt.figure(figsize = (14,5))\n",
        "for i in range(len(degrees)):\n",
        "  ax = plt.subplot(1, len(degrees), i+1)\n",
        "\n",
        "  polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias = False)\n",
        "  linear_regression = LinearRegression()\n",
        "  pipeline = Pipeline([(\"pf\", polynomial_features), ('lr', linear_regression)])\n",
        "  pipeline.fit(X[:, np.newaxis], y )\n",
        "\n",
        "  ax.plot(X_test, true_fn(X_test), label='True Functions')\n",
        "  ax.plot(X_test, pipeline.predict(X_test[:,np.newaxis]), label='Model')\n",
        "  ax.scatter(X, y, edgecolor='b', s=20, label='Samples')\n",
        "  ax.set_xlim((0,1))\n",
        "  ax.set_ylim((-2,2))\n",
        "  ax.legend(loc='best')\n",
        "  ax.set_title(\"Polynomial of Degree {}\".format(degrees[i]))"
      ],
      "metadata": {
        "id": "tNL-BwH3d6Ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The Problem With Increasing Model Capaity**\n",
        "\n",
        "As the degree of polynomial increases to the size of the dataset, we are increasingly able to fit every point in the dataset.\\\n",
        "\n",
        "However, this results in highly an irregular curve: its behaviour outside the training set is wildly inaccurate.\n"
      ],
      "metadata": {
        "id": "9y_OKw7seSaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Overfitting**\n",
        "Overfitting is one of the most common failure modes of machine learning:\n",
        "\n",
        "*   A very expressive model (a high degree polynomial) fits the training dataset perfectly.\n",
        "*   The model also makes incorrect predictions outside the training set, and does not generalize.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M5yKcbWCe-pd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Underfitting**\n",
        "A related failure mode is underfitting:\n",
        "\n",
        "\n",
        "*   A small model (e.g., a straight line) will not fit the training data well\n",
        "*   Held-out data is similar to training data, so it will not accurate either.\n",
        "\n",
        "Finding the trade off between  overfitting and underfitting is one of the main challenges in applying machine learning.\n",
        "\n"
      ],
      "metadata": {
        "id": "xqI-17z2flXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Overfitting vs. Underfitting: Evaluation**\n",
        "We can measure the overfitting and underfitting by estimating accuracy on held-out dataset and comparing it to the training dataset\n",
        "\n",
        "*   If training performance is high but held-out performance is low, we are overfitting\n",
        "*   if the performance in both training and held-out dataset are low, we are underfitting\n",
        "\n"
      ],
      "metadata": {
        "id": "z_lICjhJgcK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "degrees = [1,40,5]\n",
        "title = ['Underfitting', 'Overfitting', 'Appropriate Capacity']\n",
        "plt.figure(figsize=(14,5))\n",
        "for i in range(len(degrees)):\n",
        "  ax = plt.subplot(1, len(degrees), i+1)\n",
        "\n",
        "  polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias = False)\n",
        "  linear_regression = LinearRegression()\n",
        "  pipeline = Pipeline([(\"pf\", polynomial_features), ('lr', linear_regression)])\n",
        "  pipeline.fit(X[:, np.newaxis], y )\n",
        "\n",
        "  ax.plot(X_test, true_fn(X_test), label='True Functions')\n",
        "  ax.plot(X_test, pipeline.predict(X_test[:,np.newaxis]), label='Model')\n",
        "  ax.scatter(X, y, edgecolor='b', s=20, label='Samples', alpha=0.2)\n",
        "  ax.scatter(X_holdout[::3], y_holdout[::3], edgecolor= 'r', s=20, label='Samples')\n",
        "  ax.set_xlim((0,1))\n",
        "  ax.set_ylim((-2,2))\n",
        "  ax.legend(loc='best')\n",
        "  ax.set_title(\"{} (Degrees {})\".format(title[i], degrees[i]))\n",
        "  ax.text(0.05, -1.7, 'Hold-out MSE: %.4f' %((y_holdout - pipeline.predict(X_holdout[:,np.newaxis]))**2).mean())"
      ],
      "metadata": {
        "id": "xT2aMEFPhjFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dealing with Underfitting**\n",
        "Balancing ovefitting vs. underfitting is a major challenges in applying machine learning. Briefly, here is some approaches:\n",
        "*   To fight underfitting, we may increase our model class to encompass more expressive model.\n",
        "*   We may also create more richer features for the data that will make the dataset easier to fit.\n",
        "\n"
      ],
      "metadata": {
        "id": "CdzSMPqckAJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dealing with Overfitting**\n",
        "We will see many ways to deal with overfitting. but these are some ideas:\n",
        "\n",
        "*   We may reduce the complexity of our model by reducing the size of $\\mathcal{M}$.\n",
        "*   We may also modify our objective to penalize complex models that may overfit our data\n",
        "\n"
      ],
      "metadata": {
        "id": "oK-8s_6_kxpw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 4: Regularization**\n",
        "We will now see a very important way to reduce overfitting - regularization. We will see several important new algorithms."
      ],
      "metadata": {
        "id": "-_K7oEG1dyGK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Generalization**\n",
        "We will assume that the dataset is governed by a probability distribution $\\mathbb{P}$, which we will call the *data distribution*. We will denote this as\n",
        "$$x, y \\sim \\mathbb{P}.$$\n",
        "\n",
        "A hold-out set $\\dot{\\mathcal{D}} = \\{(\\dot{x^{(i)}}, \\dot{y^{(i)}}) \\mid i = 1,2,...,n\\}$ consists of *independent and identically distributed* (IID) samples from $\\mathbb{P}$ and is distinct from the training set."
      ],
      "metadata": {
        "id": "Xepu7M1Pevf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Overfitting**\n",
        "Overfitting is one of the most common failure modes of machine learning.\n",
        "* A very expressive model (a high degree polynomial) fits the training dataset perfectly.\n",
        "* The model also makes wildly incorrect prediction outside this dataset, and doesn't generalize."
      ],
      "metadata": {
        "id": "fTEqV-onfHwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will visualize overfitting by trying to fit small dataset with high degree polynomial."
      ],
      "metadata": {
        "id": "Xq_08ig8fXaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "degrees = [30]\n",
        "plt.figure(figsize=(14, 5))\n",
        "for i in range(len(degrees)):\n",
        "    ax = plt.subplot(1, len(degrees), i + 1)\n",
        "\n",
        "    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)\n",
        "    linear_regression = LinearRegression()\n",
        "    pipeline = Pipeline([(\"pf\", polynomial_features), (\"lr\", linear_regression)])\n",
        "    pipeline.fit(X[:, np.newaxis], y)\n",
        "\n",
        "    X_test = np.linspace(0, 1, 100)\n",
        "    ax.plot(X_test, true_fn(X_test), label=\"True function\")    \n",
        "    ax.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
        "    ax.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
        "    ax.set_xlim((0, 1))\n",
        "    ax.set_ylim((-2, 2))\n",
        "    ax.legend(loc=\"best\")\n",
        "    ax.set_title(\"Polynomial of Degree {}\".format(degrees[i]))"
      ],
      "metadata": {
        "id": "j-jsRL2qg_75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Regularization: Intuition**\n",
        "The idea of regularization is to penalty complex models that may overfit the data.\\\n",
        "In the previous example, a less complex would reply less on the polynomials terms of high degree."
      ],
      "metadata": {
        "id": "R9fVeIhFhqqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Regularization: Definition**\n",
        "The idea of regularization is to train models with an augmented objective $J : \\mathcal{M} \\to \\mathbb{R}$ defined over a training dataset $\\mathcal{D}$ of size n as\n",
        "$$J(f) = \\frac{1}{n}\\sum_{i=1}^nL(y^{(i)}, f(x^{(i)})) + λ\\cdot R(f)$$\n",
        "Let's dissect components of this objective:\n",
        "\n",
        "\n",
        "*   A loss function $L(y, f(x))$ such as a mean squared error.\n",
        "\n",
        "*   A regularizer $R: \\mathcal{M} \\to \\mathbb{R}$ that penalizes models that overly complex.\n",
        "*   A regularization parameter $\\lambda> 0$, which controls the strength of the regularizer\n",
        "\n",
        "When the model $f_{\\theta}$ is parametrized by $\\theta$, we can also use the following notation:\n",
        "$$J(\\theta) = \\frac{1}{n}\\sum_{i=1}^nL(y^{(i)}, f_{\\theta}(x^{(i)})) + λ\\cdot R(\\theta)$$\n"
      ],
      "metadata": {
        "id": "Ts4PhTTFhq0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **L2 Regularization: Definition**\n",
        "How can we define a regularizer $R: \\mathcal{M} \\to \\mathbb{R}$ to control the complexity of a model $f \\in \\mathcal{M}$?\n",
        "\n",
        "In the context of linear models $f(x) = \\theta^\\top x$, a widely used approache is L2 regularization, which defines following objectives:\n",
        "$$J(\\theta) = \\frac{1}{n}\\sum_{i=1}^nL(y^{(i)}, \\theta^\\top x^{(i)}) + \\frac{λ}{2}\\cdot||\\theta||_2^2. $$\n",
        "\n",
        "Let's dissect the components of this objective:\n",
        "\n",
        "\n",
        "\n",
        "*   The regularizer $R:\\mathcal{M} \\to \\mathbb{R}$ is the function $R(\\theta) = ||\\theta||_2^2 = \\sum_{i=1}^d\\theta_i^2$. This is also known as the L2 norm of $\\theta$.\n",
        "\n",
        "*   The regularizer penalizes large parameters. This prevents us from over-relying on any single feature and penalizes wildly irregular solutions.\n",
        "*   L2 regularization can be used with most models (linear, neural, etc.)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WVYpxvb-pFdl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **L2 Regularization for Polynomial Regression**\n",
        "Let's consider a applocation to the polynomial model we have seen so far. Given the polynomial feature $\\phi(x)$, we optimize the following objective:\n",
        "$$J(\\theta) = \\sum_{j=1}^d(y^{(i)} - \\theta^\\top \\phi(x^{(i)})) + \\frac{\\lambda}{2}||\\theta||^2_2.$$\n",
        "\n",
        "We are going to implement regularized and standard polynomial regression on three random training sets sampled from the same distribution."
      ],
      "metadata": {
        "id": "JnatGqkkszaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "degrees = [15,15,15]\n",
        "plt.figure(figsize=(14,5))\n",
        "for idx,i in enumerate(range(len(degrees))):\n",
        "  # sample a dataset\n",
        "  np.random.seed(idx)\n",
        "  n_samples = 30\n",
        "  X = np.sort(np.random.rand(n_samples))\n",
        "  y = true_fn(X) + np.random.rand(n_samples)*0.1\n",
        "\n",
        "  # Fit a least square model\n",
        "  polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)\n",
        "  linear_regression = LinearRegression()\n",
        "  pipeline = Pipeline([(\"pf\", polynomial_features), (\"lr\", linear_regression)])\n",
        "  pipeline.fit(X[:,np.newaxis], y)\n",
        "\n",
        "  # Fit a Ridge model\n",
        "  polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)\n",
        "  linear_regression = Ridge(alpha=0.1) #in sklear we use alpha insted of lambda\n",
        "  pipeline2 = Pipeline([(\"pf\", polynomial_features), (\"lr\", linear_regression)])\n",
        "  pipeline2.fit(X[:,np.newaxis], y)\n",
        "\n",
        "  # Visualize the results\n",
        "  ax = plt.subplot(1, len(degrees), i+1)\n",
        "  #ax.plot(X_test, true_fn(X_test), label='True Function')\n",
        "  ax.plot(X_test, pipeline.predict(X_test[:,np.newaxis]), label='No Regularization')\n",
        "  ax.plot(X_test, pipeline2.predict(X_test[:,np.newaxis]), label = \"L2 Regularization\")\n",
        "  ax.scatter(X,y, edgecolor='b', s=20, label='Samples')\n",
        "  ax.set_xlim([0,1])\n",
        "  ax.set_ylim({-2,2})\n",
        "  ax.legend(loc='best')\n",
        "  ax.set_title(\"Dataset sample #{}\".format(idx))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7iSiqpKLhRr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can show that by using small weights, we prevent the model from learning irregular functions."
      ],
      "metadata": {
        "id": "foSH_vfp1ysb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Non-regularized weigths of the polynomial models need to be large to fit every point:')\n",
        "print(pipeline.named_steps['lr'].coef_[:4])\n",
        "print()\n",
        "\n",
        "print('By regularizing the weight to be small, we force the curve to be more regular')\n",
        "print(pipeline2.named_steps['lr'].coef_[:4])"
      ],
      "metadata": {
        "id": "ej7HZjL6ykOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **How to choose $\\lambda$?**\n",
        "In brief, the most common approach is to choose the value of $\\lambda$ that results in the best performance on a held-out validation set.\n",
        "\n",
        "We will see this strategies and several other in more detail."
      ],
      "metadata": {
        "id": "3PQd_hln4Huw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Normal Equations for Regularized Models**\n",
        "How do we fit regularized models? In the linear case, we can do this easily by using generalized normal equations.\\\n",
        "Let $L(\\theta) = \\frac{1}{2}(X\\theta - y)^\\top(X\\theta - y)$ be our Least Square objective. We can write Ridge objective as:\n",
        "$$J(\\theta) = \\frac{1}{2}(X\\theta - y)^\\top(X\\theta - y) + \\frac{1}{2}\\lambda||\\theta||^2_2$$\n",
        "\n",
        "This allow us to derive the gradient as follow:\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\nabla_{\\theta}J(\\theta) &= \\nabla_{\\theta}\\left(\\frac{1}{2}(X\\theta - y)^\\top(X\\theta - y) + \\frac{1}{2}\\lambda||\\theta||^2_2 \\right) \\\\\n",
        "&= \\nabla_{\\theta}(L(\\theta) + \\frac{1}{2}\\lambda||\\theta||^2_2)\\\\\n",
        "&= (X^\\top X)\\theta - X^\\top y + \\lambda\\theta\\\\\n",
        "&= (X^\\top X + \\lambda I)\\theta - X^\\top y\n",
        "\\end{align*}$$\n",
        "\n",
        "We used the derivation of normal equations for least squares to obtain $\\nabla_{\\theta}L(\\theta)$ as well as the fact that $\\nabla_xx^\\top x = 2x$\n",
        "\n",
        "We can set the gradient to zero to obtain normal equation for Ridge Models:\n",
        "$$(X^\\top X + \\lambda I)\\theta = X^\\top y.$$\n",
        "\n",
        "Hence, the value $\\theta^*$ that minimizes the objective is given by:\n",
        "$$\\theta^* = (X^\\top X + \\lambda I)^{-1}X^\\top y.$$\n",
        "\n",
        "Note that the matrix $(X^\\top X + \\lambda I)$ is always invertible, which able to addresses problems with least squares we saw earlier."
      ],
      "metadata": {
        "id": "B6ED8yOl4ydF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algorithm: Ridge Model\n",
        "\n",
        "*   **Type**: Supervised Learning (Regression)\n",
        "*   **Model Family**: Linear models\n",
        "*   **Objective function**: L2-regularized mean square error\n",
        "*   **Optimizer**: Normal Equations"
      ],
      "metadata": {
        "id": "hC3Pq60S-01A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 5: Regularization and Sparsity**\n",
        "We will now look another form of regularization, which has an important new property call **sparsity**"
      ],
      "metadata": {
        "id": "NUeP_7yk_jHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Regularization: Definition**\n",
        "The idea of regularization is to train models with an augmented objective $J : \\mathcal{M} \\to \\mathbb{R}$ defined over a training set $\\mathcal{D}$ of size n as:\n",
        "$$J(f) = \\frac{1}{n}\\sum_{i=1}^nL(y^{(i)}, f(x^{(i)})) + \\lambda R(f)$$\n",
        "\n",
        "Let's dissect the components of this objective:\n",
        "\n",
        "\n",
        "*   A loss function $L(y^{(i)}, f(x^{(i)}))$ such as mean squared error\n",
        "*   A regularizer $R: \\mathcal{M} \\to \\mathbb{R}$ that penalizes models that are overly complex.\n",
        "\n"
      ],
      "metadata": {
        "id": "mbPgEGN1AzHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **L1 Regularization: Definition**\n",
        "Another closely related approach to regularization is to penalize the size of weights using the L1 norm.\\\n",
        "In the context of linear models $f(x) = \\theta^\\top x$, L1 norm yields the following form of objective:\n",
        "$$J(\\theta) = \\frac{1}{n}\\sum_{i=1}^nL(y^{(i)}, \\theta^\\top x^{(i)}) + \\lambda ||\\theta||_1.$$\n",
        "\n",
        "Let's dissect the components of this objective:\n",
        "\n",
        "\n",
        "*   The regularizer $R:\\mathcal{M} \\to \\mathbb{R}$ is the function $R(\\theta) = ||\\theta||_1 = \\sum_{j=1}^d\\|theta_j|$. This is also known as the L1 norm of $\\theta$,\n",
        "*   The regularizer also penalizes large weights, it also force more weights to decay to zero, as opposed to just being small.\n"
      ],
      "metadata": {
        "id": "Vzu-lEeuCsOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Algorithm: Lasso**\n",
        "L1-regularized linear regression is also known as Lasso (least absolute shrinkage and selection operator).\n",
        "\n",
        "\n",
        "*   **Type**: Supervised learning (Regression)\n",
        "*   **Modol family**: Linear models\n",
        "*   **Objective function**: L1-regularized mean squared error\n",
        "*   **Optimizer**: gradient descent, coordinate descent, least angle regression (LARs) and others.\n",
        "\n"
      ],
      "metadata": {
        "id": "dGm-iAeNFL6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Regularizing via. Constraints**\n",
        "Consider regularized problem with a penalty term:\n",
        "$$min_{\\theta \\in \\Theta}L(\\theta) + \\lambda R(\\theta).$$\n",
        "We may also enforce an explicit constraint on the complexity of the model:\n",
        "$$\\begin{align*}\n",
        "min_{\\theta \\in \\Theta} \\; &L(\\theta) \\\\\n",
        "\\text{such that}, \\; &R(\\theta) \\leq \\lambda'\n",
        "\\end{align*}$$\n",
        "We will not prove this, but solving this problem is equivalent to solving the penalized problem for some $\\lambda > 0$ that's different form $\\lambda'$\n",
        "\n",
        "In other words,\n",
        "\n",
        "\n",
        "*   We can regularize by explicitly enforcing $R(\\theta)$ to be less than a value instead of penalizing it.\n",
        "*   For each value of $\\lambda$, we are inplicitly setting a constraint of $R(\\theta)$.\n",
        "\n"
      ],
      "metadata": {
        "id": "YeDZyGkiGxLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Example:**\n",
        "This is what look like for a linear model:\n",
        "$$\\begin{align*}\n",
        "min_{\\theta \\in \\Theta} \\; &\\frac{1}{2n}\\sum_{i=1}^n(y^{(i)} - \\theta^\\top x^{(i)})^2 \\\\\n",
        "\\text{such that} \\; &||\\theta|| \\leq \\lambda'\n",
        "\\end{align*}$$\n",
        "\n",
        "where $||\\cdot||$ can either be the L1 or L2 norm."
      ],
      "metadata": {
        "id": "QZxf4H36KyfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **L1 vs. L2 Regularization**\n",
        "The folloing image by [David Kapil]() and Hastie et al, explains the difference between the two norms\n",
        "\n",
        "<left><img width=75% src=\"/content/l1-vs-l2-annotated.png\"></left>\n"
      ],
      "metadata": {
        "id": "x88W6uGsN-ie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sparsity: Definition**\n",
        "A vctor is said to be sparse if a large fraction of its entire is zero.\\\n",
        "L1-regularized linear regression produces *sparse weights.*\n",
        "\n",
        "*   This makes the model more interpretable.\n",
        "*   It also make it computationally more tractable in very large dimensions.\n"
      ],
      "metadata": {
        "id": "hCPiBeliSv8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sparsity: Ridge Model**\n",
        "To better understand sparsity, we will fit L2-regularized linear models to the UCI Diabetes Dataset and observe the magnitude of each weight (colored lines) as a function of the regularization parameters."
      ],
      "metadata": {
        "id": "aaW5v2GZT6qF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.linear_model import Ridge\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "X,y = load_diabetes(return_X_y=True)\n",
        "\n",
        "# Create Ridge coefficient\n",
        "alphas = np.logspace(-5,2,200)\n",
        "ridge_coefs = []\n",
        "for a in alphas:\n",
        "  ridge = Ridge(alpha = a, fit_intercept=False)\n",
        "  ridge.fit(X,y)\n",
        "  ridge_coefs.append(ridge.coef_)\n",
        "\n",
        "# Plot Ridge Coefficients\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(alphas, ridge_coefs)\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Regularization parameter (lambda)')\n",
        "plt.ylabel('Magnitude of model parameters')\n",
        "plt.title('Ridge coefficients as a function of the regularization')\n",
        "plt.axis('tight')"
      ],
      "metadata": {
        "id": "XMh0h6VOWyev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sparsity: Lasso**\n",
        "The above Ridge model dose not produce sparse weights. Let's compare it to the Lasso model. "
      ],
      "metadata": {
        "id": "3ZM_jIXmg82y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.linear_model import lars_path\n",
        "\n",
        "# Create Lasso coefficients\n",
        "_, _, lasso_coefs = lars_path(X,y,method='lasso')\n",
        "xx = np.sum(np.abs(lasso_coefs.T), axis=1)\n",
        "\n",
        "# Plot the Ridge coeffiicients\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.subplot(121)    \n",
        "plt.plot(alphas, ridge_coefs)\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Regularization parameter (alpha)')\n",
        "plt.ylabel('Magnitude of model parameters')\n",
        "plt.title('Ridge coefficients as a function of the regularization')\n",
        "plt.axis('tight')\n",
        "\n",
        "# Plot the Lasso coefficients\n",
        "plt.subplot(122)\n",
        "plt.plot(3500 - xx, lasso_coefs.T)\n",
        "ymin, ymax = plt.ylim()\n",
        "plt.xlim(ax.get_xlim()[::-1]) # reverse axis\n",
        "plt.xlabel('Regularization parameter (lambda)')\n",
        "plt.ylabel('Regularization Strength')\n",
        "plt.title('Lasso Path')\n",
        "plt.axis('tight')"
      ],
      "metadata": {
        "id": "wxCSbIiEhSxt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}