{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 11. Kernels**\n",
        "## **Applied Machine Learning**"
      ],
      "metadata": {
        "id": "RGMdsvjgczdX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 1: The Kernel Trick: Motivation**\n",
        "So far, the majority machine learning models we have seen have been linear.\n",
        "\n",
        "In this lecture, we will see a general way to make any of these models *non-linear*. We will use a new idea called *kernel*. "
      ],
      "metadata": {
        "id": "N8-ZA5fac6p2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Linear Regression**\n",
        "Recall that, a linear model has the form:\n",
        "$$f_{\\theta}(x) = \\sum_{i=1}^d\\theta_ix_i = \\theta^\\top x .$$\n",
        "\n",
        "where $x$ is a vector of features and we use notation $x_0=1$.\n",
        "\n",
        "We pick $\\theta$ to minimize the (L2 regularized) mean squared errors (MSE):\n",
        "$$J(\\theta) = \\frac{1}{2n}\\sum_{i=1}^n(y^{(i)} - \\theta^\\top x^{(i)})^2 + \\frac{\\lambda}{2}\\sum_{j=1}^d\\theta_j^2.$$"
      ],
      "metadata": {
        "id": "boVHjoZTd67-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Polynomials**\n",
        "\n",
        "Recall that a polynomial of degree $p$ is a function of the form:\n",
        "$$a_px^p + a_{p-1}x^{p-1} + \\dots + a_1x + a_0.$$\n",
        "\n",
        "Below are some examples of polynomial functions."
      ],
      "metadata": {
        "id": "YIl_4tFHd7Ji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Polynomial Regression**\n",
        "\n",
        "Specifically, given a one-dimensional continuous variable $x$, we can defining a feature function $\\phi: \\mathbb{R} \\to \\mathbb{R}^{p+1}$ as:\n",
        "\n",
        "$$ \\phi(x) = \\begin{bmatrix}\n",
        "1 \\\\\n",
        "x \\\\\n",
        "x^2 \\\\\n",
        "\\vdots \\\\\n",
        "x^p\n",
        "\\end{bmatrix}.\n",
        "$$"
      ],
      "metadata": {
        "id": "CVbbUBTofw5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The class of the models of form:\n",
        "$$f_{\\theta}(x) := \\sum_{j=0}^p\\theta_jx^j = \\theta^\\top \\phi(x).$$\n",
        "\n",
        "with parameters $\\theta$ and polynomial feature $\\phi$ is the set of $p$-degree polynomial."
      ],
      "metadata": {
        "id": "dXviQ9N5fxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Towards General Non-linear Features**\n",
        "\n",
        "Any non-linear feature map $\\phi(x): \\mathbb{R}^d \\to \\mathbb{R}^p$ can be used to obtain general models of the form:\n",
        "$$f_{\\theta}(x) := \\theta^\\top \\phi(x)$$\n",
        "\n",
        "that are highly non-linear in feature $x$ but linear in parameter $\\theta$.\n",
        "\n"
      ],
      "metadata": {
        "id": "QYILlwSrfxYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The Featurized Design Matrix**\n",
        "\n",
        "It is useful to represent the featurized dataset as a matrix $\\Phi \\in \\mathbb{R}^{n \\times p}$\n",
        "$$ \\Phi = \\begin{bmatrix}\n",
        "\\phi(x^{(1)})_1 & \\phi(x^{(1)})_2 & \\ldots & \\phi(x^{(1)})_p \\\\\n",
        "\\phi(x^{(2)})_1 & \\phi(x^{(2)})_2 & \\ldots & \\phi(x^{(2)})_p \\\\\n",
        "\\vdots \\\\\n",
        "\\phi(x^{(n)})_1 & \\phi(x^{(n)})_2 & \\ldots & \\phi(x^{(n)})_p\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "- & \\phi(x^{(1)})^\\top & - \\\\\n",
        "- & \\phi(x^{(2)})^\\top & - \\\\\n",
        "& \\vdots & \\\\\n",
        "- & \\phi(x^{(n)})^\\top & - \\\\\n",
        "\\end{bmatrix}\n",
        ".$$"
      ],
      "metadata": {
        "id": "WFZgTn-Fir7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Featurized Normal Equations**\n",
        "\n",
        "The normal equations provide a closed-form solution for $\\theta$:\n",
        "\n",
        "$$\\theta = (X^\\top X + \\lambda I)^{-1}X^\\top y.$$\n",
        "\n",
        "When the vector of attributes $x^{(i)}$ are featurized, we can write this as:\n",
        "$$\\theta = (\\Phi^\\top\\Phi + \\lambda I)^{-1}\\Phi^\\top y.$$"
      ],
      "metadata": {
        "id": "5AjZfeLWisKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Push-Through Matrix Identity**\n",
        "\n",
        "We can modify this expression by using a version of the [push-through matrix identity](https://en.wikipedia.org/wiki/Woodbury_matrix_identity#Discussion)\n",
        "$$(\\lambda I + UV)^{-1}U = U(\\lambda I + VU)^{-1}$$\n",
        "\n",
        "where $U \\in \\mathbb{R}^{n\\times m}$ and $V \\in \\mathbb{R}^{m \\times n}$ and $\\lambda \\neq 0$\n",
        "\n",
        "Proof sketch: Start with $U(\\lambda I + VU) = (\\lambda I + UV)U$ and multiply both sides by $(\\lambda I + VU)^{-1}$ on the right and $(\\lambda I + UV)^{-1}$ on the left."
      ],
      "metadata": {
        "id": "iAdAwI_UisWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Normal Equations: Dual Form**\n",
        "We can apply the identity $(\\lambda I + UV)^{-1} U = U(\\lambda I + VU)^{-1}$ to the normal equation with $U = \\Phi^\\top, V = \\Phi.$\n",
        "\n",
        "$$\\theta = (\\Phi^\\top\\Phi + \\lambda I)^{-1}\\Phi^\\top y.$$\n",
        "\n",
        "to obtain a dual form:\n",
        "$$\\theta = \\Phi^\\top(\\Phi\\Phi^\\top + \\lambda I)^{-1}y$$\n",
        "\n",
        "This approach take $O(p^3)$ times; the second is $O(n^3)$ times and is faster when $p>n.$"
      ],
      "metadata": {
        "id": "-MHy3qgUisiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Feature Representation for Parameters**\n",
        "An interesting corollary of the dual form:\n",
        "$$\\theta = \\Phi^\\top\\underbrace{(\\Phi\\Phi^\\top + \\lambda I)^{-1}y}_{\\alpha}$$\n",
        "\n",
        "is that the optimal $\\theta$ is a linear combination of the n training set features:\n",
        "$$\\theta = \\sum_{i=1}^n\\alpha_i\\phi(x^{(i)}).$$\n",
        "\n",
        "Here, the weights $\\alpha $ derived from $(\\Phi\\Phi^\\top + \\lambda I)^{-1}y$ and equals:\n",
        "$$\\alpha_i = \\sum_{j=1}^nL_{ij}y_j$$\n",
        "\n",
        "where $L_{ij} = (\\Phi^\\top \\Phi + \\lambda I)^{-1}.$"
      ],
      "metadata": {
        "id": "W5ZbBgXtpb09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Predictions from Features**\n",
        "Consider now a prediction $\\phi(x')^\\top \\theta$ at a new input $x'$\n",
        "$$\\phi(x')^\\top\\theta = \\sum_{i=1}^n\\alpha_i\\phi(x')^\\top\\phi(x^{(i)}).$$\n",
        "\n",
        "The crucail observation is that the features $\\phi(x)$ are never used directly in this equation. Only their dot product is used!\n",
        "\n",
        "This observation will be at the heart of a powerful new idea called the *kernel trick.*"
      ],
      "metadata": {
        "id": "6sKw5hfJpcDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Learning from the Feature Products**\n",
        "\n",
        "We also don't need features $\\phi$ for learning $\\theta$, just their dot products! Recall that each row $i$ of $\\Phi$ is the $i$-th featurized input $\\phi(x^{(i)})^\\top.$\n",
        "\n",
        "Thus $K = \\Phi\\Phi^\\top$ is a matrix of all dot products between all the $\\phi(x^{(i)})$\n",
        "$$K_{ij} = \\phi(x^{(i)})^\\top\\phi(x^{(j)}).$$\n",
        "\n",
        "We can compute $\\alpha = (K + \\lambda I)^{-1}y$ and uses it for predictions\n",
        "$$\\phi(x')^\\top\\theta = \\sum_{i=1}^n\\alpha_i\\phi(x')^\\top\\phi(x^{(i)}).$$\n",
        "\n",
        "and all this only requires dot products, not features $\\phi$!"
      ],
      "metadata": {
        "id": "hUz_A7afpcQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The Kernel Trick**\n",
        "\n",
        "The above observations hint at a powerful new idea -- if we can compute dot products of features $\\phi(x)$ efficiently, then we will be able to use high-dimensional features easily.\n",
        "\n",
        "It turns out that we can do this for many ML algorithms -- we call this the **Kernel Trick**"
      ],
      "metadata": {
        "id": "0ovqf8yYpcgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 2: The Kernel Trick: An Example**\n",
        "\n",
        "Many ML algorithms can be written down as optimization problems in which the feature $\\phi(x)$ only appears in the dot products form $\\phi(x)^\\top \\phi(z)$ that can be computed efficiently.\n",
        "\n",
        "Let's look at an example:"
      ],
      "metadata": {
        "id": "fyp7yso1XEUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Non-linear Features**\n",
        "\n",
        "Any non-linear feature map $\\phi(x): \\mathbb{R}^d \\to \\mathbb{R}^p$ can be used in this way to obtain the general model of form:\n",
        "$$f_{\\theta}(x) := \\theta^\\top x.$$\n",
        "\n",
        "that are highly non-linear in the feature $x$ but linear in $\\theta$."
      ],
      "metadata": {
        "id": "408udW0WXEaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Normal Equations**\n",
        "\n",
        "The normal equations provide a closed form solution for $\\theta$:\n",
        "$$\\theta = (\\Phi^\\top\\Phi + \\lambda I)\\Phi^\\top y.$$\n",
        "\n",
        "They can also be written in this form:\n",
        "$$\\theta = \\Phi^\\top(\\Phi\\Phi^\\top + \\lambda I) y.$$\n",
        "\n",
        "The first approach takes $O(d^3)$ times, and the second takes $O(n^3)$, and is faster when $d>n$"
      ],
      "metadata": {
        "id": "y0mZ_78YXEnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Learning from Feature Products**\n",
        "\n",
        "An interesting corollary is that the optimal $\\theta$ is a linear combination of the $n$ training set features:\n",
        "$$\\theta = \\sum_{i=1}^n\\alpha_i\\phi(x^{(i)})).$$\n",
        "\n",
        "We can compute a prediction $\\phi(x')^\\top\\theta$ for $x'$ without ever using the features (only their dot products).\n",
        "\n",
        "$$\\phi(x')^\\top x = \\sum_{i=1}^n \\alpha_i\\phi(x')^\\top\\phi(x^{(i)}). $$\n",
        "\n",
        "Equally importantly, we can learn $\\theta$ form only dot products.\n"
      ],
      "metadata": {
        "id": "DZNkBp9JZOod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Polynomial Regression**\n",
        "\n",
        "Note that a $p$-th degree polynomial\n",
        "\n",
        "$$a_px^p + a_{p-1}x^{p-1} + \\dots + a_1x + a_0$$\n",
        "\n",
        "forms a linear model with parameters $a_p, a_{p-1}, a_{p-2}, \\dots, a_1, a_0$. This means we can use our algorithms for linear models to learn non-linear features.\n",
        "\n",
        "specifically, given a one-dimensional continuous variable $x$, we can define the feature function $\\phi: \\mathbb{R} \\to \\mathbb{R^p}$ as\n",
        "\n",
        "$$\n",
        "\\phi(x) = \\begin{bmatrix}\n",
        "1 \\\\\n",
        "x \\\\\n",
        "x^2 \\\\\n",
        "\\vdots \\\\\n",
        "x^p\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Then the class of the model of the form:\n",
        "$$f_{\\theta}(x) := \\sum_{i=0}^p\\theta_i x^i =\\theta^\\top \\phi(x)$$\n",
        "\n",
        "with parameters $\\theta$ encompasses the set of $p$-degree polynomial. Specifically\n",
        "* It is non-linear in the input variable $x$, meaning that we can model complex data relationship.\n",
        "* It is a linear model of the function of parameters $\\theta$, meaning that we can use our familiar ordinary least squares algorithm to learn thees features."
      ],
      "metadata": {
        "id": "Y1_xin3mZOxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The Kernel Trick: A First Example**\n",
        "Can we compute the dot products of $\\phi(x)^\\top\\phi(x')$ of the polynomial $\\phi(x)$ more efficiently than using the standard definition of a dot product? \n",
        "\n",
        "Let's look at an example\n",
        "\n",
        "To start, let's consider polynomial features  $\\phi: \\mathbb{R}^d \\to \\mathbb{R}^{d^2}$ of the form:\n",
        "$$\\phi(x)_{ij} = x_ix_j \\; \\text{for} \\; i,j \\in \\{1,2,\\dots,d\\}$$\n",
        "\n",
        "For $d=3$ this looks like:\n",
        "\n",
        "$$\n",
        "\\phi(x) = \\begin{bmatrix}\n",
        "x_1x_1\\\\\n",
        "x_1x_2\\\\\n",
        "x_1x_3\\\\\n",
        "x_2x_1\\\\\n",
        "x_2x_2\\\\\n",
        "x_2x_3\\\\\n",
        "x_3x_1\\\\\n",
        "x_3x_2\\\\\n",
        "x_3x_3\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "tZ1okT4DdMyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The product of $x$ and $z$ in the feature space equals:\n",
        "\n",
        "$$\\phi(x)^\\top\\phi(z) = \\sum_{i=1}^d\\sum_{j=1}^dx_ix_jz_iz_j.$$\n",
        "\n",
        "Computing this products involves the sum over $d^2$ terms and takes $O(d^2)$ times.\n",
        "\n",
        "An alternative way of computing the product of $\\phi(x)^\\top\\phi(z)$ is to instead compute $(x^\\top z)^2$. We can check that this has the same result:\n",
        "\n",
        "\\begin{align*}\n",
        "(x^\\top z)^2 &= (\\sum_{i=1}^dx_iz_i)^2 \\\\\n",
        "&= (\\sum_{i=1}^dx_iz_i)(\\sum_{j=1}^dx_jz_j) \\\\\n",
        "&= \\sum_{i=1}^d\\sum_{j=1}^dx_ix_jz_iz_j \\\\\n",
        "&= \\phi(x)^\\top\\phi(z)\n",
        "\\end{align*}\n",
        "\n",
        "However, computing $(x^\\top z)^2$ can be done in $O(d)$ times.\n",
        "\n",
        "This is a very powerful idea.\n",
        "* We can compute the dot products between $O(d^2)$ features in only $O(d)$ times.\n",
        "* We can use high-dimensional features within the ML algorithms the only rely on the dot products (like kernelized ridge regression) without incurring extra costs."
      ],
      "metadata": {
        "id": "sosyzgSWfAKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The Kernal Trick: Polynomial Features**\n",
        "The number of polynomial features $\\phi_p$ of $p$-degree when $x \\in \\mathbb{R}^d$\n",
        "\n",
        "$$\\phi(x)_{i_1,i_2,\\dots,i_p} = x_{i_1}x_{i_2}\\dots x_{i_p} \\; \\text{for} \\; i_1, i_2, \\dots, i_p \\in \\{1,2,\\dots,d\\}$$\n",
        "\n",
        "scales as $O(d^p)$.\n",
        "\n",
        "However, we can compute the dot product $\\phi_p(x)^\\top\\phi_p(x)$ in this feature space in only $O(d)$ times for any $p$ as:\n",
        "$$\\phi_p(x)^\\top\\phi_p(x) = (x^\\top z)^p.$$"
      ],
      "metadata": {
        "id": "UmmIDEv5hbXX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Algorithm: Kenerlised Polynomial Ridge Regression** \n",
        "* Type: Supervised Learning (regression)\n",
        "* Model family: Polynomials\n",
        "* Objective function: $L2$-regularized ridge regression\n",
        "* Optimizer: Normal equations (dual forms)\n",
        " "
      ],
      "metadata": {
        "id": "eFprbqD3kAfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The Kernel Trick: General Idea**\n",
        "\n",
        "Many types of feature $\\phi(x)$ have the property that their dot product $\\phi(x)^\\top\\phi(z)$ can be computed more efficiently than if we had to form these features explicitly.\n",
        "\n",
        "Also, we will see many ML algorithms can be written down as optimization problem in which the features $\\phi(x)$ only appear as dot product $\\phi(x)^\\top\\phi(z)$.\n",
        "\n",
        "The *Kernel Trick* means that we can use complex non-linear features within these algorithms with little computational cost.\n",
        "\n",
        "Examples of algorithms in which we can use the kernel trick:\n",
        "* Supervised learning algorithms: linear regression, logistic regression, support vector machine, etc.\n",
        "* Unsupervised learning: PCA, density estimation.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k84z3umRkuK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will look at more example shortly."
      ],
      "metadata": {
        "id": "x-J8Zq8nmTqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 3: The Kernel Trick in SVMs**\n",
        "\n",
        "Many ML algorithms can be written down as optimization problems in which the features $\\phi(x)$ only appear as dot products $\\phi(x)^\\top\\phi(x)$ that can be computed efficiently.\n",
        "\n",
        "We will now see how SVMs can benefit from the Kernel Trick as well."
      ],
      "metadata": {
        "id": "-X9eVMohMnFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: SVM Model Family**\n",
        "\n",
        "We will consider models of the form:\n",
        "$$f_{\\theta}(x) = \\theta^\\top \\phi(x) + \\theta_0$$\n",
        "\n",
        "where $x$ is the input and $y \\in \\{-1,+1\\}$ is the target."
      ],
      "metadata": {
        "id": "9BVJaQIlMnT_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Dual and Primal Formulations** \n",
        "\n",
        "Recall that the max-margin hyperplane can be formulated as the solution to the following *primal* optimization problem.\n",
        "\n",
        "\\begin{align*}\n",
        "\\min_{\\theta,\\theta_0,\\xi}&\\frac{1}{2}\\|\\theta\\|^2 + C\\sum_{i=1}^n\\xi_i \\\\\n",
        "\\text{subjected to} \\; &y^{(i)}((x^{(i)})^\\top\\theta + \\theta_0) \\geq 1 - \\xi_i \\; \\text{for all} \\; i \\\\\n",
        "& \\xi_i \\geq 0\n",
        "\\end{align*}\n",
        "\n",
        "The solution to this problem also happens to be given by the following *dual* problem:\n",
        "\\begin{align*}\n",
        "\\max_{\\lambda}&\\sum_{i=1}^n\\lambda_i - \\frac{1}{2}\\sum_{i=1}^n\\sum_{k=1}^n\\lambda_i\\lambda_ky^{(i)}y^{(k)}(x^{(i)})^\\top x^{(k)}\\\\\n",
        "\\text{subjected to} \\; &\\sum_{i=1}^n\\lambda_iy^{(i)} = 0 \\\\\n",
        "&C \\geq \\lambda_i \\geq 0 \\; \\text{for all} \\; i\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "WB9KI0kpMniH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: The Primal Solution**\n",
        "\n",
        "We can obtain a primal solution from the dual via the following equation:\n",
        "\n",
        "$$\\theta^* = \\sum_{i=1}^n\\lambda_i^*y^{(i)}\\phi(x^{(i)}).$$\n",
        "\n",
        "Ignoring the $\\theta_0$ term for now, the score at a new point $x'$ will be equal:\n",
        "$$\\theta^\\top\\phi(x') = \\sum_{i=1}^n\\lambda_i^*y^{(i)}\\phi(x^{(i)})^\\top\\phi(x').$$ "
      ],
      "metadata": {
        "id": "2KhP2zw8Qs5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The Kernel Trick in SVMs**\n",
        "\n",
        "Notice that in both equations, the feature $x$ are never used directly. Only their dot product is used\n",
        "\\begin{align*}\n",
        "\\sum_{i=1}^n\\lambda_i - &\\frac{1}{2}\\sum_{i=1}^n\\sum_{k=1}^n\\lambda_i\\lambda_ky^{(i)}y^{(k)}\\phi(x^{(i)})^\\top \\phi(x^{(k)})\\\\ \n",
        "&\\theta^\\top\\phi(x') = \\sum_{i=1}^n\\lambda_i^*y^{(i)}\\phi(x^{(i)})^\\top\\phi(x')\n",
        "\\end{align*}\n",
        "\n",
        "If we can compute the dot products efficiently, we can potentilly use very complex features."
      ],
      "metadata": {
        "id": "ESWpT3QISK1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The Kernel Trick in SVMs**\n",
        "More generally, given features $\\phi(x)$, suppose that we have a function $K: \\mathcal{X} \\times \\mathcal{X} \\to [0,\\infty)$ that outputs dot products between vectors in $\\mathcal{X}$.\n",
        "$$K(x,z) = \\phi(x)^\\top\\phi(z).$$\n",
        "\n",
        "Will call $K$ is the kernel function.\n",
        "\n",
        "Recall that an example of a useful kernel function is:\n",
        "$$K(x,z) = (x\\cdot z)^p$$\n",
        "\n",
        "because it computes the dot product of polynomial features of degree $p$.\n",
        "\n",
        "Then notice that we can rewrite the dual form of SVM as\n",
        "\n",
        "\\begin{align*}\n",
        "\\max_{\\lambda}&\\sum_{i=1}^n\\lambda_i - \\frac{1}{2}\\sum_{i=1}^n\\sum_{k=1}^n\\lambda_i\\lambda_ky^{(i)}y^{(k)}K(x^{(i)}, x^{(k)})\\\\\n",
        "\\text{subjected to} \\; &\\sum_{i=1}^n\\lambda_iy^{(i)} = 0 \\\\\n",
        "&C \\geq \\lambda_i \\geq 0 \\; \\text{for all} \\; i\n",
        "\\end{align*}\n",
        "\n",
        "and *predictions* at new point $x'$ are given by $\\sum_{i=1}^n\\lambda_i^*y^{(i)}K(x^{(i)},x').$\n",
        "\n",
        "Using our earlier trick, we can use polynomial features of any degree of $p$ in SVM without forming these features and not extra cost!"
      ],
      "metadata": {
        "id": "AO7M_XbOSLJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Algorithm: Kernelized Support Vector Machine classification (Dual Form)**\n",
        "\n",
        "* **Type**: Supervised learning (binary classification)\n",
        "* **Model Family**: Non-linear decision boundaries\n",
        "* **Objective function**: Dual of SVM optimization problem\n",
        "* **Optimizer**: Sequential Minimal optimization"
      ],
      "metadata": {
        "id": "_VrUTgdtSLbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 4: Types of Kernels**\n",
        "\n",
        "Now that we saw the kernel trick, let's look at several examples of kernels."
      ],
      "metadata": {
        "id": "Cddf-MBXSLqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Kernel Trick for Ridge Regression**\n",
        "\n",
        "The normal equation provides a closed form solution for $\\theta$:\n",
        "$$\\theta = (\\Phi^\\top\\Phi + \\lambda I)^{-1}\\Phi^\\top y.$$\n",
        "\n",
        "They also can be written in the form:\n",
        "$$\\theta = \\Phi^\\top(\\Phi\\Phi^\\top + \\lambda I)^{-1}y.$$\n",
        "\n",
        "The first approach takes $O(d^3)$ times, and the second takes $O(n^3)$ times and is faster if $d>n$.\n",
        "\n",
        "An interesting corollary is that the optimal $\\theta$ is a linear combination of the $n$ training set features:\n",
        "$$\\theta = \\sum_{i=1}^n\\alpha_i\\phi(x^{(i)}).$$\n",
        "\n",
        "We can compute a prediction $\\phi(x')^\\top\\theta$ for a new point $x'$ without ever using the features (only their dot products).\n",
        "$$\\phi(x')^\\top\\theta = \\sum_{i=1}^n\\alpha_i\\phi(x')^\\top\\phi(x^{(i)})$$\n",
        "\n",
        "Equally importantly, we can learn $\\theta$ from only dot products.\n"
      ],
      "metadata": {
        "id": "VyKwAu-PZKty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Kernel Trick in SVMs**\n",
        "\n",
        "Notice that in both equations, the feature $x$ are never used directly. Only their dot product is used\n",
        "\\begin{align*}\n",
        "\\sum_{i=1}^n\\lambda_i - &\\frac{1}{2}\\sum_{i=1}^n\\sum_{k=1}^n\\lambda_i\\lambda_ky^{(i)}y^{(k)}\\phi(x^{(i)})^\\top \\phi(x^{(k)})\\\\ \n",
        "&\\theta^\\top\\phi(x') = \\sum_{i=1}^n\\lambda_i^*y^{(i)}\\phi(x^{(i)})^\\top\\phi(x')\n",
        "\\end{align*}\n",
        "\n",
        "If we can compute the dot products efficiently, we can potentilly use very complex features."
      ],
      "metadata": {
        "id": "6AnFYPf1bneH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Definition: Kernels**\n",
        "\n",
        "The kernel corresponding to the features $\\phi(x)$ is the function $K: \\mathcal{X} \\times \\mathcal{X} \\to [0,\\infty)$ that outputs the dot products between vectors in $\\mathcal{X}$.\n",
        "\n",
        "$$K(x,z) = \\phi(x)^\\top \\phi(z).$$\n",
        "\n",
        "We will also consider general functions $K:\\mathcal{X} \\times \\mathcal{X} \\to [0,\\infty)$ and call these *kernel function*.\n",
        "\n",
        "Kernels have many interpretations:\n",
        "* The dot product or geometrical angle between $x$ and $z$,\n",
        "* A notion of similarity between $x$ and $z$.\n",
        "\n"
      ],
      "metadata": {
        "id": "PP7taQUBb5cn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in order to illustrate kernels, we will use this dataset"
      ],
      "metadata": {
        "id": "9oJL_3Aed5fQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kokw5fJock4C"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm\n",
        "\n",
        "# Our dataset and target.\n",
        "X = np.c_[\n",
        "    (0.4, -0.7),\n",
        "    (-1.5, -1),\n",
        "    (-1.4, -0.9),\n",
        "    (-1.3, -1.2),\n",
        "    (-1.1, -0.2),\n",
        "    (-1.2, -0.4),\n",
        "    (-0.5, 1.2),\n",
        "    (-1.5, 2.1),\n",
        "    (1, 1),\n",
        "    # --\n",
        "    (1.3, 0.8),\n",
        "    (1.2, 0.5),\n",
        "    (0.2, -2),\n",
        "    (0.5, -2.4),\n",
        "    (0.2, -2.3),\n",
        "    (0, -2.7),\n",
        "    (1.3, 2.1),\n",
        "].T\n",
        "\n",
        "y = [0]*8 + [1]*8\n",
        "\n",
        "x_min, x_max = -3, 3\n",
        "y_min, y_max = -3, 3\n",
        "plt.scatter(X[:,0], X[:,1], c=y, zorder=10, cmap=plt.cm.Paired, edgecolors='k')\n",
        "plt.xlim(x_min, x_max)\n",
        "plt.ylim(y_min, y_max)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Linear Kernel**\n",
        "\n",
        "The simplest kind of kernel that exists is called the linear kernel. This simply corresponds to dot product multiplication of the feature:\n",
        "\n",
        "$$K(x,z) = x^\\top z$$\n",
        "\n",
        "Applied to an SVM, this corresponds to a linear decision boundary.\n",
        "\n",
        "Below is an example of how we can use the SVM inplementation in `sklearn` with a linear kernel.\n",
        "\n",
        "Internally, this solves the dual SVM optimization problem."
      ],
      "metadata": {
        "id": "-E1-qtuEgvmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = svm.SVC(kernel='linear', gamma=2)\n",
        "clf.fit(X,y)\n",
        "\n",
        "# Plot the line, the points and the nearest vectors to the plane\n",
        "plt.scatter(clf.support_vectors_[:,0], clf.support_vectors_[:,1], s=80, facecolors='none', edgecolors='k', zorder=10)\n",
        "plt.scatter(X[:,0], X[:,1], c=y, zorder=10, cmap=plt.cm.Paired, edgecolors='k')\n",
        "XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
        "Z = clf.decision_function(np.c_[XX.ravel(),YY.ravel()])\n",
        "\n",
        "# Put the results into the color plot\n",
        "Z = Z.reshape(XX.shape)\n",
        "plt.pcolormesh(XX,YY,Z>0,cmap=plt.cm.Paired)\n",
        "plt.contour(XX, YY, Z, colors=['k','k','k'], linestyles=['--','-','--'], levels=[-.5,0,.5])\n",
        "plt.xlim(x_min,x_max)\n",
        "plt.ylim(y_min,y_max)"
      ],
      "metadata": {
        "id": "h8ym3fqqimXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Polynomial Kernel**\n",
        "A more interesting example is the polynomial kernel of degree $p$, of which we have already see a simple example.\n",
        "$$K(x,z) = (x^\\top z + c)^p.$$\n",
        "\n",
        "This corresponds to a mapping to a feature space of dimension $d+p \\choose p$ that has all monomials $x_{i_1}x_{i_2}\\dots x_{i_p}$ of degree at most $p$.\n",
        "\n",
        "For $d=3$ this features map looks like:\n",
        "\n",
        "$$\n",
        "\\phi(x) = \\begin{bmatrix}\n",
        "x_1x_1 \\\\\n",
        "x_1x_2 \\\\\n",
        "x_1x_3 \\\\\n",
        "x_2x_1 \\\\\n",
        "x_2x_2 \\\\\n",
        "x_2x_3 \\\\\n",
        "x_3x_1 \\\\\n",
        "x_3x_2 \\\\\n",
        "x_3x_3 \\\\\n",
        "\\sqrt{2}cx_1 \\\\\n",
        "\\sqrt{2}cx_2 \\\\\n",
        "\\sqrt{2}cx_3 \\\\\n",
        "c\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The polynomial kernel allows us to compute dot products in a $O(d^p)$-dimensional space in $O(d)$ times."
      ],
      "metadata": {
        "id": "-Cl-Qacgk6ZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how it would be implement in `sklearn`"
      ],
      "metadata": {
        "id": "pxibfzfSoHhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = svm.SVC(kernel='poly', degree=3, gamma=2)\n",
        "clf.fit(X,y)\n",
        "\n",
        "# Plot the line, the points and the nearest vectors to the plane\n",
        "plt.scatter(clf.support_vectors_[:,0], clf.support_vectors_[:,1], s=80, facecolors='none', edgecolors='k', zorder=10)\n",
        "plt.scatter(X[:,0], X[:,1], c=y, zorder=10, cmap=plt.cm.Paired, edgecolors='k')\n",
        "XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
        "Z = clf.decision_function(np.c_[XX.ravel(),YY.ravel()])\n",
        "\n",
        "# Put the results into the color plot\n",
        "Z = Z.reshape(XX.shape)\n",
        "plt.pcolormesh(XX,YY,Z>0,cmap=plt.cm.Paired)\n",
        "plt.contour(XX, YY, Z, colors=['k','k','k'], linestyles=['--','-','--'], levels=[-.5,0,.5])\n",
        "plt.xlim(x_min,x_max)\n",
        "plt.ylim(y_min,y_max)"
      ],
      "metadata": {
        "id": "TwO_2Fjskilt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Radial Basis Function Kernel**\n",
        "\n",
        "Another type is the **Radial Basis Function** (RBF; sometimes called Gaussian) kernel\n",
        "$$K(x,z) = \\exp\\left(-\\frac{\\|x-z\\|^2}{2\\sigma^2}\\right),$$\n",
        "\n",
        "where $\\sigma$ is the hyper-parameter. It's easiest to understand this by kernel by viewing it as a similarity measure.\n",
        "\n",
        "We can show that this kernel corresponds to an **infinite-dimensional** feature map and the limit of the polynomial kernel as $p \\to \\infty$.\n",
        "\n",
        "To see why that's intuitively the case, consider the Taylor expansion\n",
        "$$\\exp\\left(-\\frac{\\|x-z\\|^2}{2\\sigma^2}\\right) \\approx 1 - \\frac{\\|x-z\\|^2}{2\\sigma^2} + \\frac{\\|x-z\\|^4}{2!.4\\sigma^4} - \\frac{\\|x-z\\|^6}{3!.8\\sigma^6} + \\dots $$\n",
        "\n",
        "Each term in the right hand side can be expanded into a polynomial.\n",
        "\n"
      ],
      "metadata": {
        "id": "_kjYpiSnojXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We look at the `sklearn` implementation again"
      ],
      "metadata": {
        "id": "9UTVl2cEqzrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = svm.SVC(kernel='rbf', gamma=.5)\n",
        "clf.fit(X,y)\n",
        "\n",
        "# Plot the line, the points and the nearest vectors to the plane\n",
        "plt.scatter(clf.support_vectors_[:,0], clf.support_vectors_[:,1], s=80, facecolors='none', edgecolors='k', zorder=10)\n",
        "plt.scatter(X[:,0], X[:,1], c=y, zorder=10, cmap=plt.cm.Paired, edgecolors='k')\n",
        "XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
        "Z = clf.decision_function(np.c_[XX.ravel(),YY.ravel()])\n",
        "\n",
        "# Put the results into the color plot\n",
        "Z = Z.reshape(XX.shape)\n",
        "plt.pcolormesh(XX,YY,Z>0,cmap=plt.cm.Paired)\n",
        "plt.contour(XX, YY, Z, colors=['k','k','k'], linestyles=['--','-','--'], levels=[-.5,0,.5])\n",
        "plt.xlim(x_min,x_max)\n",
        "plt.ylim(y_min,y_max)"
      ],
      "metadata": {
        "id": "-ASM7RI3q3TX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **When is $K$ a Kernel?**\n",
        "We have seen that for many feature $\\phi$ we can define a kernel function $K: \\mathcal{X} \\times \\mathcal{X} \\to [0,∞)$ that efficiently computes $\\phi(x)^\\top \\phi(x).$\n",
        "\n",
        "Suppose now that we use some kernel function $K: \\mathcal{X} \\times \\mathcal{X} \\to [0,∞)$ in an ML algorithm. Is there an implicit feature mapping $\\phi$ that corresponds to using $K$?\n",
        "\n",
        "Let's start by defining a necessary condition for $K: \\mathcal{X} \\times \\mathcal{X} \\to [0,∞)$ to associated with a feature map.\n",
        "\n"
      ],
      "metadata": {
        "id": "fgKP2mPArOCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose that $K$ is a kernel for some feature map $\\phi$, and consider an arbitrary set of $n$ points $\\{x^{(1)}, x^{(2)},\\dots,x^{(n)}\\}$.\n",
        "\n",
        "Consider the matrix $L \\in \\mathbb{R}^{n \\times n}$ defined as $L_{ij} = K(x^{(i)}, x^{(j)}) = \\phi(x^{(i)})^\\top\\phi(x^{(j)})$. We claim that $L$ must be symmetric and positive semidefinite."
      ],
      "metadata": {
        "id": "a7N42LxYsqjY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indeed, $L$ is symmetric because the dot products $\\phi(x^{(i)})^\\top\\phi(x^{(j)})$ is symmetric. Moreover, for any $z$:\n",
        "\n",
        "\\begin{align*}\n",
        "z^\\top Lz &= \\sum_{i=1}^n\\sum_{j=1}^n z_i L_{ij}z_j = \\sum_{i=1}^n\\sum_{j=1}^n z_i \\phi(x^{(i)})^\\top\\phi(x^{(j)}) z_j \\\\\n",
        "&= \\sum_{i=1}^n\\sum_{j=1}^n z_i \\left(\\sum_{k=1}^n \\phi(x^{(i)})_k\\phi(x^{(j)})_k\\right) z_j \\\\\n",
        "& = \\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^n z_i \\phi(x^{(i)})_k\\phi(x^{(j)})_k z_j \\\\\n",
        "&= \\sum_{k=1}^n\\sum_{i=1}^n (z_i\\phi(x^{(i)})_k )^2 \\geq 0.\n",
        "\\end{align*}\n",
        "\n",
        "Thus if $K$ is a Kernel, $L$ must be positive semidefinite for any $n$ points $x^{(i)}$."
      ],
      "metadata": {
        "id": "3hO2VA1et3qK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mercer's Theorem**\n",
        "\n",
        "If $K$ is a Kernel, $L$ must be positive semidefinite for any $n$ points $x^{(i)}$. It turns out that it is also a sufficient condition:\n",
        "\n",
        "**Theorem**(Mercer): Let $K: \\mathcal{X} \\times \\mathcal{X} \\to [0,\\infty)$ be a kernel function. There exists a mapping feature $\\phi$ associated with $K$ if for any $n$ and any dataset $\\{x^{(i)}, x^{(2)}, \\dots, x^{(n)}\\}$ of size $n \\geq 1$, if and only if the matrix $L$ defined as $L_{ij} = K(x^{(i)}, x^{(j)})$ is symmetric and positive semidefinite.\n",
        "\n",
        "This characterizes precisely which kernel functions corresponds to some feature $\\phi$."
      ],
      "metadata": {
        "id": "nH9Nh6IOv4Cy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pros and Cons of Kernels**\n",
        "\n",
        "Are Kernels a free lunch? Not quite:\n",
        "* Kernels allow us to use feature $\\phi$ of very large dimension $d$.\n",
        "* Howeve computation is at least $O(n^2)$, where $n$ is the dataset size. We need to compute distances $K(x^{(i)}, x^{(j)})$ for all $i,j$.\n",
        "* Approximate solutions can be found more quickly, but in practice kernel methods are not used with today's massive datasets.\n",
        "* However, on small and medium-sized data, kernel method will be at least as good as neural nets and probably much easier to train."
      ],
      "metadata": {
        "id": "S32bg8yCxUXa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Summary: Kernels**\n",
        "* A kernel is a function $K: \\mathcal{X} \\times \\mathcal{X} \\to [0,\\infty)$ that defines a notion of similarity over pairs of vectors in $\\mathcal{X}$.\n",
        "* Kernels are often associated with high-dimensional feature $\\phi$ and inplicitly map inputs to this feature space.\n",
        "* Kernels can be incorporated into many machine learning algorithms, which enables them to learn highly nonlinear models.\n",
        "\n",
        "Examples of algorithms in which we can use kernel include:\n",
        "* Supervised learning algorithms: linear regression, logistic regression, support vector machine, etc.\n",
        "* Unsupervised learning algorithms: PCA, density estimation.\n",
        "\n",
        "Kernels are very powerful because they can be used throughout machine learning."
      ],
      "metadata": {
        "id": "gjxTCJCxyoQv"
      }
    }
  ]
}