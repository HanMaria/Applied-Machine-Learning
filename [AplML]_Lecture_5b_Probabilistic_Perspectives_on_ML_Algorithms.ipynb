{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 5b: Probabilistic Perspectives on ML Algorithms**\n",
        "## **Applied Machine Learning**"
      ],
      "metadata": {
        "id": "JmRynIaX4H-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 1: Probabilistic Linear Regression**\n",
        "Previously, we derived maximum likelihood learning as a general way of learning machine model.\\\n",
        "we will now see how the algorithms we have seen so far are special case of this principle.\n"
      ],
      "metadata": {
        "id": "jK8GIHrb6k-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Probabilistic Models**\n",
        "A probabilistic model is a probability distribution\n",
        "$$P(x,y): \\mathcal{X} \\times \\mathcal{Y} \\to [0,1].$$\n",
        "\n",
        "This model can approximate the data distribution $\\mathbb{P}$.\n",
        "\n",
        "If we know $P(x,y)$, we can use the conditional $P(y \\mid x)$ for prediction.\n",
        "\n",
        "Probabilistic models may also have parameters $\\theta \\in \\Theta$, which we will denote as:\n",
        "$$P_{\\theta}(x,y): \\mathcal{X} \\times \\mathcal{Y} \\to [0,1]$$ "
      ],
      "metadata": {
        "id": "-RTed3I57cVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Conditional Maximum Likelihood**\n",
        "A general approach pf optimizing the conditional models of the form $P(y \\mid x)$ by minimizing the expected KL Divergence based on the data distribution\n",
        "$$\\min_{\\theta}\\mathbb{E}_{x \\sim \\mathbb{P}}[D(\\mathbb{P}(y \\mid x))\\|P_{\\theta}(y \\mid x))].$$ \n",
        "\n",
        "With a bit of math, we can see that the maximum likelihood becomes:\n",
        "$$\\max_{\\theta}\\mathbb{E}_{x,y \\sim \\mathbb{P}}\\log P_{\\theta}(y \\mid x).$$\n",
        "\n",
        "This is the principle of *conditional maximum likelihood.*"
      ],
      "metadata": {
        "id": "5JmAKjczJZLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Least Squares**\n",
        "Recall that the linear regression algorithms fit a linear model of the form:\n",
        "$$f(x) = \\sum_{j=1}^n \\theta_j x_j = \\theta^\\top x. $$\n",
        "\n",
        "It minimizes the mean squared error (MSE)\n",
        "$$J(\\theta) = \\frac{1}{2n}\\sum_{i=1}^n(y^{(i)} - \\theta^\\top x^{(i)})^2 $$\n",
        "\n",
        "on a dataset $\\{(x^{(i)}, y^{(i)}) \\mid i = 1,2, \\cdots, n\\}$\n",
        "\n",
        "Is there a specific reason for us to be optimizing the mean squared error to fit our linear model?\n",
        "\n",
        "The answer to this can found by looking at the algorithm from a probabilistic perspective"
      ],
      "metadata": {
        "id": "y0YLpeuSLT3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Probabilistic Least Squares**\n",
        "Let's derive a probabilistic algorithm by defining a class of probabilistic models and use maximum likelihood as the objective.\n",
        "\n",
        "1.   Let's choose our model family $\\mathcal{M}$ to be the set of Gaussian distributions of the form\n",
        "$$p(y \\mid x; \\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(- \\frac{(y - \\theta^\\top x)^2}{2\\sigma^2}\\right).$$\n",
        "\n",
        "Each model $\\mathcal{N}(y; \\mu(x), \\sigma)$ is a Gaussian with a standard deviation $\\sigma$ of one and a mean of $\\mu(x) = \\theta^\\top x$ that is parametrized by $\\theta$.\n",
        "2.  We optimize the model using maximum likelihood. The log-likelihood function at a point $(x,y)$ equals\n",
        "$$\\begin{align*}\n",
        "\\log L(\\theta) &= \\log p(y \\mid x; \\theta) = \\log \\left(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(- \\frac{(y - \\theta^\\top x)^2}{2\\sigma^2}\\right)\\right) \\\\\n",
        "&= - \\frac{(y - \\theta^\\top x)^2}{2\\sigma^2} + \\text{const.}\n",
        "\\end{align*}$$\n",
        "\n",
        "Note how this is the a mean squared error (MSE) objective!\n",
        "\n",
        "Thus, minimizing MSE is equivalent to maximizing the log-likelihood of a Normal distribution $\\mathcal{N}(y; \\mu(x), \\sigma)$.\n",
        "\n"
      ],
      "metadata": {
        "id": "g01EQMULNZbw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Algorithm: Gaussian Ordinary Least Sqares**\n",
        "\n",
        "\n",
        "\n",
        "*   **Type**: Supervised Learning (Regression)\n",
        "\n",
        "*   **Model class**: Linear models\n",
        "*   **Objective function**: Mean squared error\n",
        "*   **Optimizer**: Normal equations\n",
        "*   **Probabilistic Interpretation**: Conditional-Gaussian fit using max-likelihood\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iu-RbBkARbnm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Extensions of Gaussian Least Squares**\n",
        "\n",
        "This is an example of how we can interpret a machine learning algorithm in a probabilistic framework.\n",
        "\n",
        "We can see many algorithms that have these kinds of interpretations. Here are some simple extensions:\n",
        "\n",
        "We can use Gaussian model and also parametrize the standard deviation:\n",
        "\n",
        "*   This is called heteroscedatic regression, and allow us to obtain condidence intervals for our predictions.\n",
        "\n",
        "We can also parametrize other distribution, not only Gaussian\n",
        "\n",
        "\n",
        "*   Exponential or Gamma distribution for continuous variables,\n",
        "*   Bernoulli distribution for dicrete variables.\n",
        "\n",
        "This yields many new machine learning algorithms.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bw0J2tdQV6IO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 2: Bayesian Algorithms**\n",
        "We can also use the Bayesian ML we have learned to interpret several algorithms we've seen as special case of the Bayesian framework."
      ],
      "metadata": {
        "id": "LZWyIWaQhZxE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: The Bayesian Approach**\n",
        "In Bayesian algorithms, parameter $\\theta$ is a random variable whose value happens to be.\n",
        "\n",
        "We formulate the two models:\n",
        "\n",
        "\n",
        "*   A likelihood model $P(x,y \\mid \\theta)$ that defines the probability of $x,y$ by any fixed value of $\\theta$.\n",
        "*   A prior $P(\\theta)$ that specifies us existing belief about the distribution of the random variable $\\theta$.\n",
        "\n",
        "Together, the two models define the joint distribution\n",
        "$$P(x, y, \\theta) = P(x,y \\mid \\theta)P(\\theta)$$\n",
        "\n",
        "in which both $x, y$ and the parameters $\\theta$ are random variables.\n"
      ],
      "metadata": {
        "id": "Gu37e53jh12G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: A Posteriori Learning**\n",
        "Recall that in maximum a posteriori (MAP) learning, we optimize the following objective\n",
        "$$\\theta_{MAP} = \\arg \\max_{\\theta} \\left(\\log \\prod_{i=1}^n P(x^{(i)}, y^{(i)} \\mid \\theta) + \\log P(\\theta)\\right),$$\n",
        "\n",
        "Note that we used the same formula as we used for maximum log likelihood, except that we have added the prior term $P(\\theta)$"
      ],
      "metadata": {
        "id": "O2YfYYRujky6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Ridge Regression**\n",
        "Recall that a ridge regression algorithm fits a linear model\n",
        "$$f(x) = \\sum_{j=0}^d \\theta_j x_j = \\theta^\\top x,$$\n",
        "\n",
        "We minimize the L2-regularized mean squared error (MSE)\n",
        "\n",
        "$$J(\\theta) = \\frac{1}{2n}\\sum_{i=1}^n(y^{(i)} - \\theta^\\top x^{(i)})^2 + \\frac{\\lambda}{2}\\sum_{j=1}^d \\theta_j^2,$$\n",
        "\n",
        "on a dataset $\\{(x^{(i)}, y^{(i)}) \\mid i = 1,2,\\cdots, n\\}$. The term $\\frac{1}{2}\\sum_{j=1}^d\\theta_j^2 = \\frac{1}{2} \\|\\theta\\|_2^2$ is called the regulizer."
      ],
      "metadata": {
        "id": "RBe4H_Vvksc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Probabilistic Ridge Regression**\n",
        "We can intepret ridge regression as maximum a prosteriori (MAP) estimation as follow:\n",
        "\n",
        "\n",
        "\n",
        "1.   First, we select our model family $\\mathcal{M}$ is Gaussian distribution of the form (let's assume $x \\in \\mathbb{R}$ for simplitcity) \n",
        "$$p(y \\mid x; \\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(- \\frac{(y - \\theta^\\top x)^2}{2\\sigma^2}\\right).$$\n",
        "2.   We use a Gaussian prior with mean zero and variance $\\tau$ on the parameters $\\theta$\n",
        "$$p(\\theta) = \\prod_{j-1}^d\\frac{1}{\\sqrt{2\\pi}\\tau}\\left(-\\frac{\\theta_j^2}{2\\tau^2}\\right).$$\n",
        "3. We optimize the model using MAP approach. The objective at a point $(x,y)$ equals:\n",
        "$$\\begin{align*}\n",
        "\\log L(\\theta) &= \\log p(y \\mid x,\\theta) + \\log p(\\theta)\\\\\n",
        "&= \\log \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(- \\frac{(y - \\theta^\\top x)^2}{2\\sigma^2}\\right) + \\log \\prod_{j-1}^d\\frac{1}{\\sqrt{2\\pi}\\tau}\\left(-\\frac{\\theta_j^2}{2\\tau^2}\\right) \\\\\n",
        "&= - \\frac{(y - \\theta^\\top x)^2}{2\\sigma^2} - \\frac{1}{\\sqrt{2\\pi}\\tau}\\sum_{j=1}^d\\theta_j^2 + \\text{const.}\n",
        "\\end{align*}$$\n",
        "\n",
        "Thus, we saw that ridge regression actually amounts to perform MAP estimation with a Gaussian prior. The strength of the regulizar $\\lambda$ equals to $\\frac{1}{\\tau^@}$"
      ],
      "metadata": {
        "id": "3a5RtFTQnBU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Algorithms: Probabilistic Ridge Regression**\n",
        "\n",
        "\n",
        "*  Type: Supervised Learning (Regression)\n",
        "*  Model class: Linear models\n",
        "*  Objective: Mean squared error\n",
        "*  Optimizer: Normal equations\n",
        "* Probabilistic Inteprretation: Conditional Gaussian likelihood and Gaussian prior fit using MAP.\n",
        "\n"
      ],
      "metadata": {
        "id": "_Oh-xsv7siNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bayesian View on Machine Learning Algorithms**\n",
        "Very often, we can intergret classical ML algorithms as applications of the probabilistic or Bayesian approaches (although we can derive them in other ways as well)\n",
        "\n",
        "*   Regularization can often be seen as applying a prior on the weights,\n",
        "*   L1 regularization can be seen as applying a *Laplace* prior,\n",
        "*   Many other algorithms will have similar interpretations. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9ObM89qtLynf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 3: Bayesian Ridge Regression**\n",
        "Let's now look at an example of fully Bayesian machine learning algorithms.\n"
      ],
      "metadata": {
        "id": "d9_IbKrtRLyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: The Bayesian Approach**\n",
        "\n",
        "In Bayesian statistics, $\\theta$ is an *random variable* whose value happens to be unknown.\n",
        "\n",
        "We formulate the two models:\n",
        "*   A likelihood model $P(x, y \\mid \\theta)$ that defines the probability of $x, y$ for any fixed value of $\\theta$.\n",
        "*   A prior $P(\\theta)$ that specifies us existing belief about the distribution of the random variable $\\theta$.\n",
        "\n",
        "Together these two models define the joint probability:\n",
        "$$P(x,y,\\theta) = P(x,y\\mid \\theta)P(\\theta)$$\n",
        "\n",
        "in which both the $x,y$ and the parameters $\\theta$ are random variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "mi_aKzqCRo3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Ridge regression**\n",
        "Recall that a ridge regression algorithm fits a linear model:\n",
        "$$f(x) = \\sum_{j=0}^d \\theta_j \\cdot x_j = \\theta^\\top x.$$\n",
        "\n",
        "We minimize the L2-regularized mean squared error (MSE)\n",
        "$$J(\\theta) = \\frac{1}{2n}\\sum_{i=1}^n(y^{(i)} - \\theta^\\top x^{(i)})^2 + \\frac{1}{2}\\sum_{j=1}^d \\theta_j^2$$\n",
        "\n",
        "on a dataset $\\{(x^{(i)}, y^{(i)}) \\mid i= 1,2,\\dots,n\\}$. The term $\\frac{1}{2}\\sum_{j=1}^d\\theta^2 = \\frac{1}{2}\\|\\theta\\|_2^2$ is called the regularizer."
      ],
      "metadata": {
        "id": "QxGvrZ0-TIN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Probabilistic Ridge Regression**\n",
        "We can interpret ridge regression as maximum a posteriori (MAP) as follows:\n",
        "...."
      ],
      "metadata": {
        "id": "Ktwo1Lk7VpFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bayesian Predictions**\n",
        "Suppose we now want to predict the value of $y$ and $x$. Unlike in the frequentist setting, we no longer have a single estimate of $\\theta$ of the model params, but instead we have a distribution.\n",
        "\n",
        "The Bayesian approach to predicting $y$ given an input $x$ and a training dataset $\\mathcal{D}$ consists of taking the prediction of all the possible models\n",
        "$$P(y \\mid x, \\mathcal{D}) = \\int_{\\theta}P(y \\mid x, \\theta)P(\\theta \\mid \\mathcal{D})d\\theta.$$\n",
        "This is called the *pasteriori predictive* distribution. Note how each $P(x \\mid x, \\theta)$ is weighted by the probability of $\\theta$ given $\\mathcal{D}$"
      ],
      "metadata": {
        "id": "cyjeyomuWIND"
      }
    }
  ]
}