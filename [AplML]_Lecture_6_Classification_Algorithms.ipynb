{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 6. Classification Algorithms**\n",
        "## Applied Machine Learning"
      ],
      "metadata": {
        "id": "dVX8bN7DaNjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 1: Classification**\n",
        "So far, every supervised learning algorithm that we've seen has been an instance of regression.\n",
        "\n",
        "Next, let's look at somme classification algorithms. First, we will define what classification is "
      ],
      "metadata": {
        "id": "LFIVn4biaZTr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Components of A Supervised Learning Algorithms**\n",
        "At a high level, a supervised machine learning problem has the following structure:\n",
        "$$\\underbrace{\\text{Training Set}}_{\\text{Attributes + Features}} + \\underbrace{\\text{Learning Algorithm}}_{\\text{Model class + Objective + Optimizer}} \\to \\text{Predictive Model}$$"
      ],
      "metadata": {
        "id": "yeMPthvPa9ui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Regression Vs. Classification**\n",
        "Consider a training dataset $\\mathcal{D} = \\{(x^{(i)}, y^{(i)})\\mid i= 1,2,\\dots, n\\}.$\n",
        "\n",
        "We distinguish between two types of sunpervised learning problems depending the targets $y^{(i)}$.\n",
        "\n",
        "\n",
        "1.   **Regression:** The target variable $y \\in \\mathcal{Y}$ is continuous: $\\mathcal{Y} \\subseteq \\mathbb{R}$ \n",
        "2.   **Classification**: The target variable $y$ is discrete and takes on one of $K$ possible values: $\\mathcal{Y}=\\{y_1, y_2, \\dots, y_K\\}$. Each dicrete value corresponds to a *class* that we want to predict.\n"
      ],
      "metadata": {
        "id": "fXes4jNCa9x4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Binary Classification**\n",
        "An important special case of classification is when the number of classes $K=2$.\n",
        "\n",
        "In this case, we have an example of *binary classification* problem."
      ],
      "metadata": {
        "id": "NiPuyo0Xa919"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Classification Dataset: Iris Flowers**\n",
        "To demonstrate classification algorithms, we are going to use the Iris flower dataset.\n",
        "\n",
        "It's a classical dataset was published by [R.A.Fisher]() in 1936. Nowadays, it's widely used for demonstrating machine learning algorithms."
      ],
      "metadata": {
        "id": "Kow2zwXPfTrj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoC9FL6jaAnR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "from sklearn import datasets\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = datasets.load_iris(as_frame=True)\n",
        "\n",
        "print(iris.DESCR)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print part of dataset\n",
        "iris_X, iris_y = iris.data, iris.target\n",
        "pd.concat([iris_X,iris_y], axis=1).head()"
      ],
      "metadata": {
        "id": "mGyKEJxihcNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a visualization of the dataset in 3D. Note that we are using the first 3 features (out of 4) of tha data."
      ],
      "metadata": {
        "id": "2ROfj4DPinwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "fig = plt.figure(1, figsize=(8, 6))\n",
        "ax = fig.add_subplot(111, projection=\"3d\", elev=-150, azim=110)\n",
        "\n",
        "X_reduced = PCA(n_components=3).fit_transform(iris.data)\n",
        "p1 = ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=iris_y,\n",
        "                cmap=plt.cm.Set1, edgecolor=\"k\", s=40)\n",
        "\n",
        "ax.set_title(\"First three PCA directions\")\n",
        "ax.set_xlabel(\"Sepal length\")\n",
        "ax.xaxis.set_ticklabels([])\n",
        "ax.set_ylabel(\"Sepal width\")\n",
        "ax.yaxis.set_ticklabels([])\n",
        "ax.set_zlabel(\"Petal length\")\n",
        "ax.zaxis.set_ticklabels([])\n",
        "\n",
        "plt.legend(handles=p1.legend_elements()[0], labels=['Iris Setosa', 'Iris Versicolour', 'Iris Virginica'])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zrbg8kIq3X5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Understanding Classification**\n",
        "How is classification different from regression?\n",
        "\n",
        "\n",
        "*   In regression, we try to fit a curve through the set of $y^{(i)}$.\n",
        "*   **In classification, classes define a partition of the feature space, and our goal is to find the boundaries that separate these regions.**\n",
        "*   Output of classification models have a simple probabilistic interpretation: they are probabilities that a data point belongs to a given class.\n",
        "\n",
        "Let's visualise our Iris dataset to see this. Note that we are using the first two features in this dataset.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gBAEJ0-HmHHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] =[12,4]\n",
        "\n",
        "# Plot also the training points\n",
        "p1 = plt.scatter(iris_X.iloc[:,0], iris_X.iloc[:,1],  c=iris_y,\n",
        "                 edgecolor='k', s=50, cmap = plt.cm.Paired)\n",
        "plt.xlabel('Sepal length')\n",
        "plt.ylabel('Sepal width')\n",
        "plt.legend(handles=p1.legend_elements()[0], labels=['Setosa', 'Versicolour', 'Virginica'], loc='lower right')"
      ],
      "metadata": {
        "id": "Y9wHJD4loysQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train a classification algorithm on this model \\\n",
        "\n",
        "Below we see the regions predicted to be associated with the blue and non-blue classes and the line between them is the decision boundary."
      ],
      "metadata": {
        "id": "i8Ivi9TWqM90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(C=1e5)\n",
        "\n",
        "# Create an instance of Logistic Regression Classifier and fit the data\n",
        "X = iris_X.to_numpy()[:,:2]\n",
        "# Rename class 2 to class 1\n",
        "iris_y2 = iris_y.copy()\n",
        "iris_y2[iris_y2==2] = 1\n",
        "Y = iris_y2\n",
        "logreg.fit(X,Y)\n",
        "\n",
        "xx, yy = np.meshgrid(np.arange(4, 8.2, .02), np.arange(1.8, 4.5, .02))\n",
        "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# Put the results into the color plot\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.pcolormesh(xx,yy,Z,cmap=plt.cm.Paired)\n",
        "\n",
        "# PLot also the training points\n",
        "plt.scatter(X[:,0], X[:,1], c=Y, edgecolor='k', cmap=plt.cm.Paired, s=50)\n",
        "plt.xlabel('Sapal length')\n",
        "plt.ylabel('Sapal width')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eVcpjcclqnYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 2: Nearest Neighbors**\n",
        "Previously, we have seen what defines a classification problem. Let's now look at our first classification algorithm."
      ],
      "metadata": {
        "id": "kqPfFLXOtQW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Classification**\n",
        "Consider a training dataset $\\mathcal{D} = \\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \\dots, (x^{(n)},y^{(n)})\\}.$\n",
        "\n",
        "We distinguish between two types of supervised learning problems depending on the targets $y^{(i)}$\n",
        "\n",
        "\n",
        "1.   **Regression:** The target variable $y \\in \\mathcal{Y}$ is continuous: $\\mathcal{Y} ⊆ \\mathbb{R}$\n",
        "2.   **Classification:** The target variable $y$ is dicrete and take on one of $K$ possible values: $\\mathcal{Y} = \\{y_1, y_2, \\dots, y_K\\}$. Each dicrete value corresponds to a *class* that we want to predict.\n",
        "\n"
      ],
      "metadata": {
        "id": "pafTCqTQtqWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **A simple Classification Algorithm: Nearest Neighbors**\n",
        "Suppose we have a training dataset $\\mathcal{D} = \\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \\dots, (x^{(n)},y^{(n)})\\}.$ At inference time, we receive a query point $x'$ and we want to predict its label $y'$.\n",
        "\n",
        "A really simple but suprisingly effective way of returning $y'$ is the nearest neighbors approach.\n",
        "\n",
        "\n",
        "*   Given a query datapoints $x'$, find the training example $(x,y)$ in $\\mathcal{D}$ that's is closest to $x'$, in the sense that $x$ is \"nearest\" to $x'$. \n",
        "*   Return $y$, the label of \"nearest neighbor\" $x$.\n",
        "\n",
        "In the sample below on the Iris dataset, the red cross denotes the query $x'$. The closest class to it is \"Virginica\". (We're only using the first two features in the datset for simplicity)\n",
        "\n"
      ],
      "metadata": {
        "id": "EEz4ms4BvWTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [12,4]\n",
        "\n",
        "# Plot also the training points\n",
        "p1 = plt.scatter(iris_X.iloc[:, 0], iris_X.iloc[:, 1], c=iris_y,\n",
        "                 edgecolor='k', s=50, cmap=plt.cm.Paired )\n",
        "p2 = plt.plot([7.5], [4], \"rx\", ms=10, mew=5)\n",
        "plt.xlabel('Sepal length')\n",
        "plt.ylabel('Sepal width')\n",
        "plt.legend(['Query point', 'Training date'], loc='lower right')\n",
        "plt.legend(handles=p1.legend_elements()[0]+p2, labels = ['Setosa', 'Versicolour', 'Virginica', 'Query'], loc='lower right')"
      ],
      "metadata": {
        "id": "_8qyHmKjs1WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Choosing a Distance Function**\n",
        "How do we select the point $x$ that is closest to $x'$? There are many options:\n",
        "\n",
        "\n",
        "*   The Euclide distance $\\|x - x'\\|_2 = \\sqrt{\\sum_{j=1}^d|x_j - x_j'|^2}$ is a popular choice.\n",
        "*   The Minkowski distancr $\\|x - x'\\|_p = (\\sum_{j=1}^d|x_j - x_j'|^p)^{1/p}$ generalizes the Euclidean, L1 and other distances.\n",
        "*   The Mahalanobis distance $\\sqrt{xVx^\\top}$ for a positive semidefinite matrix $V \\in \\mathbb{R}^{d \\times d}$ also generalizes the Euclidean distance.\n",
        "*   Discrete-valued inputs can be examined via the Hamming distance $|j: x_j \\neq x_j'|$ and other distances.\n",
        "\n",
        "Let's apply Nearest Neighbors to the above dataset using the Euclidean distance (or equivalently Minkowski with $p=2$) \n",
        "\n"
      ],
      "metadata": {
        "id": "y6vqI2rN3TQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import neighbors\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# Train a nearest neighbors model\n",
        "clf = neighbors.KNeighborsClassifier(n_neighbors=1, metric='minkowski', p=2)\n",
        "clf.fit(iris_X.iloc[:,:2], iris_y)\n",
        "\n",
        "# Create color maps\n",
        "cmap_light = ListedColormap(['orange', 'cyan', 'cornflowerblue'])\n",
        "cmap_bold = ListedColormap(['darkorange', 'c', 'darkblue'])\n",
        "\n",
        "# Plot the decision boundary. For that, we will asign a color to each point\n",
        "# in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "x_min, x_max = iris_X.iloc[:, 0].min() - 1, iris_X.iloc[:, 0].max() + 1\n",
        "y_min, y_max = iris_X.iloc[:, 1].min() - 1, iris_X.iloc[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.2),\n",
        "                     np.arange(y_min, y_max, 0.2))\n",
        "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# Put the result into the color plot\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.figure()\n",
        "plt.pcolormesh(xx, yy, Z, cmap = cmap_light)\n",
        "\n",
        "# PLot also the training points\n",
        "plt.scatter(iris_X.iloc[:,0], iris_X.iloc[:,1], c=iris_y, cmap=cmap_bold,\n",
        "            edgecolor='k', s=60)\n",
        "plt.xlim(xx.min(), xx.max())\n",
        "plt.ylim(yy.min(), yy.max())\n",
        "\n",
        "plt.xlabel('Sepal length')\n",
        "plt.ylabel('Sepal width')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cIoKhP0K5_XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above example, the regions of the 2D space that are assigned to each class are highly irregular. In the area where the two classes overlap, the decision of the boundary flips between the classes, depending on which point is closest to it. "
      ],
      "metadata": {
        "id": "1pSAiPaw-Xyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **K-Nearest Neighbors**\n",
        "Intuitively, we expect the true decision boundary to be smooth. Therefore, we average $K$ nearest neighbors at a query point.\n",
        "\n",
        "*   Given a query datapoint $x'$, find the $K$ training examples $\\mathcal{N} = \\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \\dots, (x^{(K)},y^{(K)})\\} \\subseteq \\mathcal{D}$ that are closest to $x'$.\n",
        "*   Return $y_{\\mathcal{N}}$, the consensus label of the neighborhood $\\mathcal{N}$.\n",
        "\n",
        "The consensus $y_{\\mathcal{N}}$ can be determined by voting, weighted average, etc.\n",
        "\n",
        "Let's look at Nearest Neighbors with a neighborhood of 30. The decision boudary is much smoother than before.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s-oidUHT-6Bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Nearest Neighbors Model\n",
        "clf = neighbors.KNeighborsClassifier(n_neighbors=30, metric='minkowski', p=2)\n",
        "clf.fit(iris_X.iloc[:,:2], iris_y)\n",
        "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# Put the result into the color plot\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.figure()\n",
        "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
        "\n",
        "# Plot also the training points\n",
        "plt.scatter(iris_X.iloc[:,0], iris_X.iloc[:,1], c=iris_y,\n",
        "            edgecolor='k', s=60, cmap=cmap_bold)\n",
        "plt.xlim(xx.min(), xx.max())\n",
        "plt.ylim(yy.min(), yy.max())\n",
        "\n",
        "plt.xlabel('Sepal Length')\n",
        "plt.ylabel('Sepal Width')"
      ],
      "metadata": {
        "id": "OU09Uco_9ibX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Data Distribution**\n",
        "We will assume that the dataset is governed by a probability distribution $\\mathbb{P}$, which we will call the *data distribution*. We will denote this as:\n",
        "$$x, y \\sim \\mathbb{P}.$$\n",
        "\n",
        "The training set $\\mathcal{D} = \\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \\dots, (x^{(n)},y^{(n)})\\}$ consists of independent and identically distributed (IID) samples from $\\mathbb{P}$."
      ],
      "metadata": {
        "id": "6RcWbQu9W-1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **KNN Estimates Data Distribution** \n",
        "Suppose that the output $y'$ of KNN is the average target in the neighborhood $\\mathcal{N}(x')$ around the query $x'$. Observe that we can write:\n",
        "$$y' = \\frac{1}{K}\\sum_{(x,y) \\in \\mathcal{N}(x')}y ≈ \\mathbb{E}(y \\mid x').$$\n",
        "\n",
        "*   When $x \\approx x'$ and when $\\mathbb{P}$ is reasonably smooth, each $y$ for $(x, y) \\in \\mathcal{N}(x')$ is approximately a sample from $\\mathbb{P}(y \\mid x')$ (since $\\mathbb{P}$ dosen't change much around $x'$, $\\mathbb{P}(y \\mid x')$ ≈ $\\mathbb{P}(y \\mid x)$)\n",
        "*   Thus $y'$ is essentially a Monte Carlo estimate of $\\mathbb{E}(y \\mid x')$ (the average of $K$ samples from $P(y \\mid x')$)\n",
        "\n"
      ],
      "metadata": {
        "id": "k8AUkJ0EX-hQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Algorithm**: K-Nearest Neighbors\n",
        "\n",
        "\n",
        "*   **Type:** Supervised Learning (regression and Classification).\n",
        "*   **Model Family:** Consensus over $K$ training instances.\n",
        "*   **Objective function:** Euclidean, Minkowski, Hamming, etc.\n",
        "*   **Optimizer:** Non at training. Nearest neighbor search at inference using specialized search algorithms (Hashing, KD-trees)\n",
        "*   **Probabilistic and Interpretation:** Directly approximating the density $\\mathbb{P}(y \\mid x).$\n",
        "\n"
      ],
      "metadata": {
        "id": "6D5AcZ_WaMGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pros and Cons of KNN**\n",
        "\n",
        "*   **Pros**\n",
        " * Can approximate any data distribution arbitrarily well.\n",
        "*   **Cons**\n",
        " * Need to store entire dataset to make queries, which is computationally prohibitive.\n",
        " * Number of data needed scale exponentially with dimension (curse of dimensonality)\n",
        "\n"
      ],
      "metadata": {
        "id": "LfFBVSdscMHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 3: Non-parametric Models**\n",
        "Nearest-Neighbors is the first example of an important type of machine learning algorithm called a non-parametric model."
      ],
      "metadata": {
        "id": "LsHyvJgeh-On"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Supervised Learning Model**\n",
        "We'll say that a model is a function:\n",
        "$$f : \\mathcal{X} \\to \\mathcal{Y}$$\n",
        "\n",
        "that maps inputs $x \\in \\mathcal{X}$ to targets $y \\in \\mathcal{Y}$.\n",
        "\n",
        "Often, models have parameters $\\theta \\in \\Theta$. We will then write a model as:\n",
        "$$f_{\\theta}: \\mathcal{X} \\to \\mathcal{Y}$$\n",
        "\n",
        "to denote that it's parametrized by $\\theta$."
      ],
      "metadata": {
        "id": "8WpPj-QbNa9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: K-Nearest Neighbors**\n",
        "Suppose that we are given a training set $\\mathcal{D} = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots, (x^{(n)}, y^{(n)})\\}$. At inference time, we recieve a query point $x'$ and we want to predict its label $y'$.\n",
        "\n",
        "*   Given a query point datapoint $x'$, find the $K$ training examples $\\mathcal{N} = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots, (x^{(K)}, y^{(K)})\\} \\subseteq D$ that are closet to $x'$. \n",
        "*   Return $y_{\\mathcal{N}}$, the consensus label of neighborhood $\\mathcal{N}$\n",
        "\n",
        "The consensus $y_{\\mathcal{N}}$ can be determined by voting, weighted average, etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZucZkob1NbMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Non-Parametric Models**\n",
        "Nearest Neighbors is an example of a *non-parametric* model. Parametric vs. non-parametric is a key distinguishing characteristic for machine learning models. \n",
        "\n",
        "A parametric model $f_{\\theta}: \\mathcal{X} \\times \\Theta \\to \\mathcal{Y}$ is defined by a finite set of parameters $\\theta \\in \\Theta$ whose dimensionality is constant with respect to the dataset. Linear models of the form \n",
        "$$f_{\\theta}(x) = \\theta^\\top x$$\n",
        "\n",
        "are an example of a parametric model.\n",
        "\n",
        "In non-parametric model, the function $f$ uses the entire training dataset (or a post-processed version of it) to make predictions, as in $K$-Nearest Neighbors. In other words, the complexity of the model increases with dataset size. \n",
        "\n",
        "Non-parametric models have the advantage of not loosing any information at training time. However, they are also computationally less tractable and may easily overfit the training set."
      ],
      "metadata": {
        "id": "q13HNw6QNbex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 4: Logistic Regression**\n",
        "Next, we are going to see a simple parametric classification algorithm that addresses many of these limitations of Nearest Neighbors."
      ],
      "metadata": {
        "id": "0PH0Fc2gTrzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Classification**\n",
        "Consider a training dataset $\\mathcal{D} = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots, (x^{(n)}, y^{(n)})\\}$\n",
        "\n",
        "We distinguish between two types of supervised learning problems depending on the targets $y^{(i)}$.\n",
        "\n",
        "1.   **Regression:** The target variable $y \\in \\mathcal{Y}$ is continuous: $\\mathcal{Y} \\in \\mathbb{R}$ \n",
        "2.   **Classification:** The target variable $y$ is discrete and takes on one of $K\n",
        "$ possible values: $\\mathcal{Y}=\\{y_1, y_2, \\dots,y_K\\}$. Each discrete value corresponds to a class that we want to predict. \n"
      ],
      "metadata": {
        "id": "Uu8s1Eg1UOEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Binary Classification and the Iris datset**\n",
        "\n",
        "We are going to start by looking at binary (two-classes) classification.\n",
        "\n",
        "To keep things simple, we will use Iris dataset. We will be predicting the difference between class 0 (Iris setosa) and the other two classes.\n",
        "\n"
      ],
      "metadata": {
        "id": "GeFK2gRxUOJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [12,4]\n",
        "# Rename class two to class one \n",
        "iris_y2 = iris_y.copy()\n",
        "iris_y2[iris_y2==2] = 1\n",
        "\n",
        "# Plot also the training points\n",
        "p1 = plt.scatter(iris_X.iloc[:,0], iris_X.iloc[:,1], c=iris_y2,\n",
        "                 edgecolor='k', s=50, cmap=plt.cm.Paired)\n",
        "plt.xlabel('Sepal length')\n",
        "plt.ylabel('Sepal width')\n",
        "plt.legend(handles=p1.legend_elements()[0], labels=['Setosa', 'Non-setosa'], loc='lower right')"
      ],
      "metadata": {
        "id": "srvx2Lq6ZRr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Least Squares**\n",
        "Recall that the linear regression algorithm fits a linear model of the form:\n",
        "$$f(x) = \\sum_{j=0}^d\\theta_jx_j = \\theta^\\top x.$$\n",
        "\n",
        "It minimizes the mean squared error (MSE) \n",
        "$$J(\\theta) = \\frac{1}{2n}\\sum_{i=1}^n(y^{(i)} - \\theta^\\top x^{(i)})^2$$\n",
        "\n",
        "on a dataset $\\mathcal{D} = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots, (x^{(n)}, y^{(n)})\\}$"
      ],
      "metadata": {
        "id": "tVay-5enUOR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could also the above model for classification problem for which $\\mathcal{Y} = \\{0,1\\}$"
      ],
      "metadata": {
        "id": "4b7A74uGczoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.lib.function_base import meshgrid\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.linear_model import LinearRegression\n",
        "logreg = LinearRegression()\n",
        "\n",
        "# Create an instance of linear regression classifier and fit tge data\n",
        "X = iris_X.to_numpy()[:,:2]\n",
        "Y = iris_y2\n",
        "logreg.fit(X,Y)\n",
        "\n",
        "# Plot the decision boundary. For that, we will assign a color to each point\n",
        "# in the mesh [x_min, x_max]x[y_min, y_max]\n",
        "x_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\n",
        "y_min, y_max = X[:,1].min() -.5, X[:,1].max() + .5\n",
        "h = .02 # stepsize in the mesh\n",
        "\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z[Z>0.5] = 1\n",
        "Z[Z<0.5] = 0\n",
        "\n",
        "# Put the result into the color plot\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "\n",
        "# Plot also the training ponts\n",
        "plt.scatter(X[:,0], X[:,1], c=Y, edgecolor='k', s=60, cmap = plt.cm.Paired)\n",
        "plt.xlabel('Sepal length')\n",
        "plt.ylabel('Sepal width')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3U3ENhOvWbhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Least squares returns an acceptable decision boundary on this dataset. However, it is problematic for a few reasons:\n",
        "\n",
        "\n",
        "*   There is nothing to prevent ouputs larger than one or smaller than zero, which is conceptually wrong.\n",
        "*   We also don't have optimal performance: at least one point is misclassified, and others are too close to the decision boundary.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jm3PaX1ZBRvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The Logistic Regression**\n",
        "To address this problem, we will look at a different hypothesis class. We will choose models of the form:\n",
        "$$f(x) = \\sigma (\\theta^\\top x) = \\frac{1}{1+ \\exp(- \\theta^\\top x)},$$\n",
        "\n",
        "where\n",
        "$$\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$$\n",
        "\n",
        "is know as the *sigmoid* or *logistic* function.\n",
        "\n",
        "The logistic funtion $\\sigma : \\mathbb{R} \\to [0,1]$ 'squeezes' points in the real line into $[0,1]$"
      ],
      "metadata": {
        "id": "tEKlEtUeCZxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def logistic(z):\n",
        "  return 1/(1 + np.exp(-z))\n",
        "\n",
        "z = np.linspace(-5,5)\n",
        "plt.plot(z, logistic(z))"
      ],
      "metadata": {
        "id": "HGHEtWsiAppe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The Logistic Function: Properties**\n",
        "The sigmoid (or logistic) function is defined as:\n",
        "$$\\sigma(z) = \\frac{1}{1 + \\exp(-z)}.$$\n",
        "\n",
        "A few observations:\n",
        "\n",
        "\n",
        "*   This tends to $1$ as $z \\to ∞$ and tends to $0$ as $z \\to - ∞$\n",
        "*   Thus models of the form $\\sigma(\\theta^\\top x)$ output values between $[0,1]$, which is suitable for binary classification.\n",
        "*   It's easy to show that the derivative of $\\sigma(z)$ has a simple form $\\frac{d\\sigma}{dz} = \\sigma(z)(1 - \\sigma(z))$.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3kmDliitGQVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's implement our model using sigmoid function"
      ],
      "metadata": {
        "id": "mU9cU894GQ0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(X, theta):\n",
        "  return logistic(X.dot(theta))"
      ],
      "metadata": {
        "id": "H_P1T6TJGC9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Probabilistic Least Squares**\n",
        "Recall that least squares can be interpreted as fitting a Gaussian probabilistic model:\n",
        "$$p(y \\mid x; \\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(y - \\theta^\\top x)^2}{2\\sigma^2}\\right).$$\n",
        "\n",
        "The log-likelihood of this model at a point $(x,y)$ equals:\n",
        "$$log L(\\theta) = log p(y \\mid x; \\theta) = \\text{const}_1 \\cdot (y - \\theta^\\top x)^2 + \\text{const}_2.$$\n",
        "\n",
        "for some constant $\\text{const}_1$ and $\\text{const}_2$\n",
        "\n",
        "Least squares thus amounts to fitting a Gaussian $\\mathcal{N}(y; \\mu(x), \\sigma)$ with a standard deviation of one and mean of $\\mu(x) = \\theta^\\top x$."
      ],
      "metadata": {
        "id": "dlHUQk1cIZ4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **A Probabilistic Approach to Classifiaction**\n",
        "We can take this probabilistic perspective to derive a new algorithm for binary classification.\n",
        "\n",
        "We will start by using our logistic model to parametrize a probability distribution as follows:\n",
        "$$\\begin{align*}\n",
        "p(y = 1 \\mid x; \\theta ) &= \\sigma(\\theta^\\top x)\\\\\n",
        "p(y = 0 \\mid x; \\theta ) &= 1 -  \\sigma(\\theta^\\top x).\n",
        "\\end{align*}$$\n",
        "\n",
        "A probability over $y \\in \\{0,1\\}$ of the form $P(y=1) = p$ is called *Bernoulli*\n",
        "\n",
        "Note that we can write this compactly as:\n",
        "$$p(y \\mid x; \\theta) = \\sigma(\\theta^\\top x)^y(1 - \\sigma(\\theta^\\top x))^{1-y}$$"
      ],
      "metadata": {
        "id": "06VgSltbNZl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Conditional Maximum Likelihood**\n",
        "A general approach of optimizing the conditional models of the form $P_{\\theta}(y\\mid x)$ is by minimizing the expectted KL-divergence with respect to the data distribution\n",
        "$$\\min_{\\theta}\\mathbb{E}_{x \\sim \\mathbb{P}}[D(\\mathbb{P}(y \\mid x) \\| P_{\\theta}(y \\mid x))].$$\n",
        "\n",
        "with a bit of math, we can show that the *maximum likelihood objective* becomes\n",
        "$$\\max_{\\theta}\\mathbb{E}_{x \\sim \\mathbb{P}}\\log P_{\\theta}(y \\mid x).$$\n",
        "\n",
        "This is the principle of *conditional maximum likelihood*.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mtQ97aURPut-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Applying Maximum Likelihood**\n",
        "Following the priciple of maximum likelihood, we want the optimizing the following objective defined over the training set $\\mathcal{D} = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots, (x^{(n)}, y^{(n)})\\}$.\n",
        "\n",
        "$$\\begin{align*}\n",
        "L(\\theta) &= \\prod_{i=1}^np(y^{(i)} \\mid x^{(i)}; \\theta)\\\\\n",
        "&= \\prod_{i=1}^n\\sigma(\\theta^\\top x^{(i)})^{y^{(i)}}(1 - \\sigma(\\theta^\\top x^{(i)}))^{1 - y^{(i)}}.\n",
        "\\end{align*}$$\n",
        "\n",
        "The log of this objective is also often called the *log-loss* or **cross-entropy**"
      ],
      "metadata": {
        "id": "sJDj0WyRRkXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model and objective function define **logistic regression**, one of the most widely used classification algorithms (the name *regression* is an unfortunate misnomer! )"
      ],
      "metadata": {
        "id": "S0TKbrSwTxe6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's implement the likelihood objective"
      ],
      "metadata": {
        "id": "66-LDH2BVDqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_likelihood(theta, X, y):\n",
        "  \"\"\"\n",
        "  The cost function J(theta0, theta1) describing the goodness of the fit.\n",
        "\n",
        "  We add the 1e-6 term in order to avoid the overflow (inf and -inf).\n",
        "\n",
        "  Parameters:\n",
        "  theta (np.array): d-dimensional vector of parameters\n",
        "  X (np.array): (n,d)-dimensional design matrix\n",
        "  y (np.array): n-dimensional vector of targets\n",
        "  \"\"\"\n",
        "\n",
        "  return (y*np.log(f(X, theta) + 1e-6) + (1-y)*np.log(1 - f(X, theta) + 1e-6)).mean()"
      ],
      "metadata": {
        "id": "XAH2_x18VIfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Gradient Descent**\n",
        "If we want to optimize $J(\\theta)$, we start with an initial guess $\\theta_0$ for the parameters and repeat the following update:\n",
        "$$\\theta_i := \\theta_{i-1} - \\alpha\\nabla J(\\theta_{i-1}).$$\n",
        "\n"
      ],
      "metadata": {
        "id": "emLQVkGgXV3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Derivatives of the Log-Likelihood**\n",
        "Let's wprk out the gradient for our log-likelihood objective:\n",
        "$$\\begin{align*}\n",
        "\\frac{\\partial \\log L(\\theta)}{\\partial \\theta_j} &= \\frac{\\partial}{\\partial \\theta_j} \\log (\\sigma(\\theta^\\top x)^y(1 - \\sigma(\\theta^\\top x))^{1-y})\\\\\n",
        "&= \\left(y \\cdot \\frac{1}{\\sigma(\\theta^\\top x)} - (1-y)\\frac{1}{1 - \\sigma(\\theta^\\top x)}\\right)\\frac{\\partial}{\\partial \\theta_j}\\sigma(\\theta^\\top x)\\\\\n",
        "&= \\left(y \\cdot \\frac{1}{\\sigma(\\theta^\\top x)} - (1-y)\\frac{1}{1 - \\sigma(\\theta^\\top x)}\\right)\\sigma(\\theta^\\top x)(1 - \\sigma(\\theta^\\top x))\\frac{\\partial}{\\partial \\theta_j}\\theta^\\top x\\\\\n",
        "&= (y \\cdot (1 - \\sigma(\\theta^\\top x)) -  (1-y) \\cdot \\sigma(\\theta^\\top x))x_j\\\\\n",
        "&= (y - f_{\\theta}(x))\\cdot x_j.\n",
        "\\end{align*}$$"
      ],
      "metadata": {
        "id": "7QroBzaVYDuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradient of the Log-Likelihood**\n",
        "Using the above extension, we obtain the following gradient:\n",
        "$$\\nabla_{\\theta}J(\\theta) = (y - f_{\\theta}(x))\\cdot \\bf{x}.$$\n",
        "\n",
        "This expression looks very similar to the gradient of mean square error, but it is different because the model $f_{\\theta}$ is different. "
      ],
      "metadata": {
        "id": "9pvrJ-nWbpoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's implement the gradient"
      ],
      "metadata": {
        "id": "DjVt-YxvclOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loglik_gradient(theta, X, y):\n",
        "  return np.mean((y - f(X, theta))*X.T, axis=1)"
      ],
      "metadata": {
        "id": "SspckYRZcoel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradient Desccent for Logistic Regression**\n",
        "Putting this together, we obtain a complete learning algorithm, logistic regression.\n",
        "\n",
        "```python\n",
        "theta, theta_prev = random_initialization()\n",
        "while abs(J(theta) - J(theta_prev)) > conv_threshold:\n",
        "    theta_prev = theta\n",
        "    theta = theta_prev - step_size * (f(x, theta)-y) * x\n",
        "```\n",
        "Let's implement this algorithm"
      ],
      "metadata": {
        "id": "EPmpcRr6dJYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 5e-5\n",
        "stepsize = 1e-1\n",
        "\n",
        "theta, theta_prev = np.zeros((3,)), np.ones((3,))\n",
        "opt_pts = [theta]\n",
        "opt_grads = []\n",
        "iter = 0\n",
        "iris_X['one'] = 1\n",
        "X_train = iris_X.iloc[:,[0,1,-1]].to_numpy()\n",
        "y_train = iris_y2.to_numpy()\n",
        "\n",
        "while np.linalg.norm(theta - theta_prev) > threshold:\n",
        "  if iter % 50000 == 0:\n",
        "    print('Iteration %d. Log-likelihood: %.6f' % (iter, log_likelihood(theta, X_train, y_train)))\n",
        "    theta_prev = theta\n",
        "    gradient = loglik_gradient(theta, X_train, y_train)\n",
        "    theta = theta_prev + stepsize*gradient\n",
        "    opt_pts += [theta]\n",
        "    opt_grads +=[gradient]\n",
        "    iter += 1"
      ],
      "metadata": {
        "id": "ezQ9RD5xQaoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now visualize the result"
      ],
      "metadata": {
        "id": "nDKpvHNWNLNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
        "Z = f(np.c_[xx.ravel(), yy.ravel(), np.ones(xx.ravel().shape)], theta)\n",
        "Z[Z<0.5] = 0\n",
        "Z[Z>=0.5] = 1\n",
        "\n",
        "# Put the result into the color plot\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "\n",
        "# Plot also the training points\n",
        "plt.scatter(X[:,0], X[:,1], c=Y, edgecolor='k', s=60, cmap=plt.cm.Paired)\n",
        "plt.xlabel('Sepal length')\n",
        "plt.ylabel('Sepal width')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M8tTz_rZLVAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how we use the algorithm via [sklearn]()"
      ],
      "metadata": {
        "id": "ZlNtjaW5Otmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(C=1e5, fit_intercept=True)\n",
        "\n",
        "# Create an instance of Logistic Regression Classifier and fit the data\n",
        "X = iris_X.to_numpy()[:,:2]\n",
        "Y = iris_y2\n",
        "logreg.fit(X,Y)\n",
        "\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
        "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# Put the result into the color plot\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "\n",
        "# Plot also the training points\n",
        "plt.scatter(X[:,0], X[:,1], c=Y, edgecolor='k', s=40, cmap=plt.cm.Paired)\n",
        "plt.xlabel('Sepal length')\n",
        "plt.ylabel('Sepal width')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UEgaopSPOytA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Observations About Logistic Regression**\n",
        "\n",
        "\n",
        "*   Logistic Regression finds a linear decision boundary, is the set of points for which $P(y = 1 \\mid x) = P(y = 0 \\mid x)$, or equivalently:\n",
        "$$0 = \\log \\frac{P(y = 1 \\mid x)}{P(y = 0 \\mid x)} = \\log \\frac{\\frac{1}{1 + \\exp(-\\theta^\\top x)}}{1 - \\frac{1}{1 + \\exp(-\\theta^\\top x)}} = \\theta^\\top x.$$\n",
        "The claim holds because $\\theta^\\top x = 0$ is a linear function.\n",
        "*   Unlike least squares, we don't have a close form solution for $\\theta$, but we can still apply gradient descent. \n",
        "\n"
      ],
      "metadata": {
        "id": "YTJMbUL6Q2uZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Algorithm: Logistic Regression**\n",
        "\n",
        "\n",
        "\n",
        "*   **Type**: Supervised learning (binary classification)\n",
        "*   **Model family**: Linear decision boundary \n",
        "*   **Objective function**: Cross-entropy, a special case of log-likelihood\n",
        "*   **Optimizer**: Gradient descent\n",
        "*   **Probabilistic Interpretation**: Parametrized Bernoulli distribution\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZNndDAjJS59b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 5: Multi-class classification**\n",
        "finally, let's look at an extension of logistic regression to an arbitrary number of classes."
      ],
      "metadata": {
        "id": "yTLMI0GaZqTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Logistic Regression**\n",
        "Logistic regression fits the model of form:\n",
        "$$f(x) = \\sigma(\\theta^\\top x) = \\frac{1}{1 + \\exp(-\\theta^\\top x)}$$\n",
        "\n",
        "where \n",
        "$$\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$$\n",
        "\n",
        "is known as the *sigmoid function* or *logistic function*."
      ],
      "metadata": {
        "id": "fbWcVT8NaCrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Multi-class Classification**\n",
        "Linear regression only applies to binary classification problems. What if we have an arbitrary number of classes $K$?\n",
        "\n",
        "*   The simplest approach that can be used by machine learning algorithm is the \"one vs. all\" approach. We train one classifier for each class to distinguish one class from all the others. This works, but loses a valid probabilistic interpretation and is not very elegant. Alternatively, we may fit a probabilistic model that outputs multi-class probabilities.\n"
      ],
      "metadata": {
        "id": "hucx8M-7bBf5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load a fully multiclass version of the Iris dataset.\n"
      ],
      "metadata": {
        "id": "fBebFCYEe9uL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot also the training points\n",
        "p1 = plt.scatter(iris_X.iloc[:,0], iris_X.iloc[:,1], c=iris_y, \n",
        "                 edgecolor='k', s=40, cmap=plt.cm.Paired)\n",
        "plt.xlabel('Sepal length')\n",
        "plt.ylabel('Sepal width')\n",
        "plt.legend(handles=p1.legend_elements()[0], labels=['Setosa', 'Versicolor', 'Virginica'], loc='lower right')\n"
      ],
      "metadata": {
        "id": "EKPb7hTkQgKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The Softmax Function**\n",
        "The logistic function $\\sigma : \\mathbb{R} \\to [0,1]$ be seen as mapping an input $\\vec{z} \\in \\mathbb{R}$ to a probability.\n",
        "\n",
        "Its multi-class extension $\\vec{\\sigma}:\\mathbb{R}^K \\to [0,1]^K$: maps a $K$-dimensional input $z \\in \\mathbb{R}$ to a $K$-dimensional vector of probabilities.\n",
        "\n",
        "Each components of $\\vec{\\sigma}(\\vec{z})$ is defined as:\n",
        "$$\\sigma(\\vec{z})_k = \\frac{\\exp(z_k)}{\\sum_{l=1}^K \\exp(z_l)}.$$\n",
        "\n",
        "We call this the *softmax* function.\n",
        "\n",
        "When $K=2$, this looks as follows:\n",
        "$$\\sigma(\\vec{z})_1 = \\frac{\\exp(z_1)}{\\exp(z_1) + \\exp(z_2)}.$$\n",
        "\n",
        "We can assume that $\\exp(z_1) = 1$ because multiplying the numerator and denominator doesn't change any of the probabilities (so we can just devide by $\\exp(z_1)$). Thus we obtain:\n",
        "$$\\sigma(\\vec{z})_1 = \\frac{1}{1 + \\exp(z_2)}.$$\n",
        "\n",
        "This is essentially our sigmoid function. Hence softmax function generalizes the sigmoid function."
      ],
      "metadata": {
        "id": "yjssKZPAgizK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The Softmax Model**\n",
        "We can use the softmax function to define a $K$-class classification model.\n",
        "\n",
        "In the binary classification setting, we mapped weights $\\theta$ and features $x$ into a probability as follows:\n",
        "$$\\sigma(\\theta^\\top x) = \\frac{1}{1 + \\exp(- \\theta^\\top x)},$$\n",
        "\n",
        "In the multi-class setting, we define a model $f: \\mathcal{X} \\to [0,1]^K$ that outputs the probability of class $k$ based on the feature $x$ and class-specific weights $\\theta_k$:\n",
        "$$\\sigma(\\theta_k^\\top x)_k = \\frac{\\exp(\\theta_k^\\top x)}{\\sum_{l=1}^K \\exp(\\theta_l^\\top x)}.$$ \n",
        "\n",
        "Its parameters space lies in $\\Theta^K$, where $\\Theta = \\mathbb{R}^d$ is the parameter space of logistic regression."
      ],
      "metadata": {
        "id": "_hREq5jNjsop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We may have noticed that this model is slightly over-parametrized: multiplying every $\\theta_k$ by a constant results in an equivalent model. For this reason, it is often assumed that one of the class weights $\\theta_l = 0$."
      ],
      "metadata": {
        "id": "28Ga6nN2l68z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Softmax Regression**\n",
        "We again take a probabilistic perspective to derive a $K$-class classification algorithms based on this model.\n",
        "\n",
        "We will start by using our softmax model to parametrize a probability distribution as follow:\n",
        "$$p(y=k \\mid x; \\theta) = \\vec{\\sigma}(\\theta^\\top x)_k$$\n",
        "\n",
        "This is called the **categorical distribution**, and it generalize the Bernoulli distribution.\n",
        "\n",
        "Following the principle of maximum likelihood, we want to optimize the following objective over a training dataset\n",
        "\n",
        "$$L(\\theta) = \\prod_{i=1}^n p(y^{(i)} \\mid x; \\theta) = \\prod_{i=1}^n  \\vec{\\sigma}(\\theta^\\top x^{(i)})_{y^{(i)}}.$$\n",
        "\n",
        "This model and objective function define *softmax regression*."
      ],
      "metadata": {
        "id": "lziQBCqWmvQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now implement softmax regression to Iris dataset by using the implementation from [sklearn]()"
      ],
      "metadata": {
        "id": "TLfIrZqNplHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(C=1e5, multi_class = 'multinomial')\n",
        "\n",
        "# Create an instance of Softmax Regression and fit the data\n",
        "logreg.fit(X,iris_y)\n",
        "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# Put the result into the color plot\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "\n",
        "# Plot also the training points\n",
        "plt.scatter(X[:,0], X[:,1], c=iris_y, cmap=plt.cm.Paired,\n",
        "            edgecolor='k', s=40)\n",
        "plt.xlabel('sepal length')\n",
        "plt.ylabel('Sepal width')\n"
      ],
      "metadata": {
        "id": "poYAlLEogTxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ALgorithm: Softmax Regression**\n",
        "\n",
        "\n",
        "*   **Type:** Supervised learning (classification)\n",
        "*   **Model Family:** Linear decision boundaries.\n",
        "*   **Objective function:** Softmax loss, a special case of log-likelihood\n",
        "*   **MOptimizer:** Gradient descent\n",
        "*   **Probabilistic and Interpretation:** Parametrized categorical distribution.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lwGC4pPSrq-h"
      }
    }
  ]
}