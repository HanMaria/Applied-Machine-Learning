{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 7: Generative Algorithms**\n",
        "# **Applied Machine Learning**"
      ],
      "metadata": {
        "id": "ig27LrDRgqyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 1: Generative Models**\n",
        "In this lecture, we are going to look at generative algorithms and their application to classification.\n",
        "We will start by defining the concept of a *generative model*"
      ],
      "metadata": {
        "id": "mXXUYoRcg4uU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Components of A Supervised Machine Learning Problem**\n",
        "\n",
        "At a high level, a supervised machine learning problem has the following structure:\n",
        "$$\\underbrace{\\text{Training Dataset}}_{\\text{Attributes + Features}} + \\underbrace{\\text{Learning Algorithm}}_{\\text{Model class + Objective + Optimizer}} \\to \\text{Predictive Model}$$\n",
        "\n"
      ],
      "metadata": {
        "id": "cyXdV1s7hQvX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Probabilistic Models**\n",
        "\n",
        "A (parametric) probabilistic model with parameters $\\theta$ is a probability distribution:\n",
        "\n",
        "$$P_{\\theta}(x,y) : \\mathcal{X} \\times \\mathcal{Y} \\to [0,1]$$\n",
        "\n",
        "This model can approximate the data distribution $\\mathbb{P}(x,y)$.\n",
        "\n",
        "If we know $P_{\\theta}(x,y)$, we can compute predictions using the formula:\n",
        "\n",
        "$$P_{\\theta}(y \\mid x) = \\frac{P_{\\theta}(x,y)}{P_{\\theta}(x)} = \\frac{P_{\\theta}(x,y)}{\\sum_{y \\in \\mathcal{Y}}P_{\\theta}(x,y)}.$$"
      ],
      "metadata": {
        "id": "XdVooA_2iDQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Maximum Likelihood Learning**\n",
        "In order to fit probabilistic models, we use the following objective:\n",
        "$$\\max_{\\theta}\\mathbb{E}_{x \\sim \\mathbb{P}}\\log P_{\\theta}(x,y).$$\n",
        "\n",
        "This seeks to find a model that assigns high probability to the training data."
      ],
      "metadata": {
        "id": "mKA2f-XeSRBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Conditional Probabilistic Models**\n",
        "Alternatively, we may define a model of the conditional probabilistic distribution:\n",
        "$$P_{\\theta}(y \\mid x): \\mathcal{X} \\times \\mathcal{Y} \\to [0,1].$$\n",
        "\n",
        "These are trained using conditional maximum likelihood:\n",
        "$$\\max_{\\theta}\\mathbb{E}_{x \\sim \\mathbb{P}} \\log P_{\\theta}(y \\mid x).$$\n",
        "\n",
        "This seeks to find a model that assigns high probability to the target $y$ for each $x$.\n",
        "\n",
        "Logistic Regression is an example of this approach."
      ],
      "metadata": {
        "id": "dH31yZDeTNLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Discriminative vs. Generative Models**\n",
        "These two types of models are also known as **generative** and **discriminative**.\n",
        "\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\underbrace{P_{\\theta}(x,y) : \\mathcal{X} \\times \\mathcal{Y} \\to [0,1]}_{\\text{generative model}} & \\; \\; &\n",
        "\\underbrace{P_{\\theta}(y \\mid x): \\mathcal{X} \\times \\mathcal{Y} \\to [0,1]}_{\\text{discriminative model}}\n",
        "\\end{align*}$$\n",
        "\n",
        "\n",
        "*   The models parametrize different kinds of probabilities.\n",
        "*   They involve different training objectives and make different predictions.\n",
        "*   Their uses are different (e.g., prediction, generation, etc.); more later!\n",
        "\n"
      ],
      "metadata": {
        "id": "Ba5FwFa_Uzwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Classification Dataset: Iris Flowers**\n",
        "\n",
        "To demonstrate the two approaches, we are going to use the Iris flower.\n",
        "\n"
      ],
      "metadata": {
        "id": "2vs9Ks1cWns7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0zuY-lafzT1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = datasets.load_iris(as_frame=True)\n",
        "\n",
        "# Print part of dataset\n",
        "iris_X, iris_y = iris.data, iris.target\n",
        "pd.concat([iris_X, iris_y], axis=1).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we only consider the first two feature columns, we can visualize the dataset in 2D"
      ],
      "metadata": {
        "id": "l4JLdkgiXpEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [12,4]\n",
        "\n",
        "# Create 2d version of dataset\n",
        "X = iris_X.to_numpy()[:,:2]\n",
        "x_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\n",
        "y_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\n",
        "\n",
        "# Plot also the training points\n",
        "p1 = plt.scatter(X[:,0], X[:,1], c=iris_y, edgecolor='k', s=40, cmap=plt.cm.Paired)\n",
        "plt.xlabel('Sepal lenght')\n",
        "plt.ylabel('Sepal width')\n",
        "plt.legend(handles=p1.legend_elements()[0], labels=['Setosa', 'Versicolor', 'Virginica'], loc='lower right')"
      ],
      "metadata": {
        "id": "Kxa72eDHXfYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Example: Disciminative Model**\n",
        "An example of discriminative model are logistic regression or softmax regression\n",
        "\n",
        "*   Discriminative models directly partition the feature space into regions associated with each class ans separated by the decision boundaries.\n",
        "*   Given a feature $x$, disciminative models directly map to the predicted classes (e.g., via the function $\\sigma(\\theta^\\top x)$ for logistic regression)\n",
        "\n"
      ],
      "metadata": {
        "id": "xJX-S-iiZ4tH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(C=1e5, multi_class='multinomial')\n",
        "\n",
        "# Create an instance of softmax regression and fit the data\n",
        "logreg.fit(X, iris_y)\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# Put the result into the color plot\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "\n",
        "# Plot also the training points\n",
        "plt.scatter(X[:,0], X[:,1], c=iris_y, edgecolor='k', s=60, cmap=plt.cm.Paired)\n",
        "plt.xlabel('Sepal length')\n",
        "plt.ylabel('Sepal width')"
      ],
      "metadata": {
        "id": "fkdekaApavql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Example: Generative Models**\n",
        "\n",
        "Generative modeling can be seen as taking a different approach:\n",
        "\n",
        "\n",
        "1.   In the Iris example, we first build a model of how each type of flower looks, i.e., we can learn the distribution.\n",
        "$$p(x \\mid y = k) \\; \\text{for each class} \\; k.$$\n",
        "\n",
        "It defines a model of how each flower is **generated**, hence the name\n",
        "2.   Given a new flower datapoint $x'$, we can match each against each flower model and find the type of flower that looks most similar to it. Mathematically, this corresponds to:\n",
        "$$\\begin{align*}\n",
        "\\arg\\max_{y}\\log p(y \\mid x) &= \\arg \\max_{y}\\log \\frac{ p(x \\mid y)p(y)}{p(x)}\\\\\n",
        "&= \\arg \\max_y \\log p(x \\mid y)p(y),\n",
        "\\end{align*}$$\n",
        "\n",
        "where we have applied Bayes' rule in the first line."
      ],
      "metadata": {
        "id": "1pk_4Qksb-Jd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Generative vs. Discriminative Approach**\n",
        "\n",
        "How do we know which approach is better?\n",
        "\n",
        "*   If we only care about prediction, we don't need to know the model of $P(x)$. We can solve precisely the problem we care about.\n",
        " * Discriminative models will be often more accurate. \n",
        "*   If we care about other tasks (e.g., generation, dealing with missing values, etc.) or if we know the true model is generative, we want to use the generative approach.\n",
        "\n"
      ],
      "metadata": {
        "id": "9A5UWJz-eBqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 2: Gaussian Discriminant Analysis**\n",
        "\n",
        "We are now going to conitue our discussion of classification\n",
        "\n",
        "\n",
        "*   We wiil see a new classification algorithm, Gaussian Discriminat Analysis.\n",
        "*   This will be our first example of generative machine learning model.\n",
        "\n"
      ],
      "metadata": {
        "id": "1MWNGZ8AeZ89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Classification**\n",
        "Considering a training dataset $\\mathcal{D} = \\{(x^{(i)}, y^{(i)}), i = 1,2,\\dots, n\\}$.\n",
        "\n",
        "We distinguishing two types of supervised learning problems depending on the targets $y^{(i)}$.\n",
        "\n",
        "\n",
        "1.   **Regression**: The target variable $y \\in \\mathcal{Y}$ is continuous: $\\mathcal{Y} \\in \\mathbb{R}$ \n",
        "2.   **Classification**: The target variable $y$ is discrete and takes on one of $K$ possible values: $\\mathcal{Y} = \\{y_1, y_2, \\dots, y_K\\}$. Each discrete value corresponds to a class that we want to predict.\n",
        "\n"
      ],
      "metadata": {
        "id": "W-5Hy2EZfYkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Generative Models**\n",
        "There are two types of probabilistic models: *generative* and *disciminative*.\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\underbrace{P_{\\theta}(x,y): \\mathcal{X} \\times \\mathcal{Y} \\to [0,1]}_{\\text{generative model}} & \\; \\; & \\underbrace{P_{\\theta}(y \\mid x): \\mathcal{X} \\times \\mathcal{Y} \\to [0,1]}_{\\text{discriminative model}}\n",
        "\\end{align*}$$\n",
        "\n",
        "\n",
        "*   They involve different training objectives and make different predictions.\n",
        "*   Their uses are different (e.g. generation, prediction, etc.)\n",
        "\n"
      ],
      "metadata": {
        "id": "gHn0SKvFgp1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mixtures of Gaussians**\n",
        "\n",
        "A mixture of $K$ Gaussians is a distribution $P(X)$ of the form:\n",
        "$$\\phi_1\\mathcal{N}(x; \\mu_1, \\Sigma_1) + \\phi_2\\mathcal{N}(x; \\mu_2, \\Sigma_2) + \\dots + \\phi_K\\mathcal{N}(x; \\mu_K, \\Sigma_K).$$\n",
        "\n",
        "*   Each $\\mathcal{N}(x; \\mu_K, \\Sigma_K)$ is a (multivariate) Gaussisan distribution with mean $\\mu_K$ and covariance $\\Sigma_K$.\n",
        "*   The $\\phi_K$ are weights, and the above sum is a weighted average of the $K$ Gaussians.\n",
        "\n"
      ],
      "metadata": {
        "id": "O7ZeauFqiQIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can easily visualize this in 1D"
      ],
      "metadata": {
        "id": "hfrNuwACkERc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def N(x, mu, sigma):\n",
        "  return np.exp(-.5*(x -mu)**2/sigma**2)/np.sqrt(2*np.pi*sigma)\n",
        "\n",
        "def mixture(x):\n",
        "  return 0.6*N(x,mu=1,sigma=0.5) + 0.4*N(x,mu=-1, sigma=0.5)\n",
        "\n",
        "xs, xs1, xs2 = np.linspace(-3,3), np.linspace(-1,3), np.linspace(-3,1)\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot(xs1, 0.6*N(xs1,mu=1,sigma=0.5), label='First Gaussian', alpha=0.7)\n",
        "plt.plot(xs2, 0.4*N(xs2,mu=-1,sigma=0.5), label='Second Gaussian', alpha=0.7)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.plot(xs, mixture(xs), label='Mixture Gaussians', linewidth=2)\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "I4bnnFfrkHp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gaussian Disciminant Model**\n",
        "\n",
        "We may use this approach to define a model $P_{\\theta}$. This will be a basis of an algorithm called Gaussian Discriminant analysis.\n",
        "\n",
        "\n",
        "*   The distribution over classes is [Categorical](), denoted $\\text{Categorical}(\\phi_1, \\phi_2, \\dots, \\phi_K)$. Thus, $P_{\\theta}(y=k)=\\phi_k.$\n",
        "*   The conditional probability $P_{\\theta}(x \\mid y=k)$ of the data under the class $k$ is a [multivariate Gaussian]() $\\mathcal{N}(x; \\mu_k, \\Sigma_k)$ with mean and covariance $\\mu_k, \\Sigma_k$.\n",
        "\n",
        "Thus, $P_{\\theta}(x,y)$ is a mixture of $K$ Guassians:\n",
        "$$P_{\\theta}(x,y) = \\sum_{k=1}^K P_{\\theta}(y=k)P_{\\theta}(x \\mid y=k) = \\sum_{k=1}^K \\phi_K\\mathcal{N}(x; \\mu_K, \\Sigma_K)$$\n",
        "\n",
        "Intuitively, this model defines a story for how the data was generated. To obtain a data point.\n",
        "\n",
        "*   First, we sample a class $y \\sim \\text{Categorical}(\\phi_1, \\phi_2, \\dots, \\phi_K)$ with class proportion given by the $\\phi_k$\n",
        "*   Then, we sample an $x$ from a Guassian distribution $\\mathcal{N}(\\mu_k, \\Sigma_k)$ specific to that class.\n",
        "\n",
        "Such a story can be constructed for most generative algorithms and helps understand them.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UyNT-0RamFnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Classification Dataset: Iris Flowers**\n",
        "\n",
        "To demonstrate this approach, we are going to use the Iris flower dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "3Sn7pu_Bs1PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn import datasets\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris(as_frame=True)\n",
        "\n",
        "# print part of the dataset\n",
        "iris_X, iris_y = iris.data, iris.target\n",
        "pd.concat([iris_X, iris_y], axis=1).head()"
      ],
      "metadata": {
        "id": "URT28fQCtebt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we only care about the first two features we can visualize the dataset in 2D"
      ],
      "metadata": {
        "id": "HsRF9X0vtkeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [12, 4]\n",
        "\n",
        "# create 2d version of dataset\n",
        "X = iris_X.to_numpy()[:,:2]\n",
        "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "\n",
        "# Plot also the training points\n",
        "p1 = plt.scatter(X[:, 0], X[:, 1], c=iris_y, edgecolor='k', s=60, cmap=plt.cm.Paired)\n",
        "plt.xlabel('Sepal Length (cm)')\n",
        "plt.ylabel('Sepal Width (cm)')\n",
        "plt.legend(handles=p1.legend_elements()[0], labels=['Setosa', 'Versicolour', 'Virginica'], loc='lower right')"
      ],
      "metadata": {
        "id": "Glq5WULOtjR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Example: Iris Flower Classidication**\n",
        "Let's see how this approach can be used in practice on Iris dataset.\n",
        "\n",
        "*   We will \"guess\" a good set of parameters for a Gaussian Discriminant Model.\n",
        "*   We will sample from the model and compare to the true data.\n",
        "\n"
      ],
      "metadata": {
        "id": "ql5dfsHptx6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = 100 # number of samples\n",
        "K = 3 # number of classes\n",
        "d = 2 # number of features\n",
        "\n",
        "# Guess the parameters\n",
        "phi = 1/K*np.ones(K,)\n",
        "mus = np.array(\n",
        "    [[5.0, 3.5],\n",
        "     [6.0, 2.5],\n",
        "     [6.5, 3.0]\n",
        "     ])\n",
        "Sigmas = 0.05*np.tile(np.reshape(np.eye(2), (1,2,2)), (K,1,1))\n",
        "\n",
        "# Generate data from this model\n",
        "ys = np.random.multinomial(n=1, pvals=phi, size=(s,)).argmax(axis=1)\n",
        "xs = np.zeros([s,d])\n",
        "for k in range(K):\n",
        "  nk = (ys==k).sum()\n",
        "  xs[ys==k,:] = np.random.multivariate_normal(mus[k], Sigmas[k], size=(nk,))\n",
        "\n",
        "print(xs[:10])\n"
      ],
      "metadata": {
        "id": "SCN2i9zTujUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(121)\n",
        "plt.title('Model Sample')\n",
        "plt.scatter(xs[:,0], xs[:,1], c=ys, cmap=plt.cm.Paired)\n",
        "plt.scatter(X[:,0], X[:,1], c=iris_y, edgecolor='k', s=40, cmap=plt.cm.Paired, alpha=0.15)\n",
        "plt.xlabel('Sepal length')\n",
        "plt.ylabel('Sepal width')\n",
        "\n",
        "# Plot also the training points\n",
        "plt.subplot(122)\n",
        "plt.title('Training Dataset')\n",
        "plt.scatter(X[:,0], X[:,1], c=iris_y, edgecolor='k', s=40, cmap=plt.cm.Paired, alpha=1)\n",
        "plt.xlabel('Sepal length')\n",
        "plt.ylabel('Sepal width')\n"
      ],
      "metadata": {
        "id": "BlieWhlAxBNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Our Gaussian Discriminant Model generate data that looks not unlike the real data\n",
        "*   Let's now see how we can learn parameters from data and use the model to make predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "sBUgrVOYyNiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 3: Gaussian Discriminant Analysis: Learning**\n",
        "\n",
        "We continue our discussion of Gaussian Disciminant analysis, and look at:\n",
        "\n",
        "*   How to learn parameters of the mixture model\n",
        "*   How to use model to make predictions\n",
        "\n"
      ],
      "metadata": {
        "id": "vXDSWPMay04D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Classification**\n",
        "Consider a training dataset $\\mathcal{D}$.\n",
        "We distiguish between the two types of supervised learning problems depending on the targets $y^{(i)}$\n",
        "\n",
        "\n",
        "1.   **Regression:** The target variable $y \\in \\mathcal{Y}$ is continuous: $\\mathcal{Y} \\in \\mathbb{R}$\n",
        "2.   **Classification:** The target variable $y$ is disrete and take on one of $K$ possible values $\\mathcal{Y} = \\{y_1, y_2, \\dots, y_K\\}$. Each discrete value corresponds to a *class* that we want to predict.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZA-kPYNvh2D_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Gaussian Discriminant Analysis**\n",
        "We may define a model $P_{\\theta}$ as follows. This will be the basis of an algorithm called Gaussian Discriminant Analysis.\n",
        "\n",
        "*   The distribution over class is [Categorical](), denoted $\\text{Categorical}(\\phi_1, \\phi_2, \\dots, \\phi_K)$. Thus, $P_{\\theta}(y=k) = \\phi_k$\n",
        "*   The conditional probability $P(x \\mid y = k)$ of the data under the class $k$ is a [multivariate Gaussian]() $\\mathcal{N}(x;\\mu_k,\\Sigma_k)$ with mean and covariance $\\mu_k, \\Sigma_k$\n",
        "\n",
        "Thus, $P_{\\theta}(x,y)$ is a mixture of $K$ Gaussians\n",
        "$$P_{\\theta}(x,y) = \\sum_{k=1}^KP_{\\theta}(y=k)P(x \\mid y = k) = \\sum_{k=1}^K \\phi_k\\mathcal{N}(x;\\mu_k,\\Sigma_k).$$\n",
        "\n"
      ],
      "metadata": {
        "id": "twul9-gZi0-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Maximum Likelihood Learning**\n",
        "\n",
        "In order to fit probabilistic models, we use the following objective:\n",
        "$$\\max_{\\theta}\\mathbb{E}_{x,y \\sim \\mathbb{P}}\\log P_{\\theta}(x,y).$$\n",
        "\n",
        "This seeks to find a model that assigns high probability to the training data.\n",
        "\n",
        "Let's use maximum likelihood to fit Gaussian Discriminant Model. Note that model parameters $\\theta$ are the union of the parameters of each sub-model:\n",
        "$$\\theta = \\{\\mu_1, \\Sigma_1, \\phi_1, \\dots, \\mu_K, \\Sigma_K, \\phi_K\\}.$$\n",
        "\n",
        "Mathematically, the components of the model $P_{\\theta}(x,y)$ as follows:\n",
        "$$\\begin{align*}\n",
        "P_{\\theta}(y) &= \\frac{\\prod_{k=1}^K\\phi_k^{\\mathbb{I}\\{y=y_k\\}}}{\\sum_{k=1}^K\\phi_k} \\\\\n",
        "P_{\\theta}(x \\mid y = k) &= \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{d/2}}\\exp\\left(-\\frac{1}{2}(x - \\mu_k)^\\top\\Sigma_k^{-1}(x - \\mu_k)\\right)\n",
        "\\end{align*}$$\n"
      ],
      "metadata": {
        "id": "5hVO2lGTlDjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Optimizing the Log Likelihood**\n",
        "Given a dataset $\\mathcal{D}$, we want to optimize the log-likelihood $\\ell(\\theta)$:\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\ell(\\theta) &= \\sum_{i=1}^n \\log P_{\\theta}(x^{(i)},y^{(i)}) = \\sum_{i=1}^n \\log P_{\\theta}(x^{(i)} \\mid y^{(i)}) + \\sum_{i=1}^n\\log P_{\\theta}(y^{(i)}) \\\\\n",
        "&= \\sum_{k=1}^K \\underbrace{\\sum_{i:y^{(i)}=k}\\log P(x^{(i)} \\mid y^{(i)}; \\mu_k, \\Sigma_k)}_{\\text{all the terms that involve} \\; \\mu_k, \\Sigma_k} \\; + \\underbrace{\\sum_{i=1}^n\\log P(y^{(i)}; \\vec{\\phi})}_{\\text{all the terms that involve} \\; \\vec{\\phi}}\n",
        "\\end{align*}$$\n"
      ],
      "metadata": {
        "id": "TSd5gSxjo6FE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that each set of parameters $(\\mu_k, \\Sigma_k)$ is only found in one term of the summation over the $K$ classes and the $\\phi_k$ is also in the same term.\n",
        "\n",
        "Since each $(\\mu_k, \\Sigma_k)$ for $k= 1,2, \\dots, K$ is only found in one term, optimization over $(\\mu_k, \\Sigma_k)$ can be carried out independently of all the other parameters by just looking at that term:\n",
        "$$\\begin{align*}\n",
        "\\max_{\\mu_k, \\Sigma_k}\\sum_{i=1}^n \\log P_{\\theta}(x^{(i)}, y^{(i)}) &= \\max_{\\mu_k, \\Sigma_k}\\sum_{l=1}^K\\sum_{i:y^{(i)}=l}\\log P_{\\theta}(x^{(i)} \\mid y^{(i)}; \\mu_l, \\Sigma_l) \\\\\n",
        "&= \\max_{\\mu_k, \\Sigma_k}\\sum_{i:y^{(i)}=k}\\log P_{\\theta}(x^{(i)} \\mid y^{(i)}; \\mu_k, \\Sigma_k)\n",
        "\\end{align*}$$\n",
        "\n",
        "Similarly, optimizing the $\\vec{\\phi} = (\\phi_1, \\phi_2, \\dots, \\phi_K)$ only involves a single term:\n",
        "$$\\max_{\\vec{\\phi}}\\sum_{i=1}^n \\log P_{\\theta}(x^{(i)}, y^{(i)}; \\theta) = \\max_{\\vec{\\phi}}\\sum_{i=1}^n\\log P_{\\theta}( y^{(i)};\\vec{\\phi})$$"
      ],
      "metadata": {
        "id": "jHZteq3IrRV0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Optimizing the Class Probabilities**\n",
        "These observations greatly simplify the optimization of the model. Let's first consider the optimization over $\\vec{\\phi} = (\\phi_1, \\phi_2, \\dots, \\phi_K)$. From the previous analysis, our objective $J(\\vec{\\phi})$ equals:\n",
        "$$\\begin{align*}\n",
        "J(\\vec{\\phi}) &= \\max_{\\vec{\\phi}}\\sum_{i=1}^n\\log P_{\\theta}( y^{(i)};\\vec{\\phi}) \\\\\n",
        "&= \\sum_{i=1}^n\\log \\phi_{y^{(i)}} - n\\log\\sum_{k=1}^K\\phi_k \\\\\n",
        "&= \\sum_{k=1}^K \\sum_{i:y^{(i)}=k}\\log \\phi_k - n \\cdot \\log\\sum_{k=1}^K\\phi_k\n",
        "\\end{align*}$$"
      ],
      "metadata": {
        "id": "Ge-FAEyswx03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taking the derivative and setting it to zero, we obtain\n",
        "$$\\frac{\\phi_k}{\\sum_l\\phi_l} = \\frac{n_k}{n}$$\n",
        "\n",
        "for each $k$, where $n_k = |\\{i:y^{(i)}=k\\}|$ is the number of training targets with class $k$.\n",
        "\n",
        "Thus, the optimal $\\phi_k$ is just the proportion of data points with class $k$ in the training dataset!"
      ],
      "metadata": {
        "id": "BdgDK5Kcze4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Optimizing the Conditional Probabilities**\n",
        "Similarly, we can maximize the likelihood\n",
        "$$\\max_{\\mu_k,\\Sigma_k}\\sum_{i:y^{(i)}=k}\\log P(x^{(i)} \\mid y^{(i)}; \\mu_k, \\Sigma_k) = \\max_{\\mu_k,\\Sigma_k}\\sum_{i:y^{(i)}=k}\\log \\mathcal{N}(x^{(i)} \\mid \\mu_k,\\Sigma_k)$$\n",
        "\n",
        "over the Guassian parameters.\n",
        "\n",
        "Computing the derivative and setting it to zero, we obtain closed form solution:\n",
        "$$\\begin{align*}\n",
        "\\mu_k &= \\frac{\\sum_{i: y^{(i)} = k}x^{(i)}}{n_k} \\\\\n",
        "\\Sigma_k &= \\frac{\\sum_{i: y^{(i)} = k}(x^{(i)} - \\mu_k)(x^{(i)} - \\mu_k)^\\top}{n_k}\n",
        "\\end{align*}$$\n",
        "\n",
        "These are just empirical means and covariances of each class"
      ],
      "metadata": {
        "id": "G7rF8awLwxvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Querying the Model**\n",
        "How do we ask the model for predictions? As discussed earlier, we can apply Bayes' rule:\n",
        "$$\\arg\\max_{\\theta}P_{\\theta}(y\\mid x) = \\arg\\max_{\\theta}P_{\\theta}(x \\mid y)P(y).$$\n",
        "\n",
        "Thus, we can estimate the probability of $x$ and under each $P_{\\theta}(x \\mid y=k)P(y=k)$ and choose the class that explains the data best."
      ],
      "metadata": {
        "id": "lAtFxpmK45nP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Classification Dataset: Iris Flowers**\n",
        "\n",
        "To demonstrate this approach, we are going to use this Iris flower dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "MjZrzulpr-wP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn import datasets\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris(as_frame=True)\n",
        "\n",
        "# print part of the dataset\n",
        "iris_X, iris_y = iris.data, iris.target\n",
        "pd.concat([iris_X, iris_y], axis=1).head()"
      ],
      "metadata": {
        "id": "V1fIhuhFx4Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the dataset in 2D"
      ],
      "metadata": {
        "id": "9-15eYmpsSTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [12, 4]\n",
        "\n",
        "# create 2d version of dataset\n",
        "X = iris_X.to_numpy()[:,:2]\n",
        "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "\n",
        "# Plot also the training points\n",
        "p1 = plt.scatter(X[:, 0], X[:, 1], c=iris_y, edgecolor='k', s=60, cmap=plt.cm.Paired)\n",
        "plt.xlabel('Sepal Length (cm)')\n",
        "plt.ylabel('Sepal Width (cm)')\n",
        "plt.legend(handles=p1.legend_elements()[0], labels=['Setosa', 'Versicolour', 'Virginica'], loc='lower right')"
      ],
      "metadata": {
        "id": "O0o8szvMsZe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Example: Iris Flowers Classification**\n",
        "\n",
        "Let's see how this approach can be used in pratice on the Iris dataset\n",
        "\n",
        "*   We will learn a good set of parameters for a Gaussian Disriminant Model\n",
        "*   We will compare the outputs to the true predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "JwKQhww6sdXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first start by computing the true parameters on our dataset."
      ],
      "metadata": {
        "id": "H7CPRpP2s--4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can implement these formulas over the Iris dataset\n",
        "d = 2 # number of features in our toy dataset\n",
        "K = 3 #number of classes\n",
        "n = X.shape[0] # size of dataset\n",
        "\n",
        "# These are the shapes of the parameters\n",
        "mus = np.zeros([K,d])\n",
        "Sigmas = np.zeros([K,d,d])\n",
        "phis = np.zeros([K])\n",
        "\n",
        "# We now compute the parameters\n",
        "for k in range(3):\n",
        "  X_k = X[iris_y == k]\n",
        "  mus[k] = np.mean(X_k, axis=0)\n",
        "  Sigmas[k] = np.cov(X_k.T)\n",
        "  phis[k] = X_k.shape[0]/float(n)\n",
        "\n",
        "# print out the mean \n",
        "print(mus)\n"
      ],
      "metadata": {
        "id": "WZzTDjeqtSaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can compute predictions using Bayes' rule"
      ],
      "metadata": {
        "id": "GlaV4RNmuxrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can implement this in numpy\n",
        "def gda_predictions(x,mus,Sigmas,phis):\n",
        "  \"\"\" This returns class assignments and p(y|x) under the GDA model\n",
        "\n",
        "  We compute \\arg\\max_y p(y|x) as \\arg\\max_y p(x|y)p(y)\n",
        "  \"\"\"\n",
        "  # Adjust shapes\n",
        "  n, d = x.shape\n",
        "  x = np.reshape(x, (1,n,d,1))\n",
        "  mus = np.reshape(mus, (K,1,d,1))\n",
        "  Sigmas = np.reshape(Sigmas, (K,1,d,d))\n",
        "\n",
        "  # Compute probabilities\n",
        "  py = np.tile(phis.reshape((K,1)), (1,n)).reshape([K,n,1,1])\n",
        "  pxy = (\n",
        "      np.sqrt(np.abs((2*np.pi)**d*np.linalg.det(Sigmas))).reshape((K,1,1,1))\n",
        "      * -.5*np.exp(\n",
        "          np.matmul(np.matmul((x - mus).transpose([0,1,3,2]), np.linalg.inv(Sigmas)), x-mus)\n",
        "      )\n",
        "  )\n",
        "  pyx = pxy * py\n",
        "  return pyx.argmax(axis=0).flatten(), pyx.reshape([K,n])\n",
        "\n",
        "idx, pyx = gda_predictions(X,mus,Sigmas,phis)\n",
        "\n",
        "print(idx)\n"
      ],
      "metadata": {
        "id": "zSidJEl8u7CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize the decision boundaries like we did earlier"
      ],
      "metadata": {
        "id": "Jr8p1a1czPL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.colors import LogNorm\n",
        "xx,yy = np.meshgrid(np.arange(x_min,x_max,.02), np.arange(y_min,y_max,.02))\n",
        "Z, pyx = gda_predictions(np.c_[xx.ravel(), yy.ravel()], mus, Sigmas, phis)\n",
        "logpy = np.log(-1./3*pyx)\n",
        "\n",
        "# Put the result into color plot\n",
        "Z = Z.reshape(xx.shape)\n",
        "contours = np.zeros([K, xx.shape[0], xx.shape[1]])\n",
        "for k in range(K):\n",
        "  contours[k] = logpy[k].reshape(xx.shape)\n",
        "plt.pcolormesh(xx,yy,Z,cmap=plt.cm.Paired)\n",
        "for k in range(K):\n",
        "  plt.contour(xx,yy,contours[k], levels=np.logspace(0,1,1))\n",
        "\n",
        "# Plot also the training points\n",
        "plt.scatter(X[:,0], X[:,1], c=iris_y, edgecolor='k', s=40, cmap=plt.cm.Paired)\n",
        "plt.xlabel('Sepal length')\n",
        "plt.ylabel('Sepal width')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "McR04IsnzT_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Algorithm: Gaussian Discriminant Analysis**\n",
        "\n",
        "\n",
        "*   **Type**: Supervised Learning (multi-class classification)\n",
        "*   **Model family**: Mixture of Gaussians\n",
        "*   **Objective**: Log-likelihood\n",
        "*   **Optimizer**: Closed form solution\n",
        "\n"
      ],
      "metadata": {
        "id": "s2P944uU12AH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Special cases of GDA**\n",
        "\n",
        "Many important generative algorithms are special cases of GDA\n",
        "\n",
        "*   **Linear Discriminant Ananlysis (LDA)**: all the covariance matrices $\\Sigma_k$ take the same value.\n",
        "*   **Gaussian Naive Bayes**: all the covariance matrices $\\Sigma_k$ are diagonal.\n",
        "*   **Quadratic Discriminant Analysis (QDA)**: another term for GDA\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UwyLhU7q3r9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Generative vs. Discriminative Approaches**\n",
        "Pros of discriminative models:\n",
        " * often more accurate because they make fewer modeling assumptions.\n",
        "\n",
        "Pros of generative models:\n",
        " * Can do more than just prediction: generation, fill-in missing features, etc.\n",
        " * Can include extra prior knowledge; if prior knowledge is correct, model will be more accurate.\n",
        " * Often have closed form solutions, hence are faster to train."
      ],
      "metadata": {
        "id": "2zLPaotq41-R"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "idlMMh3Z1ey-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZdXnLkM61ydh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}