{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 9. Support Vector Machines**\n",
        "## **Applied Machine Learning**"
      ],
      "metadata": {
        "id": "AnEhBGl9bpgD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 1: Classification Margins**\n",
        "\n",
        "In this lecture, we are going to cover Support Vector Machines (SVMs), one of the most successful classification algorithms in machine learning.\n",
        "\n",
        "We start the preresentation of SVMs by defining the classification Margins."
      ],
      "metadata": {
        "id": "LaOlwpZybyJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Binary Classification**\n",
        "Consider a training set $\\mathcal{D}$.\n",
        "\n",
        "We distinguish between two types of supervised learning problems by the target variables $y^{(i)}$\n",
        "\n",
        "* Regression: The target variables $y \\in \\mathcal{Y}$ is continuous: $\\mathcal{Y} \\in \\mathbb{R}$.\n",
        "* Binary classification: Tha target variable $y \\in \\mathcal{Y}$ is discrete and takes on one of 2 value $\\mathcal{Y} = \\{-1,+1\\}$"
      ],
      "metadata": {
        "id": "kd7lWkY1cVI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Linear Model Family**\n",
        "In this lecture, we will work with linear model family of the form:\n",
        "$$f_{\\theta}(x) = \\theta_0 + \\theta_1x_1 + \\dots + \\theta_dx_d$$\n",
        "\n",
        "where $x \\in \\mathbb{R}^d$ is a vector of features and $y = \\{-1, +1\\}$ is the target. The $\\theta_j$ are the parameters of the model.\n",
        "\n",
        "We can preresent the model in the vectorized form:\n",
        "$$f_{\\theta}(x) = \\theta_0 + \\theta^\\top \\cdot x.$$"
      ],
      "metadata": {
        "id": "j7P6VohbdPJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Notation and the Iris Dataset**\n",
        "\n",
        "In this lecture, we are going again Iris dataset.\n",
        "\n",
        "As we just mentioned, we make two additional assumptions\n",
        "* We will only consider binary classification\n",
        "* We will use $\\mathcal{Y} = \\{-1,1\\} $ as the label space"
      ],
      "metadata": {
        "id": "uoTkuyM5eBLX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCbfKejkbaqB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "\n",
        "# Loaad the Iris dataset\n",
        "iris = datasets.load_iris(as_frame=True)\n",
        "iris_X, iris_y = iris.data, iris.target\n",
        "\n",
        "# subsample to a third of the data points\n",
        "iris_X = iris_X.iloc[::4]\n",
        "iris_y = iris_y.iloc[::4]\n",
        "\n",
        "# Create a binary classification dataset with label +/-1\n",
        "iris_y2 = iris_y.copy()\n",
        "iris_y2[iris_y2==2] = 1\n",
        "iris_y2[iris_y2==0] = -1\n",
        "\n",
        "# Print part of the dataset\n",
        "pd.concat([iris_X, iris_y2], axis=1).head()\n",
        "pd.concat([iris_X, iris_y2], axis=1).tail()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] =(12,4)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Create 2d version of dataset and subsample it\n",
        "X = iris_X.to_numpy()[:,:2]\n",
        "x_min, x_max = X[:,0].min() -.5, X[:,0].max() + .5\n",
        "y_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, .02), np.arange(y_min, y_max, .02))\n",
        "\n",
        "# Plot also the training points\n",
        "p1 = plt.scatter(X[:,0], X[:,1], c=iris_y, edgecolor='k', s=40, cmap=plt.cm.Paired)\n",
        "plt.xlabel('SEpal length')\n",
        "plt.ylabel('Sepal width')\n",
        "plt.legend(handles=p1.legend_elements()[0], labels=('Setosa', 'Not Setosa'), loc='lower right')"
      ],
      "metadata": {
        "id": "EHhzFmvufgMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comparing Classification Algorithms**\n",
        "\n",
        "We have different types approaches to classification.\n",
        "\n",
        "When fitting a model, there may be many valid decision boundaries. How do we select one?\n",
        "\n",
        "Consider the following three classification algorithms from `sklearn`. Each of them outputs a different classification boundary."
      ],
      "metadata": {
        "id": "0MZoOmzRhnqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression, Perceptron, RidgeClassifier\n",
        "models = [LogisticRegression(), Perceptron(), RidgeClassifier()]\n",
        "\n",
        "def fit_and_create_boundary(model):\n",
        "  model.fit(X, iris_y2)\n",
        "  Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "  Z = Z.reshape(xx.shape)\n",
        "  return Z\n",
        "\n",
        "plt.figure(figsize=(12,3))\n",
        "a = 1\n",
        "b = 3\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "  plt.subplot(a,b,i+1)\n",
        "  Z = fit_and_create_boundary(model)\n",
        "  plt.pcolormesh(xx,yy,Z,cmap=plt.cm.Paired)\n",
        "\n",
        "  #Plot also the training points\n",
        "  plt.scatter(X[:,0], X[:,1], c=iris_y2, s=40, edgecolor='k', cmap=plt.cm.Paired)\n",
        "  plt.title(\"Algorithm %d\" % (i+1))\n",
        "  plt.xlabel('Sepal length')\n",
        "  plt.ylabel('Sepal width')\n",
        "\n"
      ],
      "metadata": {
        "id": "PpmfJERWhNIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Classification Scores**\n",
        "\n",
        "Most classification algorithms output not just a class label but a score.\n",
        "* For example, logistic regression returns the class probability\n",
        "$$p(y=1\\|x) = \\sigma(\\theta^\\top x) \\in [0,1]$$\n",
        "\n",
        "If the class probability is $> 0.5$, the model output class $1$. \n",
        "\n",
        "The score is an estimate of confidence; it also preresents how far we are from the decision boundary."
      ],
      "metadata": {
        "id": "L3JdkE_lm5k4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The Max-Margin Principle**\n",
        "\n",
        "Intuitively, we want to select boundaries with *high margin*.\n",
        "\n",
        "This means that we are as confident as possible for every point and as far as possible from the decision boundary.\n",
        "\n",
        "Several of the separating boundaries in our previous example had low margin: they came too close to the boundary."
      ],
      "metadata": {
        "id": "DHCxU1lLntdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron, RidgeClassifier\n",
        "from sklearn.svm import SVC\n",
        "models = [SVC(kernel='linear', C=10000), Perceptron(), RidgeClassifier()]\n",
        "\n",
        "def fit_and_create_boundary(model):\n",
        "  model.fit(X, iris_y2)\n",
        "  Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "  Z = Z.reshape(xx.shape)\n",
        "  return Z\n",
        "\n",
        "plt.figure(figsize=(12,3))\n",
        "a=1\n",
        "b=3\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "  plt.subplot(a,b,i+1)\n",
        "  Z = fit_and_create_boundary(model)\n",
        "  plt.pcolormesh(xx,yy,Z,cmap=plt.cm.Paired)\n",
        "\n",
        "  # Plot also the training points\n",
        "  plt.scatter(X[:,0], X[:,1], c=iris_y2, s=40, edgecolor='k', cmap=plt.cm.Paired)\n",
        "  if i==0:\n",
        "    plt.title('Good Margin')\n",
        "  else:\n",
        "    plt.title('Bad margin')\n",
        "  plt.xlabel('Sepal length')\n",
        "  plt.ylabel('Sepal width')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EpK1k1iVj0tK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we plot a decision boundary between the two classes (solid line) that has a high margin. The two dashed lines that lie at the margin.\n",
        "\n",
        "Points that are the margin are highlighted in black. A good decision boundary is as far away as possible from the points at the margin."
      ],
      "metadata": {
        "id": "MpJRQf9lrHLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "\n",
        "# Fit the model, don't regularize for illustration purposes.\n",
        "clf = svm.SVC(kernel='linear', C=1000) \n",
        "clf.fit(X, iris_y2)\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.scatter(X[:,0], X[:,1], c=iris_y2, edgecolor='k', s=30, cmap=plt.cm.Paired)\n",
        "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "\n",
        "# plot decision boundary and margins\n",
        "plt.contour(xx,yy,Z,colors='k',levels=[-1,0,1], alpha=0.5,\n",
        "            linestyle=['--','-','--'])\n",
        "plt.scatter(clf.support_vectors_[:,0], clf.support_vectors_[:,1], s=100,\n",
        "            linewidth=1, facecolors='none', edgecolors='k')\n",
        "plt.xlim([4.6,6])\n",
        "plt.ylim([2.25,4])"
      ],
      "metadata": {
        "id": "3olhycOOqu3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The Functional Classification Margin**\n",
        "\n",
        "How can we define the concept of margin more formally?\n",
        "\n",
        "We can try to define the margin $\\tilde \\gamma^{(i)}$ with respect to a training example $(x^{(i)},y^{(i)})$ as\n",
        "$$\\tilde \\gamma^{(i)} = y^{(i)} \\cdot f(x^{(i)}) = y^{(i)} (\\theta^\\top x^{(i)} + \\theta_0). $$\n",
        "\n",
        "We call this *functional* margin. Let's analyze it\n",
        "\n",
        "We defined the functional margin as:\n",
        "$$\\tilde \\gamma^{(i)} = y^{(i)} \\cdot (\\theta^\\top x^{(i)} + \\theta_0).$$\n",
        "* If $y^{(i)}=1$, then the margin $\\tilde \\gamma^{(i)}$ is large if the model score $f(x^{(i)}) = \\theta^\\top x^{(i)} + \\theta_0 $ is positive and large.\n",
        "* Thus, we are classifying $x^{(i)}$ correctly and with high confidence.\n",
        "*  If $y^{(i)}=-1$, then the margin $\\tilde \\gamma^{(i)}$ is large if is negative and large in absolute value.\n",
        "* We are again classifying $x^{(i)}$ correctly and with high confidence.\n",
        "\n",
        "Thus higher margin mean higher confidence at each input point.\n",
        "\n",
        "However, we have a problem\n",
        "* If we rescale the parameters $\\theta, \\theta_0$ by a scalar $\\alpha > 0$, we get new parameter $\\alpha \\theta, \\alpha \\theta_0$.\n",
        "* The  $\\alpha \\theta, \\alpha \\theta_0$ doesn't change the classification of points. \n",
        "* However, the margin $(\\alpha\\theta^\\top x^{(i)} +\\alpha\\theta_0) = \\alpha(\\theta^\\top x^{(i)} + \\theta_0)$ is now scaled by $\\alpha$!\n",
        "\n",
        "It doesn't make sense that the same classification boundary can have different margins when we rescale it."
      ],
      "metadata": {
        "id": "M9qKy5XyuZcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The Geometric Classification Margin**\n",
        "\n",
        "We define the margin $\\tilde \\gamma^{(i)}$ with respect to an example training $(x^{(i)}, y^{(i)})$ as\n",
        "$$\\tilde \\gamma^{(i)} = y^{(i)}\\left(\\frac{\\theta^\\top x^{(i)} + \\theta_0}{\\|\\theta\\|}\\right)$$\n",
        "\n",
        "* We normalize the functional margin by $\\|\\theta\\|$\n",
        "* Rescaling the weights can no longer make the margin arbitrarily large, which addresses our previous issue. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ceKDG-2v4ubb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Geometric Intuitions**\n",
        "\n",
        "The margin $\\tilde \\gamma^{(i)}$ is called geometric because it corresponds to a distance from the $x^{(i)}$ to the separating hyperplane $\\theta^\\top x + \\theta_0 = 0$ (dashed line below)\n",
        "\n",
        "![margin.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYsAAAFXCAYAAABTHGLfAAAKxmlDQ1BJQ0MgUHJvZmlsZQAASImVlwdYU1kWgO97L73QAqFICb0J0quU0EOXDjZCEkgoMSYEFRsigyM4FkSk2cBBEQXHAshYEAtWFBSxT5BBRR0HCzZU9gFLmNn9dvfb876T+7+Tc88593735jsBgEpgi0SZsBIAWcJscVSgDyMhMYmB/x0QgDpQBCoAYnMkImZkZChAZWr8u3y4A6Dx8ZbVeKx///6/ijKXJ+EAAEWinMKVcLJQPobqa45InA0Ashe1Gy7JFo3zJZRVxWiBKD8c57RJHh7nlAnGYCZ8YqJ8UdYAgEBhs8VpAFCMUDsjh5OGxqH4oWwj5AqEKKPvwJPDZ3NRRvOCmVlZi8ZZhrJZyl/ipP0tZoo8JpudJufJtUwIwU8gEWWyl/2f2/G/JStTOpXDBFUKXxwUhY50dM/uZiwKkbMwJTxiigXcCf8J5kuDYqeYI/FNmmIu2y9EPjczPHSKUwUBLHmcbFbMFPMk/tFTLF4UJc+VKvZlTjFbPJ1XmhErt/N5LHn8XH5M/BTnCOLCp1iSER0y7eMrt4ulUfL6ecJAn+m8AfK1Z0n+sl4BSz43mx8TJF87e7p+npA5HVOSIK+Ny/Pzn/aJlfuLsn3kuUSZkXJ/Xmag3C7JiZbPzUYP5PTcSPkeprODI6cY+AF/EIo+DBAL7IADsEU/wwDI5i0dP6PAd5FomViQxs9mMNFbxmOwhBzrmQw7GzsbAMbv7OSReHd34i5CdMK0TYTGd0XPPFI7bUvRAqAFPUeaxGmb0T4AFBMAaM7jSMU5k7bx6wSwgIT+FqgCTaALDIEZsEIrcwLuwButOBhEgBiQCBYADuCDLCAGS8AKsAYUgmKwGWwDlWAXqAX7wSFwBLSAk+AsuAiugpugFzwAMjAIXoJh8AGMQhCEh6gQDdKE9CBjyBKyg1wgT8gfCoWioEQoGUqDhJAUWgGthYqhEqgS2gPVQ79AJ6Cz0GWoG7oH9UND0FvoC4zAFFgV1oFN4FmwC8yEQ+AYeD6cBi+Gc+ECeCNcDtfAB+Fm+Cx8Fe6FZfBLeAQBCBmhI/qIFeKC+CIRSBKSioiRVUgRUobUII1IG9KJ3EJkyCvkMwaHoWEYGCuMOyYIE4vhYBZjVmE2YCox+zHNmPOYW5h+zDDmO5aK1cZaYt2wLGwCNg27BFuILcPWYY9jL2B7sYPYDzgcjo4zxTnjgnCJuHTcctwG3A5cE64d140bwI3g8XhNvCXeAx+BZ+Oz8YX4CvxB/Bl8D34Q/4lAJugR7AgBhCSCkJBPKCMcIJwm9BCeEUaJSkRjohsxgsglLiNuIu4lthFvEAeJoyRlkinJgxRDSietIZWTGkkXSA9J78hksgHZlTyHLCDnkcvJh8mXyP3kzxQVigXFlzKPIqVspOyjtFPuUd5RqVQTqjc1iZpN3Uitp56jPqZ+UqApWCuwFLgKqxWqFJoVehReKxIVjRWZigsUcxXLFI8q3lB8pURUMlHyVWIrrVKqUjqh1Kc0okxTtlWOUM5S3qB8QPmy8nMVvIqJir8KV6VApVblnMoADaEZ0nxpHNpa2l7aBdqgKk7VVJWlmq5arHpItUt1WE1FzUEtTm2pWpXaKTUZHaGb0Fn0TPom+hH6HfoXdR11pjpPfb16o3qP+keNGRreGjyNIo0mjV6NL5oMTX/NDM0tmi2aj7QwWhZac7SWaO3UuqD1aobqDPcZnBlFM47MuK8Na1toR2kv167VvqY9oqOrE6gj0qnQOafzSpeu662brluqe1p3SI+m56kn0CvVO6P3gqHGYDIyGeWM84xhfW39IH2p/h79Lv1RA1ODWIN8gyaDR4YkQxfDVMNSww7DYSM9ozCjFUYNRveNicYuxnzj7cadxh9NTE3iTdaZtJg8N9UwZZnmmjaYPjSjmnmZLTarMbttjjN3Mc8w32F+0wK2cLTgW1RZ3LCELZ0sBZY7LLtnYme6zhTOrJnZZ0WxYlrlWDVY9VvTrUOt861brF/PMpqVNGvLrM5Z320cbTJt9to8sFWxDbbNt22zfWtnYcexq7K7bU+1D7Bfbd9q/8bB0oHnsNPhriPNMcxxnWOH4zcnZyexU6PTkLORc7JztXOfi6pLpMsGl0uuWFcf19WuJ10/uzm5ZbsdcfvT3co9w/2A+/PZprN5s/fOHvAw8GB77PGQeTI8kz13e8q89L3YXjVeT7wNvbnedd7PmObMdOZB5msfGx+xz3Gfj75uvit92/0Qv0C/Ir8ufxX/WP9K/8cBBgFpAQ0Bw4GOgcsD24OwQSFBW4L6WDosDqueNRzsHLwy+HwIJSQ6pDLkSahFqDi0LQwOCw7bGvYw3DhcGN4SASJYEVsjHkWaRi6O/HUObk7knKo5T6Nso1ZEdUbTohdGH4j+EOMTsynmQaxZrDS2I04xbl5cfdzHeL/4knhZwqyElQlXE7USBYmtSfikuKS6pJG5/nO3zR2c5zivcN6d+abzl86/vEBrQeaCUwsVF7IXHk3GJscnH0j+yo5g17BHUlgp1SnDHF/Ods5Lrje3lDvE8+CV8J6leqSWpD5P80jbmjbE9+KX8V8JfAWVgjfpQem70j9mRGTsyxjLjM9syiJkJWedEKoIM4TnF+kuWrqoW2QpKhTJFrst3rZ4WBwirpNAkvmS1mxVtDm6JjWT/iDtz/HMqcr5tCRuydGlykuFS68ts1i2ftmz3IDcn5djlnOWd6zQX7FmRf9K5so9q6BVKas6VhuuLlg9mBeYt38NaU3Gmuv5Nvkl+e/Xxq9tK9ApyCsY+CHwh4ZChUJxYd8693W7fsT8KPixa739+or134u4RVeKbYrLir9u4Gy48pPtT+U/jW1M3di1yWnTzs24zcLNd7Z4bdlfolySWzKwNWxrcymjtKj0/baF2y6XOZTt2k7aLt0uKw8tb60wqthc8bWSX9lb5VPVVK1dvb764w7ujp6d3jsbd+nsKt71Zbdg9909gXuaa0xqympxtTm1T/fG7e382eXn+jqtuuK6b/uE+2T7o/afr3eurz+gfWBTA9wgbRg6OO/gzUN+h1obrRr3NNGbig+Dw9LDL35J/uXOkZAjHUddjjYeMz5WfZx2vKgZal7WPNzCb5G1JrZ2nwg+0dHm3nb8V+tf953UP1l1Su3UptOk0wWnx87knhlpF7W/Opt2dqBjYceDcwnnbp+fc77rQsiFSxcDLp7rZHaeueRx6eRlt8snrrhcabnqdLX5muO149cdrx/vcupqvuF8o/Wm68227tndp3u8es7e8rt18Tbr9tXe8N7uO7F37vbN65Pd5d59fi/z3pv7OfdHH+Q9xD4seqT0qOyx9uOa38x/a5I5yU71+/VfexL95MEAZ+Dl75Lfvw4WPKU+LXum96z+ud3zk0MBQzdfzH0x+FL0cvRV4R/Kf1S/Nnt97E/vP68NJwwPvhG/GXu74Z3mu33vHd53jESOPP6Q9WH0Y9EnzU/7P7t87vwS/+XZ6JKv+K/l38y/tX0P+f5wLGtsTMQWsydaAQRVODUVgLdon0BNBIB2EwDS3MmeekKgyf8BEwT+E0/23RPiBEBtOwAxeQCEomMFOpqgqugNQCSqMd4AtreX6z9FkmpvNxmL3IK2JmVjY+/Q/hFvDsC3vrGx0ZaxsW91aLH3AWj/MNnLj4vSQQB259r5OUZf1wrNA/8i/wCtcBBnrIhspQAAAZ1pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IlhNUCBDb3JlIDUuNC4wIj4KICAgPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4KICAgICAgPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIKICAgICAgICAgICAgeG1sbnM6ZXhpZj0iaHR0cDovL25zLmFkb2JlLmNvbS9leGlmLzEuMC8iPgogICAgICAgICA8ZXhpZjpQaXhlbFhEaW1lbnNpb24+Mzk1PC9leGlmOlBpeGVsWERpbWVuc2lvbj4KICAgICAgICAgPGV4aWY6UGl4ZWxZRGltZW5zaW9uPjM0MzwvZXhpZjpQaXhlbFlEaW1lbnNpb24+CiAgICAgIDwvcmRmOkRlc2NyaXB0aW9uPgogICA8L3JkZjpSREY+CjwveDp4bXBtZXRhPgo1qHJHAABAAElEQVR4Ae1dCfwV0/t+W2UridCOyNJCoUKiEqGsIYSQXcmS0I8Usm/JVvYoQtFiKUubirJlK1sqqRCVlLb5v8/xn2m+985dvzP3ztz7vJ/P93tnzjlz5pznzL3vnHPe93nLWCpCIQJEgAgQASKQBIGySfKYRQSIABEgAkTAIEBlwQeBCBABIkAEUiJAZZESIhYgAkSACBABKgs+A0SACBABIpASASqLlBCxABEgAkSACFBZ8BkgAkSACBCBlAhQWaSEiAWIABEgAkSAyoLPABEgAikR2LBhg/zzzz8py3kV+OOPP7ySmRYxBKgsIjZgbC4RyDUCixYtkv33318GDx6c1a07deok1157bVbX8qLwIEBlEZ6xYEuIQOgQ+PLLL6VFixZSv3596dWrV1bte+GFF+Sll16SHj16CAkjsoIwFBdRWYRiGNgIIhA+BP7880857rjjpHbt2jJixAgpX7580kZCKdSrV0/uvffeEuWQNm7cOBkyZIhcfPHFJfJ4Eh0EqCyiM1ZsKRHIKQLnn3++/PLLL+ZHfosttkh57379+snPP/8ss2fPjivbqFEjueOOO+SJJ56QUaNGxeUzIfwIlCGRYPgHiS0kArlGYOzYsdKxY0ez13DXXXeldfvnnntOpk2bJn369JFdd9017hpskjdo0MDMULC8VaFChbgyTAgvAlQW4R0btowI5A2Bli1bykcffSTY3N5ll118a8fQoUOle/fuMmjQILn88st9q5cVBY8AlUXwGPMORCBSCMycOdNsardq1UomT57sa9tXrlwpO+ywg9SoUUPmz5/va92sLFgEuGcRLL6snQhEDoE33njDtLlz585ptf2HH36Q0aNHy/Lly1OWr1y5shx22GFmb2PevHkpy7NAeBCgsgjPWLAlRCAUCLz11lumHe3atUvannXr1skJJ5xgfvyvu+46qV69urz22mtJr0Hm4Ycfbsq888475pP/ooEAlUU0xomtJAI5Q2Du3LnmXjVr1kx6zyuuuMIsJX311Vdy0003ycaNG+Wee+5Jeg0y7XonTZqUsiwLhAeB5IbT4WknW0IEiEAOEFixYoWsXr1attpqK8GSUSL57rvv5Mknn5QZM2bIdtttJ9jngJQtm/r9094wX7x4caLqmR5CBFKPbAgbzSYRASIQDALwq4DYP+iJ7oIlpAMOOMD8rV27VuCQBzn++OMTXeKk23X/9ttvThoPwo8AlUX4x4gtJAI5Q8D+Ad9xxx2T3vPCCy+U9957z5TBPgU2t+HhffbZZye9DpmwhoLAQ5wSHQS4DBWdsWJLiUDgCNhLT6kYZuFQZzvVwXcCAmqQnXbaKWUbV61aZcqkUkgpK2KBnCLAmUVO4ebNiEC4EbB/wNOlFYfZ7AcffGA6BWc7CJay4E+RSH7//XeTVbdu3URFmB5CBKgsQjgobBIRyBcCUBbYpE7HZwJtfOqppwyTbJ06deToo4+W9evXG1PaV199NWEX7KUuKouEEIUyg8oilMPCRhGB/CAAwkBsXK9Zs0a++eablI2YOHGiKXPyyScbJQPl8euvv8oxxxyT8Npvv/3W5EHBUKKDAJVFdMaKLSUCSRHAbODDDz+U6dOnC0j7YgXLP1gy+uKLL4xPRGy+fd6+fXtzaC8v2elenzYbbbNmzeTjjz+W/v37GyLBZHsXEyZMMFXZ9/Gql2khRACssxQiQASii4BuGFtnnXWWVa5cOUt/YsyfWhxZt9xyi7Vp0yZLfSesLl26OHkoo45x1pgxYzw7rc5ypuypp57qme9OfPfdd62qVataujFuVapUyerZs6c7O+5YfTgsVTDWvvvuG5fHhHAjgPVGChEgAhFFQGcL1kEHHWQp1YalsSIs3Vi2lPzP2n777c0P/m233WYdccQR1rbbbms9//zz1sKFCy0l8TN56kxn6QwkrufqiW01adLEUsc8a+nSpXH5sQmoQ2cV5t6xebHnGkTJ3Pvuu++OzeJ5yBGgsgj5ALF5RCARApg1HHrooRZmEboPUKKYhkA1P8r2TEPjU5h8DUBUIn3ZsmUlrrNP1OnOlLvmmmvspFJ/Qgnts88+1jbbbGMtWbKk1PWxgtwiwD2LEC4NsklEIB0EEGxo6tSpxnsaQYXcojML57R169Zy7LHHmnPbEgkniF5nm8o6hf//4MgjjzTWTY8++qioQonNzuocoVm//vprGThwYFr+GFndhBcFhgDjWQQGLSsmAsEigB97cDjZvEzuu8GTWpedTNIzzzwj55xzjjmG/4QuAUm1atWkW7dujje1+1r7GPxPoBPfY489BFZPFStWtLMy/vzxxx+lefPmRkGhrnQ4pDK+CS8IFAEqi0DhZeVEIDgEdEPZxLPu0KFD3E323ntvsU1UEe3OZnqNK5giATEn2rZta2jFbeWT4pK4bNB6IPIe6EAwEwLxICV6CFBZRG/M2GIiYBAAJbhaQMWhgaUmmK7qirbUr19fMEMojWBWAIWBmBUXX3xxxlXB5wJstqNGjTIxLzKugBeEAgFyQ4ViGNgIIpA5Al6KArUgFCoUBeTwww83n6X5t9tuuxkqci/fjXTqRYwL7Kkkam86dbBM/hGgssj/GLAFRMBXBN5//32nPmxu+yHJnOxS1a8WUKmKMD8CCNAaKgKDxCYSgUwQUEc5p7gfMwunMh4UNQJUFkU9/Ox8oSGA6HP2xjaWj2rVqhXXRSiTvfbaKy6dCUQgGQJUFsnQYR4RCDECYHYdMmSI/Pvvv04r3377bee4VatWzrH7oG/fvsbk1p3GYyKQCgHuWaRCiPlEIKQInHLKKaZlME3t3bu3OX7xxRed1sIPI1aw+Y242XbAoth8nhOBRAhwZpEIGaYTgYggAP8FCFhibcpwnGNJyi3KCyVKOCjKJSXnnXeeO4vHRCAlAlQWKSFiASIQXgRq165tFACoyc844wxp2rSp9OjRwzQYsbFteo/x48eLEgqK8jIJ0suUKRPeTrFloUSAy1ChHBY2igikRqBKlSpm9rDffvuZgEOwfAL/EuJog05j0KBBAmWy9dZbm8h3cKxDvhIPpq6cJYhADAL04I4BhKdEgAgQASIQjwCXoeIxYQoRIAJEgAjEIEBlEQMIT4kAESACRCAeASqLeEyYQgSIABEgAjEIUFnEAMJTIkAEiAARiEeAyiIeE6YQASJABIhADAJUFjGA8JQIEAEiQATiEaCyiMeEKUSACBABIhCDAJVFDCA8JQJEgAgQgXgE6MEdjwlTiEDBIbBuo8jnf4rMXy3yzwaRbfSbv/u2Io00HHY5vjIW3HgH0SEqiyBQZZ1EICQIfLBE5OG5IuN/EVmjCiNWKlcQOaG2SA8Nb9GsWmwuz4nAZgRI97EZCx4RgYJBYPE/It2nq5IoSTybtH9n7Soy6CCR7SomLcbMIkWAyqJIB57dLlwEPvpd5Lj3RH7bHBMp7c7uuo3IW21F9qyc9iUsWCQIUFkUyUCzm8WBwOfLRQ57R2Tl+uz7W2NLkekdROpsnX0dvLLwEODWVuGNKXtUpAisUgVx0qTSKQpAt3iNSGetZ8OmIgWS3fZEgMrCExYmEoHoIXDL5yI//u1Puz/6Q2SwboxTiICNAJehbCT4SQQijMDva3XZ6DVvi6dsu7VzJZGfTxKpWC7bGnhdISHAmUUhjSb7UrQIjJjvr6IAkEtUAb2ZgTVV0YJfJB2nsiiSgWY3CxuBtwP6UX8noHoLezQKs3dUFoU5ruxVkSHwhXpnByHw+qYQASBAZcHngAgUAAJYMgpClqhlFIUIAAHSfRTxc7DJEvlDHbdWKVcQzCQr6KsD6B+2Vw/eMmWKGJgIdt3SsQxCAqo2iKayzoARoLIIGOAwVg8qiK9WiCxUUrkNHr8GFVVp1FOHrIZKMreDWsRQwo/AjjpO8I/wW1AvhQgAASqLInoOwDY6eanIAlUWyWSdzjLmrfrvbw9lJj2kuppPcsEyGWR5z4NiD0JZoF4KEQAC/Akokudgma5pv7IgtaKIheM7VRqv6nUr1sXm8DxMCLTbJZjWtNs5mHpZa/QQoLKI3phl3GLsS4Cieq0HRXU6lYFGYswikb91ZkIJJwJd6ukygc/7TNvp/lXHWuHsL1uVewSoLHKPeU7vuF6XlCb8KoKlpdLIP6poJqrNPTbFKeFDoJbuMZ2zu7/tunJvka1VYVCIABCgsijw52C2cvyUhoHUDc8ynaF8rRvjlHAicPt+ItV92pBuoBTlvfcNZz/ZqvwgQGWRH9xzclcsO/n94/6ZUmBzdpGT4cv4JtWVWvz2mj9KOSvL9cb/v2MVnU28cpjIljR/yXgMCvkCKosCHt0fdHPayzS2NF3GctQCNbmlhA+B5cuXy+1dj5SN9x0v5TboNDALqaY+Nm9r8KOGVbO4mJcUNAJUFgU8vAtTmMhm2/VFAdWbbXt4ncjGjRvl9NNPlx9//FHk83FS6Y7WsteWmbl1H76TyOxjRZrvSESJQDwCVBbxmBRMyvLsXi5T9j+oelPemAUSItCnTx+ZMGGCyS+j7vfD7uwjX55USZ47ROSgagkvM5uWR6iSGH24yPvtRepqWFUKEfBCgKuSXqgUSBqWjIKQoOoNoq3FUOcrr7wi99xzj9PVm266SU444QRz3nU3Efz9orPBmRqb+ycNjrRGTaC30X2J3VUxtNRZBL30Heh4kAQBBj9KAk7Us4Z+p5vRAXQC/FGn1wugYlaZFQLLli2Tzp07y+TJk6VTp04yevRo5fby2ekiq5bxokJCgMtQhTSaMX2pFFCEs6DqjWk+T9NEoHr16jJx4kTp37+/DBs2jIoiTdxYLDMEOLPIDK9IlYbXdhCb0XupDf5hus5NIQJEoHgQ4MyigMe65lbBdC6oeoNpLWslAkTADwSoLPxAMaR11FfGWL8HGOyzdZVagpI/BD7++GNjKpu/FvDOxYiA378lxYhhaPu8tdq67alLRn5KI6WsLs+nxk9IM6pr1qxZcthhh0mHDh0ETngUIpArBPi1zxXSebrPgWpjv6VPG91gIW1Cz948jaTI0qVL5cQTT5S1a9can4pu3brlrS28cfEhQGVR4GMOfp+2GpOgbCktKbH8hJgJnFXk54FZv369nHLKKbJo0SLTgKpVq8r999+fn8bwrkWJAJVFEQx7Dd3obo8f+iwVRiV9So6pqbG5tygCsELaxZ49e8rUqVNN68qVKycjRoyQ3XbbLaStZbMKEQEqi0IcVY8+1dFN6RNqi1TL8Ae/hjKZnlTHP+prj6YxKQUC69atk++//94pNXDgQGnfXrk5KEQghwjQzyKHYIfhVpYGL/pe2Wi/+ksE8Sm8BBOQXVRJIP5yPXIFeUGU8zQQBV533XWyePFiefHFF3N+f96QCFBZFPEzsFo5ghCbG2FTQWVeQeeZlXWPYydVFPTSDueDsWnTJilblgsC4Rydwm4VlUVhjy97RwSIABHwBQG+ovgCIyshAv4igGUnChEIEwJUFmEaDbaFCCgCjzzyiLRp00bAJkshAmFBgMtQYRkJtoMIKAJTpkyRtm3bCvwqateuLe+++67ssccexIYI5B0BzizyPgRsABH4D4GFCxcaxzsoCshOO+1kFMZ/ufxPBPKLAJVFfvHn3YmAQQAUHieddJKz9ARFMWrUKKlUqRIRIgKhQIDKIhTDwEYUOwKPP/64gCQQUqFCBUGo1Fq1ahU7LOx/iBCgsgjRYLApxYtAjx49pG/fvibK3YMPPiiHHnpo8YLBnocSAW5wh3JY2KhiRWD27NnSrFmzYu0++x1iBKgsQjw4bBoRIAJEICwIcBkqLCPBdhABIkAEQowAlUWIB4dNK1wE+vfvb0gBC7eH7FmhIUBlUWgjyv6EHoG77rpLbr75ZjnggANk+vTpoW8vG0gEgACVBZ+DgkFgw4YN8s8//2TVnz/++COr6zK96J133pHrr7/eXPbrr7/K8OHDM62C5YlAXhCgssgL7Lyp3wgg3Oj+++8vgwcPzqrqTp06ybXXXpvVtele9MMPP8jpp58uoBmHtGrVSu699950L2c5IpBXBGgNlVf4eXM/EPjyyy/l6KOPlgMPPFBGjhwp5ctrUI4MZf78+XLYYYfJCSecIPBzKFMmyxi0Ce6LGU/z5s0FbYXA4Q5mstWrV09wBZOJQLgQ4MwiXOPB1mSIwJ9//inHHXec4VBCXOpUiuKFF16QevXqxb3RI23cuHEyZMgQufjiizNsReriW221lXTp0sUELgKFB6g8qChS48YS4UGAM4vwjAVbkgUC4FMaM2aMfPrpp9KwYcOUNYDBFfGs8cPtFZ4Us4orr7xSXnvtNTnxxBNT1pdpgfHjx8uKFSvM/TO9luWJQD4RoLLIJ/q8d6kQGDt2rHTs2NHsNcDCKB157rnnZNq0adKnTx/Zdddd4y7BJnmDBg3MDAVLRuBpohABIiBCZcGnILIItGzZUj766CPB5vYuu+ziWz+GDh0q3bt3l0GDBsnll1/uW72siAhEGQEqiyiPXhG3febMmdKiRQtjUTR58mRfkVi5cqXssMMOUqNGDcHGdzaCOv766y+pU6dONpfzGiIQOgS4wR26IWGD0kHgjTfeMMU6d+6cTnGB2ero0aNl+fLlKctXrlzZWEb9/PPPMm/evJTlYwtYliVdu3Y1hIAffPBBbDbPiUAkEaCyiOSwsdFvvfWWAaFdu3ZJwVi3bp0xh4VZ7HXXXWcskLB5nUoOP/xwUwROdJlKv379BMrs999/lyOPPNJsqGdaB8sTgbAhQGURthFhe9JCYO7cuaZczZo1k5a/4oorzFLSV199JTfddJNs3LhR7rnnnqTXINOud9KkSSnLugtg9jJgwAAn6aqrrpL69es75zwgAlFFIHPvpaj2lO0uGARgerp69WqB7wKWjBLJd999J08++aTMmDFDtttuO8E+B6Rs2dTvSPaG+eLFixNVH5f+9ddfy9lnny1YhoK0b99eBg4cGFeOCUQgigik/tZEsVdsc0Ej8Msvv5j+2T/oiTqLJSSQ9eEPMa7hkAc5/vjjE13ipNt1//bbb05aqoOPP/7Y4abafffdBU6C6SimVPUynwiEAYHAlQW+pHZs4TB0mG2IPgL2D/iOO+6YtDMXXnihvPfee6YM9imwuQ0Pb7z9pxJYQ0HgIZ6unHPOOfL2229L3bp1zWZ61apV072U5YhA6BEIXFlgU/Hggw829Ao2gVroUWEDQ42AvfSUimEWDnVYqoLAdwICapCddtrJHCf7t2rVKpOdSiHF1tG2bVvB8lc63uSx1/KcCIQZgUCVBd6y4Ni0fv16ueaaawQWJjBHpBCB0iBg/4CnSysOs1nbhBXOdhAsZcEXIpHAkgmCWUKmQq/vTBFj+SggEJiywFtft27dnM0+gDFlyhRp0qSJs3YcBYDYxvAhAGWBvYB0fCbQ+qeeeso8h3CQAzstXl5gSvvqq68m7Jy91JVMWdgb2QkrYQYRKCAEAlMWmP4//vjjYr8F2pjBkuWss84yRGrwcKUQgUwR2GKLLcym9Zo1a+Sbb75JefnEiRNNmZNPPtkoGSgPBB465phjEl777bffmrxEHthQVNg4x+yZQgSKAYHAlAXAA8nbnDlzpEOHDnFYwlKkcePGkqkde1xFTIgMAtiz+umnn8ySENb1S7OHBbNUiL28lAwEKBdIs2bNBBZLiH8NIsFkexcTJkww19j3MSf//w++Gqeddpp88sknRuE89NBD7mweE4HCRECn0jmR++67z9pyyy1hgF7iT5cTrL59+1rqaZuTdvAm+UHg6aeftnbbbbcSY6/mqdYrr7yStEFKF26pY1tcGX3JMHWdeuqpcXmxCe+++66llkmWboxbGkvC6tmzZ2yREufqw2GpgrH23XffEun2CdpjP8caJMl6/fXX7Sx+EoGCRQBruTkTdVqyNPSl80Wzv3D4RDryKYWFgM4erN69e3uOuT3+Osv07LR6aVt4mXj00Ufj8vXt3tL9L0uXO62lS5fG5ccmKPW4pbMKSze1Y7PiztEetO3uu++Oyxs2bFiJvtxyyy1xZZhABAoRgZwqCwD477//WnhbxI+A/WNhf2Lm8cQTTxQizkXbJ7U+csYZb+F4W9flR/Mjb4/7NttsY+mGchxG5557rqUR7BLOOtXpztStlnZx12abACW0zz77WGjTkiVLSlSjPhdmdmK3W0OwWlCGFCJQDAjkXFnYoL7//vtW7dq1nR8S+wuIT914jPui2tfxMzoIKJmeGV+8GOAHXTeVncarRZKlBhBmeQhjfsMNNzh5OFBqcEsd6KxHHnmkRHrsiVo3WVtvvXVas4vYa73O1cvbtFlNvr2yLbXos3SvwyiUdGYpnpUwkQhEEIG8KQtghTc1DYvpqTDUisrCjw0lugio2anZI8D+QiJRz2oz/nhxcL+lX3LJJdbOO+9sqcVToktNulKIm3KtWrUys9akhVNkqj+GpZ7b1hFHHGFhhpFIFi5caGlo1kTZTCcCBYlAXpWFjejzzz9fYnrvnmUoa6iFDUdK9BDAOCrTa8qGn3/++UZhTJ8+3ZTFDAQbzMremvJaFMDeRq1atSw1yU6rvFchNYW1NJyqWSbDSwyFCBCBkgiEQlmgSVh2OPTQQz1nGfgSK79UyZbzLPQIYL9BnTNTtlP9bcySk61Y1LTVzEi89jESVYZZAe7ntRme6Bp3upp3W0pL49tylrtuHhOBQkAgVGFVYXcPG/jbb7/deNnqm6kjoFBAOuID6Bq4k86D8CIApzk1Wkirgc2bN5dq1arJmDFjZNdddzX03jafU1oVaCG1ihK1enJiUaR7HcqBXlxfSqRcuXLOZYjvDee9dFhqnYt4QAQKFYEwajyYOO65556eswysTWMWQiksBLABrgGHjM+Cftes2bNn57WDWApDe2DBdfPNN5fYT8lrw3hzIpAnBEL5ig4ahU8//dRwS8UqaZtf6uWXX47N4nmEEQBnmJqqysMPP2xoNJo2bZq33iAU6ymnnGLIBvV7acgwMcOgEIFiRiCUygIDAm4pcPgglrEXvxToFrp06SLklyqMxxeR7ECjAZqNiy66KK+d6tGjh0ybNs20ActSL730ktSoUSOvbeLNiUC+EQitsrCBIb+UjURhf2677bamg9jjOP300/PWWXUKNQSYdgPuuusuadeunX3KTyJQtAiEXllgZED4Nn78eFF+qbgNU7V5lzZt2sj//ve/uE3xoh3VCHYcyz0QbCar93TeeqC0M6JmuOb+Z555pjGoyFtjeGMiECIEIqEsbLx69eoluvEp+EK7BVZUt956q8CiJh3Kave1PA4HAnawIfxA51MOPPBAEwYYS2FDhgzJZ1N4byIQKgQipSyA3N577y0zZswQ5ZeKM6HFpjhoqPklD9UzllZjZs6caeJjIyxpvgUz2cceeyxuFpvvdvH+RCCfCITKzyJTIBDL4OyzzxYsRcUKAttggzxZzILYa3iePwRgDYWlqC+++CJ/jeCdiQARSIhA5GYW7p4gpjd+XJRfyp1sjrHH0ahRI+PkFZfJhFAhgHjYGEeMV64F8bkpRIAIpEYg0soC3YPJJWIpK7+UaHCbEj1GHOVOnToJTCERE5wSTgSUatw0LNfKAlEasayJqHnY96IQASKQGIHIKwu7a4jrjbdT5Zeyk5xPpZsWOHlhc5wSPgQ0kp1pVIsWLXLWuAULFkjnzp2NBd2dd94p1113Xc7uzRsRgSgiEOk9Cy/A8YaobKVy2223xZnSkl/KC7H8pymthsAaasWKFaJhTwNvEDir8FKBGNoQpUI3FlBoB4UIEAFvBApmZmF3DySDyuUjH374oSi/lJ1sPjXgjlx77bWCvY6ff/65RB5P8oPATz/9JIsXLxaYrOZCUaCXGr3PURQVK1YUjQOeFflgfhDjXYlAfhAoOGVhw2jzS5133nl2kvNJfikHirwfwGQWogGHctKWZcuWOVQeuOFDDz0khxxySE7uzZsQgSgjULDKAoMCfqknn3yS/FIhfkK/++470zoQ9+VCqlevbpac4M9x4YUX5p2HKhd95j2IgB8IFNyeRSJQEOugW7du8uabb8YV0ZCexpqqdevWcXlMCBaBL7/8UjDT0zCqwd4opnaQFmJ/C/tYFCJABFIjUDTKwobigQcekBtuuEGwyekW7HUgXaO18QfEDQyPiQARIAKKQNEpC4w6+KPAQQR6kFgB79QLL7xg7O9j83hOBIgAEShWBAp6zyLRoNr8UiAmjA3RSn6pRKhFM/2OO+6Qnj17mnCr0ewBW00EwoFAUc4s3NCTX8qNRmEdv/XWW3LssceavQmYS48aNcp4/BdWL9kbIpAbBIpyZuGGFj8i8Pw++eST3cnmmPxScZBEJuH77783kRRtGg+QFOYzTkZkgGNDiUACBIpeWQAX8EvBMYv8Ugmekogl//333yaIkh1yt06dOjJy5EhDgR6xrrC5RCA0CFBZuIaC/FIuMCJ8eOONN8rXX39teoAwrVh+io3jHuHuselEIC8IUFnEwF63bl0BG2m/fv3iTGjnzp0rLVu2lHvuuYcspTG4hem0f//+gngmEATCAokkhQgQgdIhUPQb3MngmzVrljGxnTdvXlyxVq1amWUrKBdK+BDAXsXbb78tHTp0CF/j2CIiEEEEqCxSDBriYFxxxRUm6l5s0SpVqsgTTzwhp556amwWz4kAESACBYUAlUWawzlmzBg5//zzBQGVYuX000+XRx99lGaZscDwnAiEAAFYwo0bN05gJr969Wo58sgjPaNrhqCpoW4C9yzSHJ6OHTvKnDlzPJc1RowYIY0bNzZ7HWlWx2I+IYDlpsGDB8u6det8qpHVFBICCEXQpk0bEzHz119/FYTwhZn8wIEDC6mbuemLal1Khgjcf//9llrZWDpCJf7UG9xSSxxLf7gyrJHFs0Wgb9++ZgzU8MDSuBjZVsPrChCBhQsXWkoSap6PF1980fRQXy6sJk2aWFtssYWltD8F2OvgusSZRRY6+corrzQhWsEj5Ra85SJCX/PmzeXbb791Z/E4AAQQex14Q6ZPn+65rxTAbVllBBCAr0379u1FFYZZPu7SpYtpdZkyZQSxbv79919joBKBroSmiVQWWQ5FKn4pmGti85sSDAKgNj/33HNF36PMDY4++mi5/vrrg7kZa40cAoipDsJQhMq97777SrQfIQkg7733Xol0niRHgMoiOT5JcxGSEw/iu+++K/YDaF8ACvSLLrrIcBMhlgbFPwT+/PNPOeGEEwRvj5D69evL8OHD40gh/bsjawoCAYQ5BsMzQuv6KVOnTjUGJ6gTqwCVK1cuUT2+txDO/kvAkvKEyiIlRKkLkF8qNUZ+lkDAIl13NlWC7+n111+nJZqfAAdcF2hY7rzzTtl1111NQLKdd97Z1zuCaRgzTkTKhAVjrNgWjatWrXJmprFleO6BQHDbIcVZs/JLWfomYzbVFO4Sn5dffrmlpnvFCYzPvcZG5YABA6zXXnvN55pZXVAIKLmjpT5L1tZbb+18Lw466CBfb6ezFAuGJvjuadhcz7pPO+00k68vHRaeI0p6CECzUnxGYP78+dahhx7qfCHcSqNBgwaWeob7fEdWRwTCi4AuC1knnnii8yPu/j7oMpGvDe/Tp4/zvZsxY4Zn3brfaMpUrVrVM5+J3ghwGUqfXL8lHX6pu+++m/xSfgPP+kKFADaYYRmoL06GzBHWgrFy8MEHxyZlfQ5fmyeffNJcv8MOO8iBBx4YV9fKlSsFHG8QkkvGwZM0gcoiKTzZZyIC38033ywffvih7LnnniUqwsZe7969BXsdcBqiJEcAG9pr165NXoi5oUNgr732MgYeMFdNJIccckiirIzTEWbA3o846qijPA0eZs6c6bykoX2U9BGgskgfq6xKwqYboVrPO++8uOunTJliNmpfeumluDwm/IfAxo0b5ZRTTjFvp7CZp0QHASiJm266SeAPg83mWMEMvEaNGrHJWZ+DkscW3HPfffeN+zv77LPtImbW45zwICUC5VOWYIFSI4AvCqbHMPeM5ZdasWKFgFtq9OjR5JfyQPqaa65x7OF1M9QsIcSaQnpcxqQQIYAZdLVq1QSknG7xcwkK9eoehVP9xRdfHBdiAJlunwu/7+/cvFAPvLcymBoUAkuWLLGUNtvZhNPnyjkGNYGSnQV168jV+9xzzznYACdYP1GihcCGDRustm3bmnEEzYbb8GPQoEG+dUZ9mZxnRU1yPetVk12rXLlyppxGT7R01upZjoneCNAayhuXwFPJL5Uc4o8//tiqVKmS8wNw0kkn0cwxOWShzIW5OBT9TjvtZIGrCbxp6qxq0j755BPf2gwLQ/vFS5eaPOvVPQ2nzK233upZhomJEaCySIxN4Dka+tNSfinnAbYfdnwivZiJzq666ioHl4YNG1rqQBX4ePAG/iIwdOhQM4bqMW2poUeJypUKx8Kswy956623nOcFL2Je0rlzZ1MG/hXKQOtVhGlJEKCySAJOLrKU0Mzq1auXpw06mG0ff/zxXDQjlPfQ8LWWmkBacOaiRAsB+FZASeDF56mnngq88WCVtV+2oDhiBcu/UBIoA6VByRwBWkPp05NPIb9UYvSvvvpq+eGHH2T33XdPXIg5oUNgwYIFJrgQ/B4wht26dQu8jeoV7tyjevXqzrF9oApLYLIOueSSS+xkfmaAAJVFBmAFWRQWI1988YUJzBJ7n/Hjx0ujRo3EbRoYW6ZQz2n5FK2RhcXT8ccfL8uWLTMU4eCAyoW4HezUM7vELaEkbAboY445Ro444ogS+TxJDwEqi/Rwykmp7bbbTuBYpPxScUyZcDbq1KmTiQcea4KYk8bxJkQgBQK6sGFo4z/77DPjiAr/IbU+SnGVP9lKo+PcK9aB85FHHhGl4JFtt91WHnvsMX9uWIS1UFmEcNDPOussM8sATUKsPPzww4JYGbNnz47Niuw5wl0iHsWPP/4Y2T6w4SK33HKLjBw5UqpUqSJvvPFGTpmAt99+e7H9JqAYbNH9LuMYCEaFZ555Ji6UgF2On2kgkPk2B6/IFQKwA9cvoLMxp8PpbOJhs+6uu+6KvK04NvgREhV90y+8NXHixFzBy/v4iADYf9Vj2xhq6LKpjzWnX5V6bZvn6MwzzzQX6X6XpZQepl3FbCiSPoLJS9IaKjk+ociFz4HySzmKwq00WrVqZYHlNqpywQUXOP2Cw5QGkopqV4q23Z9//rlDO37vvffmFYcbbrjBvFzh+4I427vttps1adKkvLapUG5OZRGRkUQcDOWXcn5Y3QpDp/3WiBEjItKTzc3UteQS/UlkH7/5Ch6FDQHdS7Pq1atnxjGRM1yu26zEk5bS51hqMEJHTh/BL4O69IeHEhEEYBEVyy9lNx0cU48++mhO14rte2f6OW/ePFFnO8ecsWvXrqL0HplWw/J5RABWRu3atZPJkycbUj59gxd9m89ji3jrIBHgBneQ6AZQd8eOHWXOnDmi/FJxtevsQho3biz40oZdQNuuMwlD9tasWTPHtDHs7Wb7NiOgUe+MoqhZs6aJV0FFsRmbQjzizCLCo/rggw/K9ddfL2vWrCnRC1h+IB3xNHQjvERe2E5A067LGLRSCdvApGgPrPKgLJRlwCgMUPFTChsBKouIjy+ikan1h4mZEdsV5ZcSpUEQBnmJRYbnpUHg/fffNw53yu1knq8uXbqUpjpeGxEEuAwVkYFK1EyNJ2x4/JV4Ly4yGIIuwSfD9l5NVAfTiUC6CMAXRrmVBIpCLY+EiiJd5KJfjjOL6I+h0wONhSGIBOYVUQ40B+DHUapop3wuD6ZNm2YC4HCWk0vU/b2XMv+K+sTIV199Jdg7Q8AuLHlSigMBjnQBjbPNL4UwpLGST34pxBlHlMDmzZsXJb9V7FhE8RxGk2AWgKLYZ5995IUXXqCiiOJAlqLNVBalAC+Ml4JfCpQLYeGXwub7iSeeKL///rusXLnSMH7GbsiHEUe2qSQCWHIChQdoNfAJniVKcSFAZVGg4x0Wfin4hGDvBAI6digyWNBQooMATLLvuOMOKV++vBk/UsZHZ+z8bCmVhZ9ohqyuunXrGp8LELzFmtDOnTvXrD/ffffdsmnTprRaDjNXbKSDsK1GjRrG+a9WrVrSunVrs9mpYTJL1KPBi2T48OFOGswtseZNiQ4CIKxU5gDTYJhqt2nTJjqNZ0v9RcBHb3BWFWIESsMv9fbbb1tNmjQpQc2hT6HnuTLlWjNnzjRIIEIa+HlQ9uKLLw4xOmyaFwIIPaovA2b8unfv7lWEaUWEAK2h/NW9oa4NcTB69OghTz75ZFw7QSutzJxy2mmnOXkwj0R5UIhkIrCQuemmm8zfRx99JPfdd58MGzYsbnaTSZ0sm1sElA1YYDAxY8YMUbJKUYLHQMYPpriIqBd1wRKrkhZGvRvJ219EipFd/X8EdIPS0shinjMDtZu3QMSmX2Dr2GOP9SyjT1Ra6WCUpUQTgXPOOceMsS5lWhr1LrBO7Lrrrmk9S+k+c/kqt8ceewSGUVgq5swiuS4t2NylS5ea2MhvvvlmXB9r164t++23ny9mrgMGDJC+ffvG3YMJ4UUAe03XXnutIK41/GN0CTKwxiL8KmawURds/nvF/o56v9ztp7Jwo1GEx4n4pfyCAmE1db9EQD1CCT8Cuj8lOqM0Rg8I8XvSSSeFv9FsYU4QoLLICcypb7J8+XL59ttvRaONyYEHHmjMFN1XwU/hyy+/NHbu++67rxNv2F0m2+Nk/FLZ1um+7sgjj5R33nnHncTjECIAC7kWLVrIX3/9Jf3795f//e9/IWwlm5Q3BMKyHlas7VAKBUt9IixEidOHwPztsMMOJpyqmrRaK1assLCPYOfhUymhLY1r4StkCG+qZrEmBKX7Xn4dq0Lytb2szF8EsE9lR2M8+eSTGTTIX3gLojZGysvjMOpswTrooIMsXeu0lOzPUg9nSwPJmFjU+JG+7bbbrCOOOMJSb1lLPbIt5Xyy1L/BKA711LZ0rdf31nfq1KmEYvJLWaAvlHAigOfoqKOOMuMOE+m///47nA1lq/KKAJVFnuDHrAE+CZhF6PJTiVb06tWrxA/22LFjTb560ZZID8JKJV1/ikyVyHHHHVeijzwJDwL28wYLOeXxymvD1q5dm/T+sNLDdydISdUG3DudMkG2MR91l9UvPSUPCCCE6NSpUw0hW4MGDUq0APw7tsA7GhuOEI13bCdLo0aNRL/czrlfB7B7D0J++umnIKplnaVE4Nlnn3UiFr722mtSp06dUtaY/eVgscWz78WabNeqM3Hj+2Of+/2ZThvSKeN3u8JQX/kwNKIY2wDzRDz47du3j+s+4lPb0q1bN/vQRL+DiV61atWM2auT4TrQNw5DsfH0008LNixB2gdlBDK/yy67TCpVquQqHX8Ix70gZPXq1UFUyzpLgcD06dPloosuMjU88sgjojPdUtRW+ksXLFggeP7wUgTzbS8Bg3GiPK/ySMN3Yty4cQIKfzyHMLhIZOWVThvSKZOoLVFO58wiD6OnU1j57rvvpF+/fp53h6mpLe3atbMPjZIAoRts4HX5ykm3DzZu3Gi+BIicB+4mvDXCjwKeuNddd53xnViyZIld3PMTtvVBSFD1BtHWYqhz0aJF5lmBpza89NWBsiC7DeUCPivdixOlL5FffvlFdANfBg4cWJD9DbJTnFkEiW6CuvF2jzcc+CDECt6qMCOA1K9fX9TyKbZIwvNbb73VBKRR71t55plnnHKIjww6AsTk7tq1q0yYMMHJiz0AZcFnn30Wm1zq84KnQig1QrmrALNNxBfBi0Pbtm3l3nvvzd3Nc3gnKERQlWBZC+GFEdUPswz4/IBcE7NtBuNKf0A4s0gfK19LeikK3ECtocwDjWPMCNIVxIqwv/S9e/eOu+zGG280imfixImOMoorpAlBscIGVa9XH5iWHAGwyIJNFlTjL7/8cpxPT/Kro5GrFl1miReKAjT5dvhX+DHh5QkzKsR8oaSPAJVF+ljlpOT777/v3Aeb2+kKNssR9lJZXj3flqCcGjZsaKp77LHHElaLKXoQkmiNOIh7sc7ECKgJsyA+ReXKlZ1gRolLRzcHy65wNsXMHESWbrH3PN577z13Mo9TIEBlkQKgXGeD3dOWTGYWYAeFKDFbwnCX6nRlyiRbhsL6LjzE/RS14Teb7H7WyboyRwAR7uCVDVZghEVFeNRCFLw42UzJV155pVGM7n5iSRYCxgRK+ghQWaSPVeAlFy9e7DzAWONHYKFYgTLxWme1N8XdZrex19qhMN0muLFlME2HpZZfgh8mvM1S8osAYmcjeiLW7LG5q34v+W1QgHeHEQj6udVWW5klqNhb2c8/ZuIoR0kPASqL9HDytRSsll599VUZMmSIWTu1KweJmy3YmPMSMLjiSxArYJGFwLQ2kdjR8sAzlSw63tFHHy2XXnppomoySsd9NHCOowQzupiFfUHgjz/+MNZA+HE844wzxGtPy5cbhaCS+fPnGwtANAXKsWrVqnGtwksZBC8ylPQRIFrpY+VbSZjxnXLKKXLhhRcKWF9tgcWGLXC6ixVsfmO5Cf4SsQJzXEiijXPk2coCP+D44UgmaNfxxx+frEjaeYjB3bRpUxNcKe2LWNAXBED/3blzZ4GzJQgqvQJf+XKjkFSCAF72i5AdDja2aV988YVJ2mabbQxxZ2w+z70RoLLwxiXQVHcMCXsmAIchWCrZYr/92Oew6sCbEhz5vL4EWD6C2F8U+zr3J2Y0ttjKxT6P/US7QFENG/xMBW9siLznFphramhV441uz4Lc+TwOBgGMH4wmdtllF2NWncopM5hW5KZWRNyzlSH8kKAcYwVWg7ZpehAMCLH3K6RzKos8jKY9/YVVBhTAhx9+aJYH8PZt/ziDesFeWx0/frwooaDgTQjptmJwNx15kGSBZNzKIh0nOSgMzDCwT9KsWTP37RIeo50IpYrlAMyeYgV9waxJWXNjs3juMwJ4y8ZGLyzkRo0aZRw1fb5FqKrDy439nYFRhf09czdS48M7L1Ree3/usjwuiUDiBe6S5XjmIwIw5UOMasweEJEOnqWHH364Y9KIh3zQoEGG1gA/6oh1AecpmDx6eW6jadgQx490suUlvFVBENHLVi4mIcU/WEjNmjXLLIHhRwdLYeB6AjUDNs1hr3/IIYcYz9jGjRs7tY0cOdLE3saymX1vZOILjaU4pN91112eezBOJTzICgEsWV5xxRXmWuyNNW/ePKt6onSR+wUEe4JY/owVfJdsKQZM7L768Ull4QeKGdaB2cPll18un3zyiXz99ddmPR/mqvaM4f777zc/pHCcwmwAP8C2j0SiW8HpDcoEvDWJBB6tEAS4yUZwXabXYuYEziF4jsOk0S2DBw82S28w40x35uK+nsfeCGBWB3+Z9evXm81sYF8MYpuPo69Y8rT36Nx9d/tcHHzwwe4sHqdAgMoiBUBBZWP2AE9S/HkJqD7wl67YDz6inGnApLg9A9RjM7/aZdOtu7Tl6tWrJ5MmTRLQkeAPP2K2YP0Yig7p11xzjefSgV2Wn6kRAI0MDBNg8dahQ4ei4UBCLG8oSQh8jfDCFSv4XtjKAuy6hx12WGwRnidBgHsWScCJUhbezDUWhWkyKBxiBcSFmMlg/Rpv+7kWKEcsvWF/xnYOtNsA5QGP28N1KQ7Eb5TsEIDPAGYRsPbBevzw4cOLRvm6ac0TmZ3DgMTet4MloteeRnbIF8dVVBYFMs5Ywnr44YeNMsCPspvmHHsL+HLgxwQEapmQE/oND2ZSWEsGX0+sTJkyxSg8LKdRMkcA4449JfgWwFs71iIt8xqjcwVmUraAKNBLXnrpJZOM5Smv58/rGqZtRoDKYjMWkT/C3gB+LDB7AJUDNqahJPbYYw+Bhzc8W/EGn2+BU+HQoUPND1qs+SKWCkD6BucxLKlR0kMAxgTwlIefDX4UMebFJO6N67333juu6zDXRtAiCBh3d95557gyTEiOAJVFcnwil4t1auwDwEQVFMzY98CMA05ZYVAUbkA7duwoc+bMMWvr7nQcYwkFG/vY66AkRwCU8ueee66ZOYJ5GMF9ik3cpuCw9ouVp556ytkru+SSS2KzeZ4GAtzgTgOkqBXBzAIR+Lyi8IWtLzvttJNRbA899JD06dPHRPaz24h1aMyOkN6vXz9P6xa7bLF+YmMXG9pYaoSzZs+ePYsSCvcMNZbiA3tiTzzxhMHlmGOOMT5LRQlSKTvNmUUpAeTl/iAAc2KYCseuN8Mj/fbbbzd+AmQJLYk1PJZB/Q5zaVi42UyrJUsVxxlCB9tUN7HsBAgZO18tpeATlIyevziQyr6XVBbZY8crfUYAa82wlb/66qvjLFXILxUPNsgep02bZpw34dlvU2/Hlyz8FLAt2ybhUAy2fP/998YKD5ZPiB5px7Kw8/mZPgJUFuljxZI5QAA/eKBIB8VI7Beb/FKbBwA0LOBBgrHA66+/LljOK3ZB7ArIsGHDzCf26bAvBlYDzLoYgMvAkvU/KousoeOFQSIAnwv4C4AxNVaKnV8K/gKYfUGefvrpuKW7WLyK5RzK4IYbbjChYrEsBYtALNWBpBNWgZTSIZA/ZbFhk2z68x/Z+OtK2bhohWxcslI2/bVGZOOm0vWIVxcMAtttt5354iNWMsKAusXmlwJtCjZ3i0WwrHLqqaca5zJEvcMxZTMCMB/Gpj84x2AuDrzoqb0Zn9Ic5dwaatPvq2XjLyvE+lMVg5eoc1mZaltJuVpVpOx2W3qVYFqRIUB+qf8GHGSMIGD8888/ja8AHCwp8QjgJcOvWCzxtRdvSs5mFtaa9bL+019kw5dLEisKjIN6GVuqUDZ8tljWz/lVrHWbYzAU7zCx5za/VP/+/eNMaG1+KbxNJovnEWUU0S84K37zzTeG4h2zLZt4Msr9Ytujg0BOlAWWl9bPXiTWiv+iuaULj/XHP7J+1kKx/l6X7iUsV8AIwKIFSy/FyC8FXxPs1YCiHhvamVDMF/Ajwa7lEIHAlcWmlWtlwxe/alSeLPcidGax/vNfxPpnM1NpDvHhrUKIgM0vdcEFF8S1rhD5pWDdc/fdd5sZFQL8gFWVQgRyjUCgysJSBbHhqyUa69MqXb/Wb5L1ftRTulbw6hAhAJNRBPUBYZ7bexdNLCR+KQS06t69u0EeXu6tW7cO0SiwKcWEQKDKYuNPf4j869Oew+p1ajVFYrliejjT6WsqfinQtkeVXwqRFMHvBY9kOOAhoE8hi70HY3969RV5yfK9rskkza7b/vS61s6zP73KFGJaGaWtLuVrvzcs1roNsn7GgtLPKtzVVygrFVvWEylbxp3KYyJgEPDil0IG9jqixi8FBYFZBGYW8DmZMGGCICY6hQjkC4HAlAVmARu/15mFz1J+352k7I7b+FwrqysUBGAtdOaZZ3rGXwbv1IsvvmgCA4W9vzAXRrhZ7E/AX6BatWphbzLbV+AIBLYMtWl5Aj+KUgK6KZF/Rinr5eWFgUAh8EvdeeedRlHA4gl7MlQUhfFsRr0Xgc0s1k2f799+hQvlMpW3kApNa7lSeEgEvBEAzcPZZ58t7pCbdklQVSPGQdg4lcaNG2cc77A6jEBWdC6zR4yf+UYgsJmFrMvSVDYFIlZA9aa4LbMjiADW+qPEL4UlNEQIhAPegAEDqCgi+MwVcpODUxaFjBr7FhkE0uGXuuyyy/LOLwUKD1B5gNLjtNNOkxtvvDEyGLOhxYFAcMqiYjBVlwmo3uIY7uLtJTaMP//8c0Gc8lhBcJymTZua4Euxebk437hxoyEEBOkd2gEmWQoRCBsCwfyiay/LbL1FIH0ts00w9QbSWFYaKgTS4ZfC5nKu+aWuuuoqAe049k9A5bHlliTQDNWDw8YYBALb4KbpLJ+wMCMwa9YsY2I7b968uGa2atVKQNRXt27duDy/ExDACLQlCPqEDfmWLVv6fQvWRwR8QSCwmUXZ6uoL4bfznDrlla22tS8dZyXFjUA6/FLDhw8PFCSERIVnNuTxxx+noggUbVZeWgQCUxZlKpaXsrtsW9r2lbi+XO3t/FdAJe7Ak2JCIBW/FCyT8PfXX/7TzCxYsMCE+UQkt169esm5555bTNCzrxFEILBlKGABIsH1Hy/wx99i64pSsZn6V/g9W4ngoLHJ/iOwdOlS6datm7z55ptxldepU0eee+4530j8ENnvkEMOkc8++0zat29vqMfLlSsXd18mEIEwIRDYzAKdLFO+rJTfd+fS/8Dr8lMFP+oJE/JsS6gQwOYy4kU8+OCDcRvMmAW0adPGxHdev750VPlwtjvnnHOMothzzz3lpZdeEiqKUD0KbEwCBAKdWdj3RPAjRMjLKqZFxXJSoXENKbNNRbs6fhKBQBEIkl8Kkf5uvvlmqVKlisycOVMaNGgQaF9YORHwC4FAZxZ2IxFLu4IuIZWpUslOSusTsbgrHFCbiiIttFjILwTS4Zd67LHHMr4d6Dv69etnWHBHjBhBRZExgrwgnwjkZGbh7uAmja+98ZcVieNwg69elUS5WlUESoZCBPKJAMxZ/eCXAu3IwQcfLKtXr5Z77rlHrr766nx2i/cmAhkjkHNl4bRQN783rVor1toNIhs1pEZ5VRKVKkjZbdXprlxOJjxOU3hABJIhAGuoCy+8UEaOHBlXDFH64CuBIEyJ5Pfff5cDDzxQ5s+fbxTPs88+m6go04lAaBHIn7IILSRsGBHwRgCxsMEjBf6mWIG/BOJkwxzXLdgQb9eunUyePFlatGhhHO+22EJfiChEIGIIUFlEbMDY3PwigNlB165dZerUqXENwWY1AhY1a9bMybvooovkiSeekJo1awq8xnfeWa0DKUQggghwvSeCg8Ym5w+BTPilBg8ebBQFuJ5Gjx5NRZG/YeOdfUCAMwsfQGQVxYlAMn4pON3BPwOBlxDKtUuXLsUJEntdMAhQWRTMULIj+UAA3tg9e/aUoUOHxt2+cuXKhnp8yJAhcXlMKE4EELvtt7Uiq9S3c4Pa9ai/sVSuILKjbmOpD3Oohcoi1MPDxkUFgTFjxsj5558vv/32W1yTMatAzAwEYqIUJwLz/xb5SinGFq9RGiQPCMBiVEdtIxrqI1KjpI2ER+n8JFFZ5Ad33rUAEcglv1QBwleQXVqpM4gPlLxiic4m0pW6Sqx9WHWRLcune0VuyoV84pMbEHgXIuAHAja/1EMPPRQov5QfbWUdwSOw+B+R1xZkpijQqp9Xi7yq1y3/N/g2ZnIHziwyQYtliUCaCATJL5VmE1gsjwgs05nE2EX/7Utk24xKSkR8vBJtVwkJLR5nFtmOJK8jAkkQsPmlrrnmGsMF5S766aefmljb2fBLuevhcTgR+HejyIRfS6co0LO1/1/PJq9Njjx0nTOLPIDOWxYXAn7xSxUXatHt7bRlupm9wr/2H1RNZL/t/asv25o4s8gWOV5HBNJE4PDDDxcQCZ5++ulxVyCGRqNGjWTs2LFxeUyIHgL/KNXdt/FsMKXqyBdqRQX6vHwLlUW+R4D3LwoEYDaLmN4gEYT/hVtgbgsiQvBOwW+DEl0EflATWb9/2LEctUA3vfMtVBb5HgHev6gQAN35559/Loceemhcv+GL0bRpU5k9e3ZcHhOigcCigH7UF4XgHYLKIhrPIFtZQAjY/FIDBgyQChXUfdclc+fOlZYtW8qdd94pmzapuy8lUggsXxdMc8NgRktlEczYslYikBSBsmXLSt++feXDDz8UxOJ2C2jN+/TpI9jr+Pnnn91ZPA45Amt0ySgICareTNpKZZEJWixLBHxG4IADDhCY0nbv3j2u5ilTpkiTJk3MXkdcJhOIQI4RoLLIMeC8HRGIRQABkxDz4vXXXxdE3nPLihUr5IwzzjB/iNhHCTcCW6ojXRASVL2ZKR+uTwAAE99JREFUtJXKIhO0WJYIBIhAp06dZM6cOdKhQ4e4u8CSCrOMSZMmxeUxITwIbB+Qt/X2IQiuSGURnueMLSECQn6paD8EtZQEMAipFQImWnpwBzGyrJMI+IAA+aV8ADHHVcApb/h8f30twBF15q4i5ZTGPJ/CmUU+0ee9iUASBMgvlQSckGZtpbTie5X0uSx1SxtrjIt8Kwp0gjOLUg8lKyACwSNAfqngMfbrDiASfGWByGqdZZRWsAdyUh0RBEfKt3Bmke8R4P2JQBoIkF8qDZBCUmQLXTY6chcNk1rKH3gsP6GeMCgKQEtlEZIHjM0gAqkQSIdf6tJLLyW/VCogc5BfvZLI0TVEKmb5C7uVKorjaoYnlgUg4zJUDh4c3oII+I3A/PnzpWvXrjJ16tS4qhs0aCAvvPCCNGvWLC6PCblFoJDCqlJZ5PbZ4d2IgG8IgDvq9ttvl/79+wsoQtwCzimk9+7dOy74krscj3ODwHxlo/1KfSoXrxHxYhvHUlMdNY9tqJvZNUJgJuuFCpWFFypMIwIRQmDWrFly5plnyrx58+Ja3apVK3n++eelbt26cXlMyD0C65Qb8jcNubpKdfsG1RoVdJmqsnJJ7qhOd+WzXLLKVS+oLHKFNO9DBAJEAHEwrrzyShkyZEjcXapUqSKPPvqodOnSJS6PCUQgXQSoLNJFiuWIQAQQeOONN+SCCy4QBFSKFSgLxMzARjmFCGSKAJVFpoixPBEIOQJLly6Vbt26yZtvvhnX0jp16shzzz0nrVu3jstjAhFIhkDIV8mSNZ15RIAIeCGQDr/U9ddfH7cp7lUX04iAjQBnFjYS/CQCBYgA+aUKcFDz1CXOLPIEPG9LBHKBQDr8Utj8phCBVAhwZpEKIeYTgQJBgPxSBTKQeeoGZxZ5Ap63JQK5RiAVv1Tjxo1l7NixuW4W7xcRBKgsIjJQbCYR8AOBZPxSy5Ytk44dOwr5pfxAuvDq4DJU4Y0pe0QE0kKA/FJpwcRC/48AZxZ8FIhAkSJQr149E9N7wIABAi4pt8ydO1datmwpd9xxh4CDikIEOLPgM0AEiICQX4oPQSoEOLNIhRDziUARIHDAAQfIp59+Kt27d4/r7ZQpU6RJkyby4osvxuUxoXgQ4MyieMaaPSUCaSFAfqm0YCq6QlQWRTfk7DARSI0A+aVSY1RsJbgMVWwjzv4SgTQQsPmlBg0aJFtuuWWJKxYsWCBt2rQR8kuVgKXgTzizKPghZgeJQOkQIL9U6fArlKs5syiUkWQ/iEBACJBfKiBgI1YtZxYRGzA2lwjkEwHyS+UT/fzemzOL/OLPuxOBSCFAfqlIDZevjaWy8BVOVkYECh8Bm18KEfcqV65cosPklyoBR0GdcBmqoIaTnSECuUWA/FK5xTufd+PMIp/o895EIOII2PxSt956K/mlIj6WqZrPmUUqhJhPBIhAWgiQXyotmCJbiDOLyA4dG04EwoWAzS914YUXxjWM/FJxkEQugTOLyA0ZG0wEwo8A+aXCP0aZtpDKIlPEMii/cuVKw+K5atWqDK6KL1qmTBm5+OKLTRSz+FymEIFwIkB+qXCOS7atKp/thbwuNQIbNmwQfGGgNBLJL7/8Ir/99pvst99+iYpI2bJlZfny5QnzmUEEwoiAzS/18MMPS+/evWXNmjVOM21+KaT3798/bnPcKciD0CDAmUWeh+Lqq6+WwYMHy9q1a/PcEt6eCASHAPmlgsM2VzVzgztXSPM+RKCIEbD5pa699lozU3ZDgaBLTZs2lUcffdSdzOOQIUBlEbIBYXOIQKEiULFiRbnrrrvk3Xffldq1a5foJpaoLr30Ujn22GPN0m2JTJ6EAgEqi1AMAxtBBIoHAfJLRXOsqSyyGLc///xTnnzySenWrZuceeaZZvqMzWxKuBDgOIVrPNytSYdf6pJLLpF//vnHfRmP84gAlUWG4I8fP14aNmwocDxasmSJfPfdd2b6fMopp2RYE4sHiQDHKXt0LcuSuXPnysSJE+Xtt9+WH3/8MfvKUlzZtWtX+fzzz+XQQw+NK/nYY4+ZvYzZs2fH5TEhDwjog0FJE4EhQ4ZYOkTWFltsYb3++uvmKp1RWE2aNDHpw4YNS7OmzcWuuuoqU9/mFB6VFoEgxqm0bYrC9Zs2bbIeeOABa7fddjPPc4UKFSw12zbH+oJkZfN8p9vvjRs3WsovZeGe+I65/5A2cOBAC2Uo+UNA8nfraN351VdftcqVK2ceYjy4brnxxhtN+p577ulOTuuYyiItmNIuFNQ4pd2AiBZcvXq1ddJJJ5nnuEaNGtbLL79s4UVIl4GsRx55xFIqcpOnM+pAf7Q//vhjC98jt7Kwj1u1amUpy21EEY5+s6ks0hjDn376ydp6663NA9y8eXPzJXJfpiZ/zsO9cOFCd1bK40JWFl999ZX1ww8/pMTArwJBjpNfbQxrPT169DDPMJSCLgvFNfO9996z1JrJlLn77rvj8v1MgOKCUrKVhPuzSpUq1gsvvODn7VhXmghQWaQBVNu2bZ0HV23C464YPXq0kz98+PC4/GQJhawstt9+e2vfffdN1n1f84IcJ18bGrLK8DZvLzddf/31CVt3zjnnmOe8UqVK1rx58xKW8ysDS7077rij891yK40uXbpYasDg161YTxoIcINbn8Bkoj/+xi4cZVq3bu1Jy+G22Pj111+TVVdUeX///bfgLxfCccoe5UGDBonuV5gKkhlqYDMaArYBsA4ELZ06dZI5c+ZIhw4d4m6F8da9Qpk0aVJcHhOCQYDKIgWuDz30kFNCp+rOsfsAoSRt+ffff+1DfuYQAY5TdmDrC6W888475uJddtnFWB8lqgkvSyC1hLz44ouiG86JivqWbvNLQaFtueWWJeq1+aX69Okj69evL5HHE/8RoLJIgiloCGbMmGFK1KlTR44//njP0osWLXLS+dA6UOTsgOOUPdQwW4UJOKRWrVpJKypfvrzssMMOpgzIL3VPKml5PzMvv/xygQnt/vvvX6JazIjuvPNOadGihXz77bcl8njiLwJUFknwdHPV6HqtqDWUZ2l84Wyxv0z2ea4/wU774YcfyvTp08XLUfD333+XDz74QL744oucvBnmov9RHKdc4JLOPeBHYUs6zy7e9G3RvQ77MCefyfilPvnkEzMrUsutnLSlGG9CZZFg1FesWGGm2na217op8jCNdzsNpfOFs+v08xN7A1hTrl69uhxyyCFy8MEHC5YVQP+MNoIm/YwzzhDdMJQjjjjCrPfWrVtXxo4d62czcl5X1MYp5wCluCGcSm1J59nF82WLe0ZtpwX9mYpf6rLLLiO/VECDQGWRANhnn31W1ITP5KpVj6jJrGdJTH3dsSYQwD4T2WOPPQR/pZE//vhD1BLIrD3jLRuKYfLkyWbT8uabbxb1C5ETTjjBKIbnn39e1LxX1JZeEEsDCiYXa8+l6V+ya3M1TsnaEOU8d6wV/BCnEnWQc4pAUedLbH6p0047La4J8N5v3Lhx5F+E4jqW5wQGP0owAPhRtQVfEGyweQnWy21Rk0JPayk73+sTEfDwl61g1gBFAEqGqVOnSoMGDUxV6sAkWDq7//77RZ0GTRpmEWD1xBrv4sWLTdpff/1llB1mHFGUXI1TabHB2rp6KIvbGKK0dbqvxxIN3qozFbeySLTM6q4T+xa25FNZoA3glxoxYoR5prGn4e4LcO7YsaP5bt17772y1VZb2c3mZ7YI6I8NJQYBOAXpl8LTvltxTpjesmXLmJqCP33mmWdMe3TtOe5mAwYMcNqqlixOvgZcctIbNWrkpPt9ACcuXeryu1qnviiNE5wTbV+GZM9QtnlqKWQpzbeDTboHeGbte+pLS8rL9AfYKa9mtinL56oAHDKVX8ppm90nfOoLlDVr1qxcNaVg77P5NUFRpfyHAPYg3JvDU6ZM8bQUwdviXnvt5ZjtYSko13LPPffIQQcdJO3bt4+7tTpOOWlgyLVFHa8Eb4jVqlUzzLl2etQ+ozROyrdk9rZgRRSEwFoPM9tMxW3qbZvFpltHOjORdOsqbTks/8LnAkuut9xyi/OdRL0gRVSlaNKvu+66uOBLpb13sVxPZeEx0u4Na2wSezFi4jJYQdmmsoiTfcEFF3jUFlwSnKOwQTlq1CjPm7itVdq1a+eUgZK44447nPNMDrCP457uJ7t23bp1Zj8kXUfFqlWrZvSD5/c4YcP2tttuM+bS+qYqO++8szHVRJzoWJPNZP1OlJcsznqia4JO33bbbZ1b6Cuxc5zowL2/hWc+TIL2YMn1qKOOMqED3C9L+J7ecMMN8uabbwqWLmHcQckMASoLD7xsu3NkJVIUyMNGmi3HHHNMzh9AvEnix9vrDQ9vsHijgtSvX19q1qxpNzXrT2yI77777uJ+G01VGX6AsZmejsAs8+effxZl9U2nuOMfgMKlHSfs9+BHBgrrpptuEuz5YD/qwQcflGbNmglmcErNkla7olRIuZac5tovPk6Cx4F7xq08Uh4l8p90wAEHmLHr1auXPPHEEyUahFUCeH7DAx2xaCjpI0Bl4YGVe6kg2RslKAdsKc0mtV1HNp9eigL1wBrKflOE5YgfAqswOCYuXbo0rersNsDzNx3B5nw6Fjl2XX6NE6zZ0C/MhN5//33HOg2bxpiR4cdF93nMcl8ypWS3K0qfbmWRzkuAW6GEVVkAf2xoP/7442bzGzN+97OCjfmzzjpLxo0bJ/DLwEY5JQ0ECnY3phQdU+siZ6MMJIFeMm3aNKeMrpcGStvsdf9UaWoZ47RPp92pigeSH/QGt1/jpObFBqvOnTt74qBBeEw+yOsKTa644grnOVEuppTdAzGk/qyYv9tvvz1l+TAU0JUCS/2knHbb7cen7vVY6qQahmaGvg2cWXgoVPfbrdtj1V0Uby22IGpe2NZv3333Xbt54tfMwqkwJAd+jZO9nIgZhJfY6Rorw7yhZmNmjLV+rKcHaTp77bXXejU/ado+++zj5Lv9hZzEmAP3/hM2jaMgNr/Uww8/LNh/Uqsxp9k2vxSwU+tBcfuROIV48B8CoVdneWig+61cN5DjWqBOcBZomhVBS71eLaXQiCuTzwTdW3DeohD1zEs0ZKYxKfTK8yst6JmFH+MEc1NEYsNYqs2+Z9cx3sjH38iRIz3LpEqEaacuGTr12PX59bnNNttkZTqr+zJOm2rXrp20G7pM5ZTF/XTZLmn5MGZ+/fXXli4tO/1w49+0aVPrm2++CWOzQ9Emziz0aYkVWEDZ4nZCstOw2QlLJAg2QGFdlA/B26oukxmnurPPPtvZGHbz/WCj1kv69u0beUclP8bJbdGGPRkvcVsMude+vcomSoNpJ4j34G0fhMCAIRvTWXg6o20agc549IM7LBHtB/jEbGnTpk0k38Jtfik8/3DWs6nZ0S+bXwrf70svvdTuKj9tBEKhskLWCCXhc9489AEq0brvv//exMxW/Kx01nhLXOzziVpgOe1Ur2yndt2UddL1wXfS7QO1Rzf5Q4cOtZMC+Qx6ZuHHOCHADsYSf4gGl0jsMv369UtUJLLpCBNs9w/7M4nkf//7n1Mu6GcnURv8TFdjBguzKbvv7k98t7DXQdmMACxmKDEI6Bu7pYRp5iFyfyl0NmGpGaVJx6f6G8RcmbtTpeuw1InKedD1LcncHF8A90OPSHxu0TVa8wVRRz5L36rcWb4fB60s/Binl156ycELSjSR2B79ao6ZqEhk0xFxzv7RVF8QC8tNsYIy2AzGs6WmqXGhhWPLR+Uc/VJ+KecZcH938BswZsyYqHQl8HZSWSSA+KmnnjIPEJQC1rXxlqFTb5MGigzdqExwZW6SldPJWQPHF13NWS1YaOnSjIW1VzumMiy17LaqqaClfhKWTsUt9X8IvKFBKwt0oLTj9PLLLzs/FFC0icRWzOlQYiSqI8zpaubsUNwgfKpbYaxatco67rjjDE7Yq9DlqDB3Jau2wWIQ8cfdygLH6iNkgVaGoh6bBCExAngrxw+e2mwbXh9sUKrVhIUZRhhE90ssBLBHu6Ak8HCr5ZNRbBrq1bryyitNnjq5WYiHjXzEqdZ195w0PxfKAh0pzTipJZTzAzFhwgRPXDADs39EwKtVqKKOiZbufZi+4qWiZ8+e5g8/mOg/OJZ036VQu2/BCEH9aJyxxguCRhEs2P5m2rEyuEAfBEoCBODAg7CTcEBCjAj3ZmeCS3KajA06bMyplYcJ/qJ28E7oSzRE91gMJxE2w7GZ2bBhw5y1D57Y2ITG5mnQku04wcsd/F6Q1157TU488cS4pqJu23ELnr+FvPkJs1K1+DLPvP54GidJO0qkzi4ycpqMAzICCfg+gQpH96YMiy9Ymyn/IUBlwSchMARgnQNlgR+dsArelWD9Ax+DBx54QPRtOq6psGKylSwUczKv/riLmRBJBED1Ah+UdKlnItnJDBsdLiawDBvP4uFGAAGjwIgbZgHTqu1clkip2em6Xm9mZ2HuD9vmDwJ4IaCiKIkllUVJPHjmIwKgjFZrIx9rDKaq888/31QM9l4s18WK3Ydkcdhjr+E5ESg0BKgsCm1E2Z+MEUCkQUQQBPUD4h24t/HUD8NEYwOlNdhoKUSgWBHgnkWxjjz7XQIBtR6TSy65RIYNGybgEjryyCMF3syIf9CiRQvRiISy5557lriGJ0SgmBCgsiim0WZfiQARIAJZIsBlqCyB42VEgAgQgWJCgMqimEabfSUCRIAIZIkAlUWWwPEyIkAEiEAxIUBlUUyjzb4SASJABLJEgMoiS+B4GREgAkSgmBD4P4kaoC6ndKaWAAAAAElFTkSuQmCC)"
      ],
      "metadata": {
        "id": "EducyLBD7Hji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose that $y^{(i)} = 1$ ($x^{(i)}$ lies on the positive side of boundary). Then:\n",
        "1. The point $x$ which lie on the decision boundary are those for which $\\theta^\\top x + \\theta_0 = 0$ (score is precisely $0$ and lie between $-1$ and $1$)\n",
        "2. The vector $\\frac{\\theta}{\\|\\theta\\|}$ is perpedicular to the hyperplane $\\theta^\\top x + \\theta_0$ and has unit norm (fact from calculus),\n",
        "3. Let $x_0$ is the point on the decision boundary closest to $x^{(i)}$. Then by the definition of the margin $x^{(i)} = x_0 + \\tilde \\gamma^{(i)}\\frac{\\theta}{\\|\\theta\\|}$ or \n",
        "$$x_0 = x^{(i)} - \\tilde \\gamma^{(i)}\\frac{\\theta}{\\|\\theta\\|}$$\n",
        "4. Since $x_0$ is on the hyperplane, $\\theta^\\top x_0 + \\theta_0 = 0$, or \n",
        "$$\\theta^\\top \\left( x^{(i)} - \\tilde \\gamma^{(i)}\\frac{\\theta}{\\|\\theta\\|}\\right) + \\theta_0 = 0$$\n",
        "\n",
        "5. Solving for $\\tilde \\gamma^{(i)}$, and using the fact that $\\theta^\\top \\theta = \\theta^2$, we obtain;\n",
        "$$\\tilde \\gamma^{(i)} = \\frac{\\theta^\\top x^{(i)} + \\theta_0}{\\|\\theta\\|}$$\n",
        "\n",
        "Which our geometric margin. The case of $y^{(i)}=-1$ can be proven in a similar way."
      ],
      "metadata": {
        "id": "Dy1TaP3M9Xg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use our formula for $\\psi^{(i)}$ to precisely plot the margin on our earlier plot"
      ],
      "metadata": {
        "id": "Bq_odO1nnImd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the decision boundary\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.scatter(X[:,0], X[:,1], c=iris_y2, s=30, edgecolor='k', cmap=plt.cm.Paired)\n",
        "plt.contour(xx,yy,Z,color='k', levels=[-1,0,1], alpha=0.15,\n",
        "            linestyle=['--','-','--'])\n",
        "plt.scatter(clf.support_vectors_[:,0], clf.support_vectors_[:,1], s=100,\n",
        "            linewidth=1, facecolors='none', edgecolors='k')\n",
        "plt.xlim([4.5,6])\n",
        "plt.ylim([2.25,4])\n",
        "\n",
        "# Plot the margin vector\n",
        "theta = clf.coef_[0]\n",
        "theta0 = clf.intercept_\n",
        "for idx in clf.support_[:3]:\n",
        "  x0 = X[idx]\n",
        "  y0 = iris_y2.iloc[idx]\n",
        "  margin_x0 = (theta.dot(x0) + theta0)[0] / np.linalg.norm(theta)\n",
        "  w = theta/np.linalg.norm(theta)\n",
        "  plt.plot([x0[0], x0[0]-w[0]*margin_x0],[x0[1], x0[1] - w[1]*margin_x0], color='blue')\n",
        "  plt.scatter([x0[0]-w[0]*margin_x0], [x0[1] - w[1]*margin_x0], color='blue')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SXAPhIhsryqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 2: The Max-Margin Classifier**\n",
        "\n",
        "We have seen a way to measure the confidence level of a classifier at a data point using the notion of a *margin*.\n",
        "\n",
        "Next, we are going to see how to maximize the margin of linear classifiers."
      ],
      "metadata": {
        "id": "3_YQSitgpe--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Linear Model Family**\n",
        "In this lecture, we consider classification with linear models of the form:\n",
        "$$f_{\\theta}(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\dots + \\theta_dx_d.$$\n",
        "\n",
        "where $x \\in \\mathbb{R}^d$ is a vector of features and $y \\in \\{-1,1\\}$ is the target. The $\\theta_j$ is the parameters of the model.\n",
        "\n",
        "We can represent the model in vectorized form:\n",
        "\n",
        "$$f_{\\theta}(x) = \\theta^\\top x + \\theta_0.$$"
      ],
      "metadata": {
        "id": "R0kn6zQYqHgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Geometric Margin**\n",
        "We define the *geometric* margin $\\gamma^{(i)}$ with respect to a training example $(x^{(i)}, y^{(i)})$ as\n",
        "$$\\gamma^{(i)} = y^{(i)}\\frac{\\theta^\\top x + \\theta_0}{\\|\\theta\\|}$$\n",
        "\n",
        "This also corresponds to the distance from $x^{(i)}$ to the hyperplane."
      ],
      "metadata": {
        "id": "NaqCp2zwrOn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Maximizing the Margin**\n",
        "\n",
        "We want to define an objective that will result in maximizing the margin. As a first attempt, consider the following optimization problem.\n",
        "\n",
        "\\begin{align*}\n",
        "\\max_{\\theta. \\theta_0, \\gamma} \\gamma \\; & \\\\\n",
        "\\text{subjected to} \\; & y^{(i)}\\frac{(x^{(i)})^\\top\\theta + \\theta_0}{\\|\\theta\\|} \\geq \\gamma \\; \\text{for all} \\;  i\n",
        "\\end{align*}\n",
        "\n",
        "This is maximies the smallest margin over the $(x^{(i)},y^{(i)})$. It gaurantees each point has margin at least $\\gamma$"
      ],
      "metadata": {
        "id": "7bbZK1dvsIXa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This problem is difficult to optimize because of the devision by $\\|\\theta\\|$ and we would like to simplify it. First, consider the equivalent problem\n",
        "\n",
        "\\begin{align*}\n",
        "\\max_{\\theta. \\theta_0, \\gamma} \\gamma \\; & \\\\\n",
        "\\text{subjected to} \\; & y^{(i)}(x^{(i)})^\\top\\theta + \\theta_0 \\geq \\|\\theta\\|\\gamma \\; \\text{for all} \\;  i\n",
        "\\end{align*}\n",
        "\n",
        "Note that this problem has an extra degree of freedom:\n",
        "\n",
        "* Suppose we multiply $\\theta, \\theta_0$ by some constant $c > 0$\n",
        "* This yields another valid solution!\n",
        "\n",
        "To enforce uniqueness, we add another constraint that doesn't change the minimizer\n",
        "\n",
        "$$\\|\\theta\\| = \\frac{1}{\\gamma}.$$\n",
        "\n",
        "This ensures we cannot rescale $\\theta$ and also takes our linear model to assign each $x^{(i)}$ a score of at least $\\pm 1$\n",
        "\n",
        "$$y^{(i)}(x^{(i)})^\\top\\theta + \\theta_0 \\geq 1 \\; \\text{for all} \\;  i$$"
      ],
      "metadata": {
        "id": "xwq3VAuttvYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we constraint $\\|\\theta\\| = \\frac{1}{\\gamma}$ holds, then we know that $\\gamma = 1/\\theta$ and we can replace $\\gamma$ in the optimization problem to obtain:\n",
        "\n",
        "\\begin{align*}\n",
        "\\max_{\\theta. \\theta_0, \\gamma} \\frac{1}{\\|\\theta\\|} \\; & \\\\\n",
        "\\text{subjected to} \\; & y^{(i)}(x^{(i)})^\\top\\theta + \\theta_0 \\geq 1 \\; \\text{for all} \\;  i\n",
        "\\end{align*}\n",
        "\n",
        "The solution of this problem is still the same.\n"
      ],
      "metadata": {
        "id": "ZsgtFed7tvdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Version**\n",
        "\n",
        "Instead of maximizing $1/\\theta$, we can minimize $\\theta$, or equivalently we can minimize $\\frac{1}{2}\\|\\theta\\|^2$\n",
        "\n",
        "\\begin{align*}\n",
        "\\min_{\\theta. \\theta_0, \\gamma} \\frac{1}{2}\\|\\theta\\|^2 \\; & \\\\\n",
        "\\text{subjected to} \\; & y^{(i)}(x^{(i)})^\\top\\theta + \\theta_0 \\geq 1 \\; \\text{for all} \\;  i\n",
        "\\end{align*}\n",
        "\n",
        "This is now a quadratic program that can be solved using off-the-shelf optimization algorithms!"
      ],
      "metadata": {
        "id": "yQLZvfrhwqXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Algorithm: Linear Support Vector Machine Classification**\n",
        "\n",
        "* **Type**: Supervised learning (binary classification)\n",
        "* **Model Family**: Linear decision boundaries\n",
        "* Objective Function: Max-margin optimization\n",
        "* Optimizer: Quadratic optimization algorithms"
      ],
      "metadata": {
        "id": "vKd6f6hsxvKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 3: Soft Margins and the Hinge Loss**\n",
        "\n",
        "Let's continue look at how can we maximize the margin."
      ],
      "metadata": {
        "id": "DJzGte1byhAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Maximizing the Margin**\n",
        "We saw that maximizing the margin amounts to solving the following optimization problem:\n",
        "\n",
        "\\begin{align*}\n",
        "\\min_{\\theta. \\theta_0, \\gamma} \\frac{1}{2}\\|\\theta\\|^2 \\; & \\\\\n",
        "\\text{subjected to} \\; & y^{(i)}(x^{(i)})^\\top\\theta + \\theta_0 \\geq 1 \\; \\text{for all} \\;  i\n",
        "\\end{align*}\n",
        "\n"
      ],
      "metadata": {
        "id": "npL6LECRyulU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Non-Separable Problem**\n",
        "So far, we have assume that a linear hyperplane exists. However, what if the classes are non-separable? Then our optimization problem does not have a solution and we need to modify it.\n",
        "\n",
        "Our solution is going to be make each constraint \"soft\", by introducing \"slack\" variables, which allow the constraint to be violated.\n",
        "\n",
        "$$y^{(i)}(x^{(i)})^\\top\\theta + \\theta_0 \\geq 1 - \\xi_i.$$\n",
        "\n",
        "* If we can classify each point with a perfect score of $\\geq 1$, the $\\xi_i = 0.$\n",
        "* If we cannot assign a perfect score, we assign a score of $1 - \\xi_i.$\n",
        "* We define optimization such that the $\\xi_i$ are chosen to be as small as possible.\n",
        "\n",
        "In the optimization problem, we assign a penalty $C$ to these slack variables\n",
        "\n",
        "\\begin{align*}\n",
        "\\min_{\\theta. \\theta_0, \\gamma} \\frac{1}{2}\\|\\theta\\|^2 + &C\\sum_{i=1}^n\\xi_i \\;   \\\\\n",
        "\\text{subjected to} \\; & y^{(i)}(x^{(i)})^\\top\\theta + \\theta_0 \\geq 1 \\; \\text{for all} \\;  i \\; & \\\\\n",
        "\\xi_i \\geq 0\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "09cg1laqz6QZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Towards an Unconstrainted Objective**\n",
        "\n",
        "Since $\\xi_i = (1 - y^{(i)}((x^{(i)})^\\top\\theta + \\theta_0))^+$, we can take \n",
        "\n",
        "\\begin{align*}\n",
        "\\min_{\\theta. \\theta_0, \\gamma} \\frac{1}{2}\\|\\theta\\|^2 + &C\\sum_{i=1}^n\\xi_i \\;   \\\\\n",
        "\\text{subjected to} \\; &\\xi_i \\geq 1 - y^{(i)}(x^{(i)})^\\top\\theta + \\theta_0 \\geq 0\\; \\text{for all} \\;  i \n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "RR0Ap6xh9rF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we turn it into the following by plugging in the definition of $\\xi_i$\n",
        "\n",
        "$$\\min_{\\theta, \\theta_0} \\frac{1}{2}\\|\\theta\\|^2 + C\\sum_{i=1}^n (1 - y^{(i)}((x^{(i)})^\\top\\theta + \\theta_0))^+$$\n",
        "\n",
        "Since it doesn't matter which term we multiply by $C > 0$, this is equivalent to:\n",
        "\n",
        "$$\\min_{\\theta, \\theta_0,\\xi}\\sum_{i=1}^n (1 - y^{(i)}((x^{(i)})^\\top\\theta + \\theta_0))^+ + \\frac{\\lambda}{2}\\|\\theta\\|^2 $$\n",
        "\n",
        "for some $\\lambda > 0$."
      ],
      "metadata": {
        "id": "CQHjdS8j9rN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **An Unconstrainted Objective**\n",
        "\n",
        "We have now turned our optimization problem into an unconstrainted form:\n",
        "\n",
        "$$\\min_{\\theta, \\theta_0,\\xi}\\underbrace{\\sum_{i=1}^n (1 - y^{(i)}((x^{(i)})^\\top\\theta + \\theta_0))^+}_{\\text{Hinge Loss}} + \\underbrace{frac{\\lambda}{2}\\|\\theta\\|^2}_{\\text{regularizer}} $$\n",
        "\n",
        "* The hinge loss penalizes the incorrect predictions.\n",
        "* The L2-regularizer ensures the weights are small and well-behaved.\n"
      ],
      "metadata": {
        "id": "PorQJT6d_fMs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The Hinge Loss**\n",
        "\n",
        "Consider again our new loss term for a label $y$ and a prediction $f$:\n",
        "$$L(y, f) = \\max(1 - y \\cdot f, 0).$$\n",
        "\n",
        "* If the prediction $f$ has same class as $y$, and $\\|f\\| \\geq 1,$ the loss is zero.\n",
        " * If class correct, no penalty if score $f$ is larger than target $y$.\n",
        "* If this prediction $f$ is of wrong class, or $\\|f\\| \\leq 1$, loss is $|y - f|$."
      ],
      "metadata": {
        "id": "ZktoMq7z9rUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize a few losses $L(y=1,f)$, as a function of $f$, including hinge"
      ],
      "metadata": {
        "id": "d3fVy_sFBhvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define losses for a target of y=1\n",
        "hinge_loss = lambda f: np.maximum(1 -f, 0)\n",
        "l2_loss = lambda f: (1-f)**2\n",
        "l1_loss = lambda f: np.abs(f-1)\n",
        "\n",
        "# plot them\n",
        "fs = np.linspace(0,2)\n",
        "plt.plot(fs, l1_loss(fs), fs, l2_loss(fs), fs, hinge_loss(fs), linewidth=5, alpha=0.5)\n",
        "plt.legend(['L1 loss','L2 loss','Hinge loss'])\n",
        "plt.xlabel('Prediction f')\n",
        "plt.ylabel('L(y=1,f)')"
      ],
      "metadata": {
        "id": "uC0nJ5R9n3aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The Hinge loss is linear as L1 loss\n",
        "* But it only penalizes errors that are on the 'wrong' side:\n",
        " * We don't have error of $|f - y|$ if true class is $1$ and $f < 1$.\n",
        " * We don't penalize for predicting $f >1$ if the true class is $1$."
      ],
      "metadata": {
        "id": "itxqytLzC7OQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(fs, hinge_loss(fs), linewidth=5, alpha=0.5)\n",
        "plt.legend([\"Hinge loss\"])"
      ],
      "metadata": {
        "id": "-2HI-M-iCzqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Properties of Hinge Loss**\n",
        "The hinge loss is one of the best losses in machine learning!\n",
        "* It penalizes the errors that \"matter\", hence is less sensitive to outlliers.\n",
        "* Minimizing a regularized hinge loss optimizes for a high margin,\n",
        "* THh loss is non-differential at point, which may make it more challenging to optimize."
      ],
      "metadata": {
        "id": "JR7sCg7eDrTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 4: Optimization for SVM**\n",
        "\n",
        "We have seen a new way to formulate the SVM objective. Let's now see how to optimize it"
      ],
      "metadata": {
        "id": "xlTtaaT7EbMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: The Hinge Loss**\n",
        "The Hinge loss for a label $y$ and a prediction $f$ is:\n",
        "$$L(y,f) = \\max(1 - y\\cdot f, 0).$$\n",
        "* The Hinge Loss is linear like L1 loss.\n",
        "* But only penalizes errors that are on the side of the wrong class."
      ],
      "metadata": {
        "id": "JPqS8rsnErWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(fs, hinge_loss(fs), linewidth=5, alpha=0.5)\n",
        "plt.legend([\"Hinge loss\"])"
      ],
      "metadata": {
        "id": "B7NR_ikZFIx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: SVM Objective**\n",
        "\n",
        "Maximizing the margin can be done in the following form:\n",
        "\n",
        "$$\\min_{\\theta, \\theta_0,\\xi}\\underbrace{\\sum_{i=1}^n (1 - y^{(i)}((x^{(i)})^\\top\\theta + \\theta_0))^+}_{\\text{Hinge Loss}} + \\underbrace{\\frac{\\lambda}{2}\\|\\theta\\|^2}_{\\text{regularizer}} $$\n",
        "\n",
        "* The Hinge Loss penalizes incorrect predictions.\n",
        "* The L2 regularizer ensures the weights are small and well-behaved."
      ],
      "metadata": {
        "id": "ly_gB1ZWFLeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can easily implement this objective in numpy.\n",
        "\n",
        "First we define the model"
      ],
      "metadata": {
        "id": "e-DimxBjFxU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(X, theta):\n",
        "  return X.dot(theta)"
      ],
      "metadata": {
        "id": "zgQ4N8a8F4fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then we define the objective"
      ],
      "metadata": {
        "id": "eRiDX73OF_T3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def svm_objective(theta, X, y, C=.1):\n",
        "  return (np.maximum(1 - y*f(X,theta), 0) + C*0.5*np.linalg.norm(theta[:-1])**2).mean()"
      ],
      "metadata": {
        "id": "JuXbEFQ6GByG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Review: Gradient Descent**\n",
        "\n",
        "If we want to optimize $J(\\theta)$, we start with an initial guess $\\theta_0$ for the parameters and repeat the following update:\n",
        "\n",
        "$$\\theta_i := \\theta_{i-1} - \\alpha \\cdot \\nabla_{\\theta}J(\\theta_{i-1}).$$"
      ],
      "metadata": {
        "id": "djU4nsrfGmva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **A Gradient for Hinge Loss**\n",
        "\n",
        "What is the gradient for Hinge loss with a linear $f$?\n",
        "\n",
        "$$J(\\theta) = \\max(1 - y\\cdot f_{\\theta}(x), 0) = \\max(1 - y \\cdot\\theta^\\top x,0).$$"
      ],
      "metadata": {
        "id": "XMM-NnJvHClO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we see that the linear part of $J$ that behaves like $1 - y \\cdot f_{\\theta}(x)$ (when $y \\cdot f_{\\theta}(x) < 1$) in orange:"
      ],
      "metadata": {
        "id": "GHagLO-SHjYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(fs, hinge_loss(fs), fs[:25], hinge_loss(fs[:25]), linewidth=5, alpha=0.5)\n",
        "plt.legend(['Hinge loss','Hinge loss when $y \\cdot f_{\\theta}(x) < 1$'])"
      ],
      "metadata": {
        "id": "2yRSgpAMVd6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When $y \\cdot f_{\\theta}(x) < 1$, we are in the \"line\" part and $J(\\theta)$ behaves a like $1 - y\\cdot f_{\\theta}(x)$.\n",
        "\n",
        "Our objective is:\n",
        "$$J(\\theta) = \\max(1 - y \\cdot f_{\\theta}(x), 0) = \\max(1 - y \\cdot \\theta^\\top x, 0).$$\n",
        "\n",
        "Hence the gradient is in this regime is:\n",
        "$$\\nabla_{\\theta}J(\\theta) = -y\\cdot\\nabla_{\\theta}f_{\\theta}(x) = -y\\cdot x.$$\n",
        "\n",
        "where we used $\\nabla_{\\theta}\\theta^\\top x = x.$"
      ],
      "metadata": {
        "id": "gT9-ZJkCWpwo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the gradient for the hinge loss with a linear $f$?\n",
        "$$J(\\theta) = \\max(1 - y \\cdot f_{\\theta}(x),0) = \\max(1 - y \\cdot \\theta^\\top x, 0).$$\n",
        "\n",
        "* When $y \\cdot f_{\\theta}(x) < 1,$ we are in the \"flat\" and $J(\\theta) = 0$\n",
        "* Hence the gradient just also zero!\n",
        "\n",
        "When $y \\cdot f_{\\theta}(x) = 1$, we are in the 'kink', and the gradient is not defined!\n",
        " * In practice, we can either take the gradient when $y \\cdot f > 1$ or $y \\cdot f < 1$ or anything in between. This is called *subgradient.*\n"
      ],
      "metadata": {
        "id": "BwEKNxI5YFSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **A Steepest Descent Direction for Hinge Loss**\n",
        "\n",
        "We can define a \"gradient\" like function for $\\tilde \\nabla_{\\theta}J(\\theta)$ for the Hinge loss:\n",
        "$$J(\\theta) = \\max(1 - y \\cdot f_{\\theta}(x), 0) = \\max(1 - y \\cdot \\theta^\\top x, 0).$$\n",
        "\n",
        "It equals:\n",
        "\n",
        "$$\\tilde \\nabla_{\\theta} J(\\theta) = \\begin{cases} -y \\cdot x  \\; &\\text{if} \\; y \\cdot f_{\\theta}(x) > 1 \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}.$$"
      ],
      "metadata": {
        "id": "Ur2fQ8x-aJGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Subgradient Descent for SVM**\n",
        "\n",
        "Putting this together, we obtain a complete learning algorithm, based on an optimization procedure called subgradient descent."
      ],
      "metadata": {
        "id": "8RguizgQcW48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's implement this algorithm"
      ],
      "metadata": {
        "id": "NXGy5bMActXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we implement the approximate gradient:"
      ],
      "metadata": {
        "id": "ux89HzwXcygO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def svm_gradient(theta, X, y, C=.1):\n",
        "  \"\"\" The (approximate) gradient of cost function \"\"\"\n",
        "  yy = y.copy()\n",
        "  yy[y*f(X,theta) >= 1] = 0\n",
        "  subgradient = np.mean(-yy * X.T, axis=1)\n",
        "  subgradient[:-1] += C*theta[:-1]\n",
        "  return subgradient"
      ],
      "metadata": {
        "id": "uktzna8Lc4q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **And then we implemet the subgradient descent**"
      ],
      "metadata": {
        "id": "cla2StjteLog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loaad the Iris dataset\n",
        "iris = datasets.load_iris(as_frame=True)\n",
        "iris_X, iris_y = iris.data, iris.target\n",
        "\n",
        "# subsample to a third of the data points\n",
        "iris_X = iris_X.iloc[::4]\n",
        "iris_y = iris_y.iloc[::4]\n",
        "\n",
        "# Create a binary classification dataset with label +/-1\n",
        "iris_y2 = iris_y.copy()\n",
        "iris_y2[iris_y2==2] = 1\n",
        "iris_y2[iris_y2==0] = -1\n",
        "\n",
        "# Print part of the dataset\n",
        "pd.concat([iris_X, iris_y2], axis=1).head()\n",
        "pd.concat([iris_X, iris_y2], axis=1).tail()"
      ],
      "metadata": {
        "id": "PsoYevhrjQeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 2d version of dataset and subsample it\n",
        "X = iris_X.to_numpy()[:,:2]\n",
        "x_min, x_max = X[:,0].min() -.5, X[:,0].max() + .5\n",
        "y_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, .02), np.arange(y_min, y_max, .02))\n"
      ],
      "metadata": {
        "id": "SnPwYap0jZp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 5e-4\n",
        "step_size = 1e-2\n",
        "\n",
        "theta, theta_prev = np.ones((3,)), np.zeros((3,))\n",
        "iter = 0\n",
        "iris_X['one'] = 1\n",
        "X_train = iris_X.iloc[:,[0,1,-1]].to_numpy()\n",
        "y_train = iris_y2.to_numpy()\n",
        "\n",
        "while np.linalg.norm(theta - theta_prev) > threshold:\n",
        "  if iter % 1000 == 0:\n",
        "    print('Iteration %d. J: %.6f' % (iter, svm_objective(theta, X_train, y_train)))\n",
        "    theta_prev = theta\n",
        "    gradient = svm_gradient(theta, X_train, y_train)\n",
        "    theta = theta_prev - step_size*gradient\n",
        "    iter += 1"
      ],
      "metadata": {
        "id": "izeuJuOweRSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visialize the results to convince ourselves we found a good boundary"
      ],
      "metadata": {
        "id": "J33dmtWVgfDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
        "Z = f(np.c_[xx.ravel(), yy.ravel(), np.ones(xx.ravel().shape)], theta)\n",
        "Z[Z<0] = 0\n",
        "Z[Z>0] = 1\n",
        "\n",
        "# Put the result into the colow plot\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.pcolormesh(xx,yy, Z, cmap=plt.cm.Paired)\n",
        "\n",
        "# Plot also the training points\n",
        "plt.scatter(X_train[:,0], X_train[:,1], c=y_train, edgecolor='k', s=40, cmap=plt.cm.Paired)\n",
        "plt.xlabel('Sepal length')\n",
        "plt.ylabel('Sepal width')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f22sDI97goL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Algorithm: Linear Support Vector Machine Classification**\n",
        "\n",
        "* **Type:** Supervised learning (binary classification)\n",
        "* **Model Family:** Linear decision boundaries\n",
        "* **Objective function:** L2-regularized hinge loss\n",
        "* **Optimizer:** Subgradient descent"
      ],
      "metadata": {
        "id": "Ov8EkaVRmJ1Q"
      }
    }
  ]
}